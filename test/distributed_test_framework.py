#!/usr/bin/env python3
"""
PowerAutomation åˆ†å¸ƒå¼æµ‹è¯•æ¡†æ¶

åŸºäºMCPåè®®çš„åˆ†å¸ƒå¼æµ‹è¯•ç³»ç»Ÿï¼Œæ”¯æŒMacç»ˆç«¯MCPå’ŒWindows WSLè·¨å¹³å°æµ‹è¯•éƒ¨ç½²
"""

import os
import sys
import json
import yaml
import asyncio
import aiohttp
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import concurrent.futures
import threading
import time

@dataclass
class TestNode:
    """æµ‹è¯•èŠ‚ç‚¹é…ç½®"""
    node_id: str
    platform: str  # "mac", "windows", "linux"
    mcp_endpoint: str
    status: str = "idle"  # "idle", "running", "error"
    capabilities: List[str] = None
    current_load: float = 0.0
    max_concurrent_tests: int = 5

@dataclass
class DistributedTestCase:
    """åˆ†å¸ƒå¼æµ‹è¯•ç”¨ä¾‹"""
    test_id: str
    test_name: str
    test_type: str
    target_platforms: List[str]
    test_script_path: str
    environment_requirements: Dict[str, Any]
    estimated_duration: int  # ç§’
    priority: int = 1  # 1-5, 5æœ€é«˜

class DistributedTestCoordinator:
    """åˆ†å¸ƒå¼æµ‹è¯•åè°ƒå™¨"""
    
    def __init__(self, config_path: str = "distributed_test_config.yaml"):
        self.config_path = Path(config_path)
        self.test_nodes: Dict[str, TestNode] = {}
        self.test_queue: List[DistributedTestCase] = []
        self.running_tests: Dict[str, Dict] = {}
        self.test_results: Dict[str, Dict] = {}
        
        # åŠ è½½é…ç½®
        self.load_configuration()
        
        # åˆå§‹åŒ–MCPè¿æ¥
        self.mcp_sessions: Dict[str, Any] = {}
        
    def load_configuration(self):
        """åŠ è½½åˆ†å¸ƒå¼æµ‹è¯•é…ç½®"""
        if self.config_path.exists():
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                
            # åŠ è½½æµ‹è¯•èŠ‚ç‚¹é…ç½®
            for node_config in config.get('test_nodes', []):
                node = TestNode(**node_config)
                self.test_nodes[node.node_id] = node
                
        else:
            # åˆ›å»ºé»˜è®¤é…ç½®
            self.create_default_configuration()
    
    def create_default_configuration(self):
        """åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶"""
        default_config = {
            'test_nodes': [
                {
                    'node_id': 'mac_node_1',
                    'platform': 'mac',
                    'mcp_endpoint': 'mcp://localhost:8901/mac-terminal',
                    'capabilities': ['terminal', 'xcode', 'simulator'],
                    'max_concurrent_tests': 3
                },
                {
                    'node_id': 'windows_node_1', 
                    'platform': 'windows',
                    'mcp_endpoint': 'mcp://localhost:8902/windows-wsl',
                    'capabilities': ['wsl', 'powershell', 'visual_studio'],
                    'max_concurrent_tests': 3
                },
                {
                    'node_id': 'linux_node_1',
                    'platform': 'linux',
                    'mcp_endpoint': 'mcp://localhost:8903/linux-docker',
                    'capabilities': ['docker', 'bash', 'python'],
                    'max_concurrent_tests': 5
                }
            ],
            'test_distribution': {
                'load_balancing_strategy': 'round_robin',  # 'round_robin', 'least_loaded', 'capability_based'
                'retry_failed_tests': True,
                'max_retries': 3,
                'timeout_seconds': 300
            }
        }
        
        with open(self.config_path, 'w', encoding='utf-8') as f:
            yaml.dump(default_config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"âœ… åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {self.config_path}")
    
    async def initialize_mcp_connections(self):
        """åˆå§‹åŒ–MCPè¿æ¥"""
        print("ğŸ”— åˆå§‹åŒ–MCPè¿æ¥...")
        
        for node_id, node in self.test_nodes.items():
            try:
                # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„MCPè¿æ¥é€»è¾‘
                # ç›®å‰æ¨¡æ‹Ÿè¿æ¥è¿‡ç¨‹
                print(f"è¿æ¥åˆ° {node.platform} èŠ‚ç‚¹: {node.mcp_endpoint}")
                
                # æ¨¡æ‹ŸMCPæ¡æ‰‹
                await asyncio.sleep(0.5)
                
                # éªŒè¯èŠ‚ç‚¹èƒ½åŠ›
                capabilities = await self.verify_node_capabilities(node)
                node.capabilities = capabilities
                node.status = "idle"
                
                self.mcp_sessions[node_id] = {
                    'connected': True,
                    'last_ping': datetime.now(),
                    'session_id': f"mcp_session_{node_id}"
                }
                
                print(f"âœ… {node.platform} èŠ‚ç‚¹è¿æ¥æˆåŠŸ: {node_id}")
                
            except Exception as e:
                print(f"âŒ {node.platform} èŠ‚ç‚¹è¿æ¥å¤±è´¥: {node_id} - {e}")
                node.status = "error"
    
    async def verify_node_capabilities(self, node: TestNode) -> List[str]:
        """éªŒè¯èŠ‚ç‚¹èƒ½åŠ›"""
        # æ¨¡æ‹Ÿèƒ½åŠ›æ£€æµ‹
        if node.platform == "mac":
            return ['terminal', 'python3', 'git', 'xcode']
        elif node.platform == "windows":
            return ['wsl', 'powershell', 'python3', 'git', 'visual_studio']
        elif node.platform == "linux":
            return ['bash', 'python3', 'git', 'docker']
        else:
            return ['basic']
    
    def add_test_case(self, test_case: DistributedTestCase):
        """æ·»åŠ æµ‹è¯•ç”¨ä¾‹åˆ°é˜Ÿåˆ—"""
        self.test_queue.append(test_case)
        print(f"ğŸ“ æ·»åŠ æµ‹è¯•ç”¨ä¾‹: {test_case.test_id} - {test_case.test_name}")
    
    def select_optimal_node(self, test_case: DistributedTestCase) -> Optional[TestNode]:
        """é€‰æ‹©æœ€ä¼˜æµ‹è¯•èŠ‚ç‚¹"""
        available_nodes = []
        
        # ç­›é€‰æ”¯æŒç›®æ ‡å¹³å°çš„èŠ‚ç‚¹
        for node in self.test_nodes.values():
            if (node.platform in test_case.target_platforms and 
                node.status == "idle" and 
                node.current_load < node.max_concurrent_tests):
                available_nodes.append(node)
        
        if not available_nodes:
            return None
        
        # è´Ÿè½½å‡è¡¡ç­–ç•¥é€‰æ‹©èŠ‚ç‚¹
        # è¿™é‡Œä½¿ç”¨æœ€å°‘è´Ÿè½½ç­–ç•¥
        optimal_node = min(available_nodes, key=lambda n: n.current_load)
        return optimal_node
    
    async def execute_test_on_node(self, test_case: DistributedTestCase, node: TestNode) -> Dict[str, Any]:
        """åœ¨æŒ‡å®šèŠ‚ç‚¹æ‰§è¡Œæµ‹è¯•"""
        print(f"ğŸš€ åœ¨ {node.platform} èŠ‚ç‚¹æ‰§è¡Œæµ‹è¯•: {test_case.test_id}")
        
        # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
        node.status = "running"
        node.current_load += 1
        
        # è®°å½•æµ‹è¯•å¼€å§‹
        test_execution = {
            'test_id': test_case.test_id,
            'node_id': node.node_id,
            'start_time': datetime.now(),
            'status': 'running'
        }
        self.running_tests[test_case.test_id] = test_execution
        
        try:
            # é€šè¿‡MCPå‘é€æµ‹è¯•è„šæœ¬åˆ°è¿œç¨‹èŠ‚ç‚¹
            result = await self.send_test_via_mcp(test_case, node)
            
            # ç­‰å¾…æµ‹è¯•å®Œæˆ
            await self.wait_for_test_completion(test_case, node)
            
            # æ”¶é›†æµ‹è¯•ç»“æœ
            test_result = await self.collect_test_results(test_case, node)
            
            # æ›´æ–°æµ‹è¯•ç»“æœ
            test_execution.update({
                'end_time': datetime.now(),
                'status': 'completed',
                'result': test_result
            })
            
            self.test_results[test_case.test_id] = test_execution
            
            print(f"âœ… æµ‹è¯•å®Œæˆ: {test_case.test_id} on {node.node_id}")
            return test_result
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {test_case.test_id} on {node.node_id} - {e}")
            
            test_execution.update({
                'end_time': datetime.now(),
                'status': 'failed',
                'error': str(e)
            })
            
            return {'success': False, 'error': str(e)}
            
        finally:
            # é‡Šæ”¾èŠ‚ç‚¹èµ„æº
            node.current_load -= 1
            if node.current_load == 0:
                node.status = "idle"
            
            # ç§»é™¤è¿è¡Œä¸­çš„æµ‹è¯•è®°å½•
            if test_case.test_id in self.running_tests:
                del self.running_tests[test_case.test_id]
    
    async def send_test_via_mcp(self, test_case: DistributedTestCase, node: TestNode) -> Dict[str, Any]:
        """é€šè¿‡MCPå‘é€æµ‹è¯•åˆ°è¿œç¨‹èŠ‚ç‚¹"""
        # æ„å»ºMCPæ¶ˆæ¯
        mcp_message = {
            'method': 'execute_test',
            'params': {
                'test_id': test_case.test_id,
                'test_script': self.read_test_script(test_case.test_script_path),
                'environment': test_case.environment_requirements,
                'platform': node.platform
            }
        }
        
        # æ¨¡æ‹ŸMCPé€šä¿¡
        print(f"ğŸ“¤ å‘é€æµ‹è¯•è„šæœ¬åˆ° {node.node_id}")
        await asyncio.sleep(1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        
        return {'success': True, 'message': 'Test script sent successfully'}
    
    def read_test_script(self, script_path: str) -> str:
        """è¯»å–æµ‹è¯•è„šæœ¬å†…å®¹"""
        try:
            with open(script_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"âŒ è¯»å–æµ‹è¯•è„šæœ¬å¤±è´¥: {script_path} - {e}")
            return ""
    
    async def wait_for_test_completion(self, test_case: DistributedTestCase, node: TestNode):
        """ç­‰å¾…æµ‹è¯•å®Œæˆ"""
        # æ¨¡æ‹Ÿæµ‹è¯•æ‰§è¡Œæ—¶é—´
        await asyncio.sleep(test_case.estimated_duration)
    
    async def collect_test_results(self, test_case: DistributedTestCase, node: TestNode) -> Dict[str, Any]:
        """æ”¶é›†æµ‹è¯•ç»“æœ"""
        # æ¨¡æ‹Ÿç»“æœæ”¶é›†
        return {
            'success': True,
            'test_id': test_case.test_id,
            'node_id': node.node_id,
            'platform': node.platform,
            'execution_time': test_case.estimated_duration,
            'screenshots': f"screenshots/{test_case.test_id}_{node.node_id}",
            'logs': f"logs/{test_case.test_id}_{node.node_id}.log"
        }
    
    async def run_distributed_tests(self):
        """è¿è¡Œåˆ†å¸ƒå¼æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹åˆ†å¸ƒå¼æµ‹è¯•æ‰§è¡Œ")
        
        # åˆå§‹åŒ–MCPè¿æ¥
        await self.initialize_mcp_connections()
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºæµ‹è¯•é˜Ÿåˆ—
        self.test_queue.sort(key=lambda t: t.priority, reverse=True)
        
        # å¹¶å‘æ‰§è¡Œæµ‹è¯•
        tasks = []
        
        for test_case in self.test_queue:
            # é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹
            node = self.select_optimal_node(test_case)
            
            if node:
                # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
                task = asyncio.create_task(
                    self.execute_test_on_node(test_case, node)
                )
                tasks.append(task)
            else:
                print(f"âš ï¸ æ— å¯ç”¨èŠ‚ç‚¹æ‰§è¡Œæµ‹è¯•: {test_case.test_id}")
        
        # ç­‰å¾…æ‰€æœ‰æµ‹è¯•å®Œæˆ
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ç»Ÿè®¡æµ‹è¯•ç»“æœ
            self.generate_test_report()
        else:
            print("âŒ æ²¡æœ‰å¯æ‰§è¡Œçš„æµ‹è¯•ç”¨ä¾‹")
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\\nğŸ“Š ç”Ÿæˆåˆ†å¸ƒå¼æµ‹è¯•æŠ¥å‘Š")
        
        total_tests = len(self.test_results)
        successful_tests = sum(1 for result in self.test_results.values() 
                              if result.get('status') == 'completed')
        failed_tests = total_tests - successful_tests
        
        report = {
            'summary': {
                'total_tests': total_tests,
                'successful_tests': successful_tests,
                'failed_tests': failed_tests,
                'success_rate': f"{(successful_tests/total_tests*100):.1f}%" if total_tests > 0 else "0%"
            },
            'node_utilization': {},
            'test_results': self.test_results
        }
        
        # èŠ‚ç‚¹åˆ©ç”¨ç‡ç»Ÿè®¡
        for node_id, node in self.test_nodes.items():
            node_tests = [r for r in self.test_results.values() if r.get('node_id') == node_id]
            report['node_utilization'][node_id] = {
                'platform': node.platform,
                'tests_executed': len(node_tests),
                'status': node.status
            }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = Path("distributed_test_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        print(f"ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡: {successful_tests}/{total_tests} æˆåŠŸ ({report['summary']['success_rate']})")

class E2ETestIntegrator:
    """ç«¯åˆ°ç«¯æµ‹è¯•é›†æˆå™¨"""
    
    def __init__(self):
        self.coordinator = DistributedTestCoordinator()
        self.e2e_test_dir = Path("e2e_tests_complete")
    
    def load_e2e_tests(self) -> List[DistributedTestCase]:
        """åŠ è½½ç«¯åˆ°ç«¯æµ‹è¯•ç”¨ä¾‹"""
        test_cases = []
        
        # æ‰«ææµ‹è¯•ç›®å½•
        for test_type_dir in self.e2e_test_dir.iterdir():
            if test_type_dir.is_dir():
                test_type = test_type_dir.name
                
                # æŸ¥æ‰¾æµ‹è¯•è„šæœ¬
                for test_script in test_type_dir.glob("test_*.py"):
                    test_case = DistributedTestCase(
                        test_id=f"E2E_{test_type.upper()}_{len(test_cases)+1:03d}",
                        test_name=f"{test_type} ç«¯åˆ°ç«¯æµ‹è¯•",
                        test_type="E2E",
                        target_platforms=["mac", "windows", "linux"],
                        test_script_path=str(test_script),
                        environment_requirements={
                            "python_version": ">=3.8",
                            "required_packages": ["requests", "unittest", "json"]
                        },
                        estimated_duration=30,  # 30ç§’
                        priority=3
                    )
                    test_cases.append(test_case)
        
        return test_cases
    
    async def run_integrated_tests(self):
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        print("ğŸš€ PowerAutomation åˆ†å¸ƒå¼ç«¯åˆ°ç«¯æµ‹è¯•é›†æˆ")
        print("=" * 60)
        
        # åŠ è½½ç«¯åˆ°ç«¯æµ‹è¯•ç”¨ä¾‹
        test_cases = self.load_e2e_tests()
        print(f"ğŸ“‹ åŠ è½½æµ‹è¯•ç”¨ä¾‹: {len(test_cases)} ä¸ª")
        
        # æ·»åŠ æµ‹è¯•ç”¨ä¾‹åˆ°åè°ƒå™¨
        for test_case in test_cases:
            self.coordinator.add_test_case(test_case)
        
        # è¿è¡Œåˆ†å¸ƒå¼æµ‹è¯•
        await self.coordinator.run_distributed_tests()
        
        print("\\nğŸ‰ åˆ†å¸ƒå¼ç«¯åˆ°ç«¯æµ‹è¯•å®Œæˆ!")

async def main():
    """ä¸»å‡½æ•°"""
    integrator = E2ETestIntegrator()
    await integrator.run_integrated_tests()

if __name__ == "__main__":
    asyncio.run(main())

