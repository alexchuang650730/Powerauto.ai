#!/usr/bin/env python3
"""
åå±¤æ¸¬è©¦æ¶æ§‹CLIé©…å‹•ç¨‹åº
çµ±ä¸€çš„å‘½ä»¤è¡Œæ¥å£ï¼Œç”¨æ–¼é‹è¡Œå„å±¤ç´šæ¸¬è©¦

æ”¯æŒåŠŸèƒ½ï¼š
- é‹è¡Œå–®å€‹å±¤ç´šæ¸¬è©¦
- é‹è¡Œå¤šå€‹å±¤ç´šæ¸¬è©¦
- é‹è¡Œæ‰€æœ‰å±¤ç´šæ¸¬è©¦
- ç”Ÿæˆç¶œåˆå ±å‘Š
- æ¸¬è©¦çµæœç®¡ç†
"""

import sys
import os
import argparse
import time
import json
import logging
from typing import Dict, Any, List, Optional
from pathlib import Path
from datetime import datetime

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å°å…¥æ¸¬è©¦æ¡†æ¶
from test.standardized_test_interface import StandardizedTestRunner
from test.optimized_module_importer import import_test_framework

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TenLayerTestCLI:
    """åå±¤æ¸¬è©¦æ¶æ§‹CLI"""
    
    def __init__(self):
        self.test_runner = StandardizedTestRunner()
        self.available_levels = list(range(1, 11))
        self.level_descriptions = {
            1: "å–®å…ƒæ¸¬è©¦ + ä»£ç¢¼è³ªé‡ (åŸºç¤å±¤)",
            2: "é›†æˆæ¸¬è©¦ + æ™ºèƒ½é«”å”ä½œ (æ¥­å‹™å±¤)",
            3: "MCPåˆè¦æ¸¬è©¦ + æ¨™æº–åŒ–é©—è­‰ (æ¥­å‹™å±¤)",
            4: "ç«¯åˆ°ç«¯æ¸¬è©¦ + ç”¨æˆ¶å ´æ™¯ (æ¥­å‹™å±¤)",
            5: "æ€§èƒ½æ¸¬è©¦ + å››å±¤å…œåº•æ€§èƒ½ (æˆ°è¡“å±¤)",
            6: "å®‰å…¨æ¸¬è©¦ + ä¼æ¥­ç´šå®‰å…¨ (æˆ°è¡“å±¤)",
            7: "å…¼å®¹æ€§æ¸¬è©¦ + ç·¨è¼¯å™¨é›†æˆ (æˆ°è¡“å±¤)",
            8: "å£“åŠ›æ¸¬è©¦ + è­·åŸæ²³é©—è­‰ (æˆ°ç•¥å±¤)",
            9: "GAIAåŸºæº–æ¸¬è©¦ + ç«¶å°æ¯”è¼ƒ (æˆ°ç•¥å±¤)",
            10: "AIèƒ½åŠ›è©•ä¼° + æ¨™æº–åŸºæº–æ¸¬è©¦ (æˆ°ç•¥å±¤)"
        }
        
        # å·²å¯¦ç¾çš„æ¸¬è©¦æ¡†æ¶
        self.implemented_frameworks = {
            1: "test.level1.enhanced_unit_test_framework",
            2: "test.level2.enhanced_integration_test_framework", 
            3: "test.level3.enhanced_mcp_compliance_framework"
        }
    
    def run_level_test(self, level: int, adapter_name: Optional[str] = None) -> Dict[str, Any]:
        """é‹è¡ŒæŒ‡å®šå±¤ç´šçš„æ¸¬è©¦"""
        if level not in self.available_levels:
            raise ValueError(f"ç„¡æ•ˆçš„æ¸¬è©¦å±¤ç´š: {level}")
        
        print(f"ğŸš€ é–‹å§‹é‹è¡ŒLevel {level}æ¸¬è©¦...")
        print(f"ğŸ“‹ {self.level_descriptions[level]}")
        
        if level not in self.implemented_frameworks:
            print(f"âš ï¸  Level {level}æ¸¬è©¦æ¡†æ¶å°šæœªå¯¦ç¾")
            return {
                "level": level,
                "status": "not_implemented",
                "message": f"Level {level}æ¸¬è©¦æ¡†æ¶å°šæœªå¯¦ç¾"
            }
        
        try:
            # å‹•æ…‹å°å…¥æ¸¬è©¦æ¡†æ¶
            framework_module = self.implemented_frameworks[level]
            
            if level == 1:
                from test.level1.enhanced_unit_test_framework import Level1UnitTestFramework
                framework = Level1UnitTestFramework()
            elif level == 2:
                from test.level2.enhanced_integration_test_framework import Level2IntegrationTestFramework
                framework = Level2IntegrationTestFramework()
            elif level == 3:
                from test.level3.enhanced_mcp_compliance_framework import Level3MCPComplianceFramework
                framework = Level3MCPComplianceFramework()
            else:
                raise ImportError(f"æœªæ‰¾åˆ°Level {level}çš„æ¸¬è©¦æ¡†æ¶")
            
            # é‹è¡Œæ¸¬è©¦
            start_time = time.time()
            results = framework.run_tests(adapter_name)
            execution_time = time.time() - start_time
            
            # ç²å–æ¸¬è©¦æ‘˜è¦
            summary = framework.get_test_summary()
            
            # ä¿å­˜çµæœ
            framework.save_results(f"test/level{level}")
            
            print(f"âœ… Level {level}æ¸¬è©¦å®Œæˆ")
            print(f"ğŸ“Š æ¸¬è©¦æ‘˜è¦: {summary['passed_tests']}/{summary['total_tests']} é€šé ({summary['pass_rate']:.1%})")
            print(f"â±ï¸  åŸ·è¡Œæ™‚é–“: {execution_time:.2f}ç§’")
            
            return {
                "level": level,
                "status": "completed",
                "summary": summary,
                "execution_time": execution_time,
                "results_count": len(results)
            }
            
        except Exception as e:
            error_msg = f"Level {level}æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {str(e)}"
            print(f"âŒ {error_msg}")
            logger.error(error_msg, exc_info=True)
            
            return {
                "level": level,
                "status": "error",
                "error": error_msg
            }
    
    def run_multiple_levels(self, levels: List[int], adapter_name: Optional[str] = None) -> Dict[str, Any]:
        """é‹è¡Œå¤šå€‹å±¤ç´šçš„æ¸¬è©¦"""
        print(f"ğŸ¯ é–‹å§‹é‹è¡Œå¤šå±¤ç´šæ¸¬è©¦: Level {', '.join(map(str, levels))}")
        
        results = {}
        total_start_time = time.time()
        
        for level in levels:
            print(f"\n{'='*60}")
            result = self.run_level_test(level, adapter_name)
            results[f"level_{level}"] = result
        
        total_execution_time = time.time() - total_start_time
        
        # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
        completed_levels = [r for r in results.values() if r["status"] == "completed"]
        total_tests = sum(r["summary"]["total_tests"] for r in completed_levels)
        total_passed = sum(r["summary"]["passed_tests"] for r in completed_levels)
        overall_pass_rate = total_passed / total_tests if total_tests > 0 else 0
        
        print(f"\n{'='*60}")
        print(f"ğŸ† å¤šå±¤ç´šæ¸¬è©¦å®Œæˆ")
        print(f"ğŸ“Š ç¸½é«”çµ±è¨ˆ:")
        print(f"   å®Œæˆå±¤ç´š: {len(completed_levels)}/{len(levels)}")
        print(f"   ç¸½æ¸¬è©¦æ•¸: {total_tests}")
        print(f"   ç¸½é€šéæ•¸: {total_passed}")
        print(f"   ç¸½é«”é€šéç‡: {overall_pass_rate:.1%}")
        print(f"   ç¸½åŸ·è¡Œæ™‚é–“: {total_execution_time:.2f}ç§’")
        
        return {
            "levels": levels,
            "results": results,
            "summary": {
                "completed_levels": len(completed_levels),
                "total_levels": len(levels),
                "total_tests": total_tests,
                "total_passed": total_passed,
                "overall_pass_rate": overall_pass_rate,
                "total_execution_time": total_execution_time
            }
        }
    
    def run_all_implemented_tests(self, adapter_name: Optional[str] = None) -> Dict[str, Any]:
        """é‹è¡Œæ‰€æœ‰å·²å¯¦ç¾çš„æ¸¬è©¦"""
        implemented_levels = list(self.implemented_frameworks.keys())
        return self.run_multiple_levels(implemented_levels, adapter_name)
    
    def list_available_levels(self):
        """åˆ—å‡ºå¯ç”¨çš„æ¸¬è©¦å±¤ç´š"""
        print("ğŸ“‹ å¯ç”¨çš„æ¸¬è©¦å±¤ç´š:")
        print()
        
        for level in self.available_levels:
            status = "âœ… å·²å¯¦ç¾" if level in self.implemented_frameworks else "â³ å¾…å¯¦ç¾"
            print(f"Level {level:2d}: {self.level_descriptions[level]}")
            print(f"          {status}")
            print()
    
    def generate_comprehensive_report(self, results: Dict[str, Any], output_file: Optional[str] = None):
        """ç”Ÿæˆç¶œåˆæ¸¬è©¦å ±å‘Š"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"comprehensive_test_report_{timestamp}.md"
        
        report = f"""# PowerAutomation åå±¤æ¸¬è©¦æ¶æ§‹ç¶œåˆå ±å‘Š

## ğŸ“Š æ¸¬è©¦åŸ·è¡Œæ‘˜è¦
- **æ¸¬è©¦æ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æ¸¬è©¦å±¤ç´š**: {', '.join(f"Level {level}" for level in results.get('levels', []))}

"""
        
        if "summary" in results:
            summary = results["summary"]
            report += f"""## ğŸ¯ ç¸½é«”çµ±è¨ˆ
- **å®Œæˆå±¤ç´š**: {summary['completed_levels']}/{summary['total_levels']}
- **ç¸½æ¸¬è©¦æ•¸**: {summary['total_tests']}
- **ç¸½é€šéæ•¸**: {summary['total_passed']}
- **ç¸½é«”é€šéç‡**: {summary['overall_pass_rate']:.1%}
- **ç¸½åŸ·è¡Œæ™‚é–“**: {summary['total_execution_time']:.2f}ç§’

"""
        
        # å„å±¤ç´šè©³ç´°çµæœ
        report += "## ğŸ“‹ å„å±¤ç´šæ¸¬è©¦çµæœ\n\n"
        
        for level_key, level_result in results.get("results", {}).items():
            level_num = level_key.split("_")[1]
            level_desc = self.level_descriptions.get(int(level_num), "æœªçŸ¥å±¤ç´š")
            
            if level_result["status"] == "completed":
                summary = level_result["summary"]
                status_icon = "âœ…" if summary["pass_rate"] >= 0.8 else "âš ï¸" if summary["pass_rate"] >= 0.6 else "âŒ"
                
                report += f"""### {status_icon} Level {level_num}: {level_desc}
- **é€šéç‡**: {summary['pass_rate']:.1%} ({summary['passed_tests']}/{summary['total_tests']})
- **ç¸½é«”åˆ†æ•¸**: {summary['overall_score']:.1f}
- **åŸ·è¡Œæ™‚é–“**: {level_result['execution_time']:.2f}ç§’

"""
            elif level_result["status"] == "not_implemented":
                report += f"""### â³ Level {level_num}: {level_desc}
- **ç‹€æ…‹**: å°šæœªå¯¦ç¾

"""
            else:
                report += f"""### âŒ Level {level_num}: {level_desc}
- **ç‹€æ…‹**: åŸ·è¡Œå¤±æ•—
- **éŒ¯èª¤**: {level_result.get('error', 'æœªçŸ¥éŒ¯èª¤')}

"""
        
        # ä¿å­˜å ±å‘Š
        output_path = Path(output_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“„ ç¶œåˆæ¸¬è©¦å ±å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        return output_path

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(
        description="PowerAutomation åå±¤æ¸¬è©¦æ¶æ§‹CLI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s --list                    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨å±¤ç´š
  %(prog)s --level 1                 # é‹è¡ŒLevel 1æ¸¬è©¦
  %(prog)s --level 1 2 3             # é‹è¡ŒLevel 1-3æ¸¬è©¦
  %(prog)s --all                     # é‹è¡Œæ‰€æœ‰å·²å¯¦ç¾çš„æ¸¬è©¦
  %(prog)s --level 1 --adapter simple_gemini  # æ¸¬è©¦ç‰¹å®šé©é…å™¨
        """
    )
    
    parser.add_argument(
        "--level", "-l",
        type=int,
        nargs="+",
        help="è¦é‹è¡Œçš„æ¸¬è©¦å±¤ç´š (1-10)"
    )
    
    parser.add_argument(
        "--all", "-a",
        action="store_true",
        help="é‹è¡Œæ‰€æœ‰å·²å¯¦ç¾çš„æ¸¬è©¦"
    )
    
    parser.add_argument(
        "--list",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¸¬è©¦å±¤ç´š"
    )
    
    parser.add_argument(
        "--adapter",
        type=str,
        help="æŒ‡å®šè¦æ¸¬è©¦çš„é©é…å™¨åç¨±"
    )
    
    parser.add_argument(
        "--report", "-r",
        type=str,
        help="ç”Ÿæˆç¶œåˆå ±å‘Šçš„æ–‡ä»¶å"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="è©³ç´°è¼¸å‡º"
    )
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥èªŒç´šåˆ¥
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # å‰µå»ºCLIå¯¦ä¾‹
    cli = TenLayerTestCLI()
    
    try:
        if args.list:
            cli.list_available_levels()
            
        elif args.all:
            results = cli.run_all_implemented_tests(args.adapter)
            if args.report:
                cli.generate_comprehensive_report(results, args.report)
                
        elif args.level:
            if len(args.level) == 1:
                result = cli.run_level_test(args.level[0], args.adapter)
            else:
                results = cli.run_multiple_levels(args.level, args.adapter)
                if args.report:
                    cli.generate_comprehensive_report(results, args.report)
        else:
            parser.print_help()
            
    except KeyboardInterrupt:
        print("\nâš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()

