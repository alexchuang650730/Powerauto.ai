#!/usr/bin/env python3
"""
Level 3 MCPåˆè¦æ¸¬è©¦æ¡†æ¶ - ä¿®å¾©ç‰ˆ
æ¥­å‹™å±¤ï¼šMCPåˆè¦æ¸¬è©¦ + æ¨™æº–åŒ–é©—è­‰

ä¸»è¦åŠŸèƒ½ï¼š
- MCPå”è­°åˆè¦æ€§é©—è­‰
- æ¨™æº–åŒ–æ¸¬è©¦æ¡†æ¶
- æ€§èƒ½åˆè¦æ€§æ¸¬è©¦
- éŒ¯èª¤è™•ç†æ¸¬è©¦
- èƒ½åŠ›è²æ˜æ¨™æº–åŒ–

ä¿®å¾©å•é¡Œï¼š
- çµ±ä¸€é©é…å™¨è¨»å†Šè¡¨æ ¼å¼
- æ¨™æº–åŒ–æ¸¬è©¦æ–¹æ³•æ¥å£
- å„ªåŒ–æ¨¡çµ„å°å…¥æ©Ÿåˆ¶
"""

import sys
import os
import time
import json
import logging
from typing import Dict, Any, List, Optional, Union
from pathlib import Path
from datetime import datetime
import jsonschema

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å°å…¥æ¨™æº–åŒ–æ¥å£
from test.standardized_test_interface import BaseTestFramework, TestResult, TestStatus, TestSeverity
from test.optimized_module_importer import get_safe_importer

logger = logging.getLogger(__name__)

class Level3MCPComplianceFramework(BaseTestFramework):
    """Level 3 MCPåˆè¦æ¸¬è©¦æ¡†æ¶ - ä¿®å¾©ç‰ˆ"""
    
    def __init__(self):
        super().__init__(
            name="Level3_MCPCompliance",
            description="æ¥­å‹™å±¤MCPåˆè¦æ¸¬è©¦å’Œæ¨™æº–åŒ–é©—è­‰"
        )
        
        self.importer = get_safe_importer()
        
        # MCPå”è­°æ¨™æº–
        self.mcp_standards = {
            "protocol_version": "1.0",
            "required_methods": ["get_name", "get_capabilities"],
            "optional_methods": ["process", "get_version", "get_description", "get_metadata"],
            "response_format": {
                "type": "object",
                "properties": {
                    "status": {"type": "string"},
                    "data": {"type": ["object", "array", "string", "number", "boolean", "null"]},
                    "timestamp": {"type": "string"}
                }
            },
            "capability_schema": {
                "type": "array",
                "items": {"type": "string"}
            }
        }
        
        # æ¸¬è©¦é…ç½®
        self.test_config = {
            "timeout": 45.0,
            "performance_thresholds": {
                "response_time": 2.0,  # ç§’
                "throughput": 5.0,     # æ“ä½œ/ç§’
                "memory_limit": 100    # MB
            },
            "compliance_threshold": 80.0  # åˆè¦æ€§é–¾å€¼
        }
    
    def run_tests(self, adapter_name: Optional[str] = None) -> List[TestResult]:
        """é‹è¡ŒLevel 3 MCPåˆè¦æ¸¬è©¦"""
        self.logger.info("é–‹å§‹Level 3 MCPåˆè¦æ¸¬è©¦...")
        self.test_results.clear()
        
        # ç²å–é©é…å™¨åˆ—è¡¨
        adapters = self.get_adapters()
        
        if not adapters:
            self.logger.warning("æœªæ‰¾åˆ°ä»»ä½•é©é…å™¨")
            return []
        
        # éæ¿¾é©é…å™¨
        if adapter_name:
            adapters = [(name, instance) for name, instance in adapters if name == adapter_name]
        
        self.logger.info(f"æ¸¬è©¦ {len(adapters)} å€‹é©é…å™¨çš„MCPåˆè¦æ€§")
        
        # é‹è¡Œåˆè¦æ¸¬è©¦
        for name, instance in adapters:
            self._test_adapter_compliance(name, instance)
        
        self.logger.info(f"Level 3æ¸¬è©¦å®Œæˆï¼Œå…± {len(self.test_results)} å€‹æ¸¬è©¦çµæœ")
        return self.test_results
    
    def _test_adapter_compliance(self, adapter_name: str, adapter_instance: Any):
        """æ¸¬è©¦å–®å€‹é©é…å™¨çš„MCPåˆè¦æ€§"""
        self.logger.debug(f"æ¸¬è©¦é©é…å™¨MCPåˆè¦æ€§: {adapter_name}")
        
        # 1. å”è­°é©—è­‰æ¸¬è©¦
        self._test_protocol_compliance(adapter_name, adapter_instance)
        
        # 2. æ¨™æº–åŒ–æ¸¬è©¦
        self._test_standardization_compliance(adapter_name, adapter_instance)
        
        # 3. æ€§èƒ½åˆè¦æ€§æ¸¬è©¦
        self._test_performance_compliance(adapter_name, adapter_instance)
        
        # 4. éŒ¯èª¤è™•ç†æ¸¬è©¦
        self._test_error_handling_compliance(adapter_name, adapter_instance)
        
        # 5. èƒ½åŠ›è²æ˜æ¨™æº–åŒ–æ¸¬è©¦
        self._test_capability_standardization(adapter_name, adapter_instance)
    
    def _test_protocol_compliance(self, adapter_name: str, adapter_instance: Any):
        """æ¸¬è©¦å”è­°åˆè¦æ€§"""
        start_time = time.time()
        
        compliance_checks = {}
        total_score = 0
        
        # 1. æª¢æŸ¥å¿…éœ€æ–¹æ³•
        required_methods = self.mcp_standards["required_methods"]
        missing_methods = []
        
        for method_name in required_methods:
            if hasattr(adapter_instance, method_name):
                compliance_checks[f"has_{method_name}"] = True
                total_score += 25
            else:
                compliance_checks[f"has_{method_name}"] = False
                missing_methods.append(method_name)
        
        # 2. æª¢æŸ¥å¯é¸æ–¹æ³•
        optional_methods = self.mcp_standards["optional_methods"]
        available_optional = []
        
        for method_name in optional_methods:
            if hasattr(adapter_instance, method_name):
                available_optional.append(method_name)
                compliance_checks[f"has_{method_name}"] = True
                total_score += 5
        
        # 3. æ¸¬è©¦æ–¹æ³•èª¿ç”¨åˆè¦æ€§
        method_compliance_score = self._test_method_compliance(adapter_instance)
        compliance_checks["method_compliance"] = method_compliance_score
        total_score += method_compliance_score * 0.5
        
        # è¨ˆç®—æœ€çµ‚åˆ†æ•¸
        final_score = min(total_score, 100)
        passed = final_score >= self.test_config["compliance_threshold"]
        
        execution_time = time.time() - start_time
        
        message = f"å”è­°åˆè¦æ€§: {final_score:.1f}% (ç¼ºå¤±æ–¹æ³•: {len(missing_methods)})"
        
        result = self.create_test_result(
            test_name="test_compliance_protocol",
            adapter_name=adapter_name,
            passed=passed,
            score=final_score,
            execution_time=execution_time,
            message=message,
            details={
                "compliance_checks": compliance_checks,
                "missing_methods": missing_methods,
                "available_optional": available_optional,
                "method_compliance_score": method_compliance_score
            },
            severity=TestSeverity.HIGH
        )
        
        self.add_test_result(result)
    
    def _test_method_compliance(self, adapter_instance: Any) -> float:
        """æ¸¬è©¦æ–¹æ³•èª¿ç”¨åˆè¦æ€§"""
        score = 0
        
        # æ¸¬è©¦get_nameæ–¹æ³•
        if hasattr(adapter_instance, "get_name"):
            try:
                name = adapter_instance.get_name()
                if isinstance(name, str) and len(name) > 0:
                    score += 30
                elif name is not None:
                    score += 15
            except Exception as e:
                self.logger.debug(f"get_nameæ–¹æ³•èª¿ç”¨å¤±æ•—: {e}")
        
        # æ¸¬è©¦get_capabilitiesæ–¹æ³•
        if hasattr(adapter_instance, "get_capabilities"):
            try:
                capabilities = adapter_instance.get_capabilities()
                
                # é©—è­‰èƒ½åŠ›æ ¼å¼
                if self._validate_capabilities_format(capabilities):
                    score += 40
                elif capabilities is not None:
                    score += 20
            except Exception as e:
                self.logger.debug(f"get_capabilitiesæ–¹æ³•èª¿ç”¨å¤±æ•—: {e}")
        
        # æ¸¬è©¦processæ–¹æ³•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(adapter_instance, "process"):
            try:
                test_data = {"test": "compliance_check"}
                result = adapter_instance.process(test_data)
                
                # é©—è­‰éŸ¿æ‡‰æ ¼å¼
                if self._validate_response_format(result):
                    score += 30
                elif result is not None:
                    score += 15
            except Exception as e:
                self.logger.debug(f"processæ–¹æ³•èª¿ç”¨å¤±æ•—: {e}")
        
        return score
    
    def _validate_capabilities_format(self, capabilities: Any) -> bool:
        """é©—è­‰èƒ½åŠ›æ ¼å¼"""
        try:
            jsonschema.validate(capabilities, self.mcp_standards["capability_schema"])
            return True
        except:
            # å¦‚æœä¸æ˜¯æ¨™æº–æ ¼å¼ï¼Œæª¢æŸ¥æ˜¯å¦ç‚ºåˆç†çš„æ›¿ä»£æ ¼å¼
            if isinstance(capabilities, (list, dict)) and len(capabilities) > 0:
                return True
            return False
    
    def _validate_response_format(self, response: Any) -> bool:
        """é©—è­‰éŸ¿æ‡‰æ ¼å¼"""
        if response is None:
            return False
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå­—å…¸æ ¼å¼
        if isinstance(response, dict):
            # æª¢æŸ¥æ˜¯å¦åŒ…å«åŸºæœ¬å­—æ®µ
            if "status" in response or "data" in response:
                return True
        
        # ä»»ä½•éNoneéŸ¿æ‡‰éƒ½è¢«èªç‚ºæ˜¯æœ‰æ•ˆçš„
        return True
    
    def _test_standardization_compliance(self, adapter_name: str, adapter_instance: Any):
        """æ¸¬è©¦æ¨™æº–åŒ–åˆè¦æ€§"""
        start_time = time.time()
        
        standardization_checks = {}
        total_score = 0
        
        # 1. å‘½åæ¨™æº–æª¢æŸ¥
        naming_score = self._check_naming_standards(adapter_instance)
        standardization_checks["naming"] = naming_score
        total_score += naming_score * 0.3
        
        # 2. æ¥å£æ¨™æº–æª¢æŸ¥
        interface_score = self._check_interface_standards(adapter_instance)
        standardization_checks["interface"] = interface_score
        total_score += interface_score * 0.4
        
        # 3. æ–‡æª”æ¨™æº–æª¢æŸ¥
        documentation_score = self._check_documentation_standards(adapter_instance)
        standardization_checks["documentation"] = documentation_score
        total_score += documentation_score * 0.3
        
        passed = total_score >= 70
        execution_time = time.time() - start_time
        
        message = f"æ¨™æº–åŒ–åˆè¦æ€§: {total_score:.1f}%"
        
        result = self.create_test_result(
            test_name="test_compliance_standardization",
            adapter_name=adapter_name,
            passed=passed,
            score=total_score,
            execution_time=execution_time,
            message=message,
            details=standardization_checks,
            severity=TestSeverity.MEDIUM
        )
        
        self.add_test_result(result)
    
    def _check_naming_standards(self, adapter_instance: Any) -> float:
        """æª¢æŸ¥å‘½åæ¨™æº–"""
        score = 0
        
        # æª¢æŸ¥é¡å
        class_name = adapter_instance.__class__.__name__
        if any(keyword in class_name.lower() for keyword in ['adapter', 'mcp', 'tool', 'engine']):
            score += 30
        
        if class_name[0].isupper():  # é¦–å­—æ¯å¤§å¯«
            score += 20
        
        # æª¢æŸ¥æ–¹æ³•å
        public_methods = [name for name in dir(adapter_instance) 
                         if callable(getattr(adapter_instance, name)) and not name.startswith('_')]
        
        if public_methods:
            descriptive_count = sum(1 for name in public_methods 
                                  if any(keyword in name.lower() for keyword in ['get', 'set', 'process', 'handle']))
            score += (descriptive_count / len(public_methods)) * 50
        
        return score
    
    def _check_interface_standards(self, adapter_instance: Any) -> float:
        """æª¢æŸ¥æ¥å£æ¨™æº–"""
        score = 0
        
        # æª¢æŸ¥æ¨™æº–æ–¹æ³•ç°½å
        if hasattr(adapter_instance, "get_name"):
            try:
                import inspect
                sig = inspect.signature(adapter_instance.get_name)
                if len(sig.parameters) == 0:  # ç„¡åƒæ•¸
                    score += 25
            except:
                pass
        
        if hasattr(adapter_instance, "get_capabilities"):
            try:
                import inspect
                sig = inspect.signature(adapter_instance.get_capabilities)
                if len(sig.parameters) == 0:  # ç„¡åƒæ•¸
                    score += 25
            except:
                pass
        
        if hasattr(adapter_instance, "process"):
            try:
                import inspect
                sig = inspect.signature(adapter_instance.process)
                if len(sig.parameters) == 1:  # ä¸€å€‹åƒæ•¸ï¼ˆé™¤äº†selfï¼‰
                    score += 25
            except:
                pass
        
        # æª¢æŸ¥è¿”å›å€¼é¡å‹ä¸€è‡´æ€§
        consistency_score = self._check_return_type_consistency(adapter_instance)
        score += consistency_score * 0.25
        
        return score
    
    def _check_return_type_consistency(self, adapter_instance: Any) -> float:
        """æª¢æŸ¥è¿”å›å€¼é¡å‹ä¸€è‡´æ€§"""
        score = 0
        
        # æ¸¬è©¦get_nameçš„ä¸€è‡´æ€§
        if hasattr(adapter_instance, "get_name"):
            try:
                name1 = adapter_instance.get_name()
                name2 = adapter_instance.get_name()
                if type(name1) == type(name2) and name1 == name2:
                    score += 50
            except:
                pass
        
        # æ¸¬è©¦get_capabilitiesçš„ä¸€è‡´æ€§
        if hasattr(adapter_instance, "get_capabilities"):
            try:
                caps1 = adapter_instance.get_capabilities()
                caps2 = adapter_instance.get_capabilities()
                if type(caps1) == type(caps2):
                    score += 50
            except:
                pass
        
        return score
    
    def _check_documentation_standards(self, adapter_instance: Any) -> float:
        """æª¢æŸ¥æ–‡æª”æ¨™æº–"""
        score = 0
        
        # æª¢æŸ¥é¡æ–‡æª”
        class_doc = adapter_instance.__class__.__doc__
        if class_doc and len(class_doc.strip()) > 20:
            score += 50
        elif class_doc:
            score += 25
        
        # æª¢æŸ¥æ–¹æ³•æ–‡æª”
        documented_methods = 0
        total_methods = 0
        
        for name in dir(adapter_instance):
            if callable(getattr(adapter_instance, name)) and not name.startswith('_'):
                total_methods += 1
                method = getattr(adapter_instance, name)
                if hasattr(method, '__doc__') and method.__doc__ and len(method.__doc__.strip()) > 10:
                    documented_methods += 1
        
        if total_methods > 0:
            doc_ratio = documented_methods / total_methods
            score += doc_ratio * 50
        
        return score
    
    def _test_performance_compliance(self, adapter_name: str, adapter_instance: Any):
        """æ¸¬è©¦æ€§èƒ½åˆè¦æ€§"""
        start_time = time.time()
        
        performance_tests = {}
        total_score = 0
        
        # 1. éŸ¿æ‡‰æ™‚é–“æ¸¬è©¦
        response_time_score = self._test_response_time(adapter_instance)
        performance_tests["response_time"] = response_time_score
        total_score += response_time_score * 0.4
        
        # 2. ååé‡æ¸¬è©¦
        throughput_score = self._test_throughput(adapter_instance)
        performance_tests["throughput"] = throughput_score
        total_score += throughput_score * 0.3
        
        # 3. å…§å­˜ä½¿ç”¨æ¸¬è©¦
        memory_score = self._test_memory_usage(adapter_instance)
        performance_tests["memory"] = memory_score
        total_score += memory_score * 0.3
        
        passed = total_score >= 60
        execution_time = time.time() - start_time
        
        message = f"æ€§èƒ½åˆè¦æ€§: {total_score:.1f}%"
        
        result = self.create_test_result(
            test_name="test_compliance_performance",
            adapter_name=adapter_name,
            passed=passed,
            score=total_score,
            execution_time=execution_time,
            message=message,
            details=performance_tests,
            severity=TestSeverity.MEDIUM
        )
        
        self.add_test_result(result)
    
    def _test_response_time(self, adapter_instance: Any) -> float:
        """æ¸¬è©¦éŸ¿æ‡‰æ™‚é–“"""
        if not hasattr(adapter_instance, "process"):
            return 70  # æ²’æœ‰processæ–¹æ³•ï¼Œçµ¦äºˆåŸºç¤åˆ†æ•¸
        
        response_times = []
        threshold = self.test_config["performance_thresholds"]["response_time"]
        
        for i in range(5):
            start = time.time()
            try:
                adapter_instance.process({"test": f"response_time_{i}"})
                end = time.time()
                response_times.append(end - start)
            except:
                response_times.append(threshold * 2)  # éŒ¯èª¤æ™‚è¨˜éŒ„è¶…æ™‚
        
        if response_times:
            avg_response_time = sum(response_times) / len(response_times)
            
            if avg_response_time <= threshold:
                return 100
            elif avg_response_time <= threshold * 2:
                return 70
            elif avg_response_time <= threshold * 5:
                return 40
            else:
                return 20
        
        return 50
    
    def _test_throughput(self, adapter_instance: Any) -> float:
        """æ¸¬è©¦ååé‡"""
        if not hasattr(adapter_instance, "process"):
            return 70  # æ²’æœ‰processæ–¹æ³•ï¼Œçµ¦äºˆåŸºç¤åˆ†æ•¸
        
        start_time = time.time()
        operations = 0
        test_duration = 3.0  # 3ç§’æ¸¬è©¦
        
        while time.time() - start_time < test_duration:
            try:
                adapter_instance.process({"test": "throughput", "op": operations})
                operations += 1
            except:
                break
        
        actual_duration = time.time() - start_time
        throughput = operations / actual_duration if actual_duration > 0 else 0
        
        threshold = self.test_config["performance_thresholds"]["throughput"]
        
        if throughput >= threshold:
            return 100
        elif throughput >= threshold * 0.7:
            return 80
        elif throughput >= threshold * 0.4:
            return 60
        elif throughput > 0:
            return 40
        else:
            return 20
    
    def _test_memory_usage(self, adapter_instance: Any) -> float:
        """æ¸¬è©¦å…§å­˜ä½¿ç”¨"""
        try:
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # åŸ·è¡Œå¤šæ¬¡æ“ä½œ
            for i in range(20):
                try:
                    if hasattr(adapter_instance, "process"):
                        adapter_instance.process({"test": "memory", "iteration": i})
                    elif hasattr(adapter_instance, "get_capabilities"):
                        adapter_instance.get_capabilities()
                except:
                    pass
            
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = final_memory - initial_memory
            
            threshold = self.test_config["performance_thresholds"]["memory_limit"]
            
            if memory_increase <= threshold * 0.1:
                return 100
            elif memory_increase <= threshold * 0.3:
                return 80
            elif memory_increase <= threshold:
                return 60
            else:
                return 30
                
        except ImportError:
            return 70  # ç„¡æ³•æ¸¬è©¦å…§å­˜ï¼Œçµ¦äºˆåŸºç¤åˆ†æ•¸
        except:
            return 50
    
    def _test_error_handling_compliance(self, adapter_name: str, adapter_instance: Any):
        """æ¸¬è©¦éŒ¯èª¤è™•ç†åˆè¦æ€§"""
        start_time = time.time()
        
        error_tests = {}
        total_score = 0
        
        # 1. ç•°å¸¸è¼¸å…¥è™•ç†
        exception_handling_score = self._test_exception_handling(adapter_instance)
        error_tests["exception_handling"] = exception_handling_score
        total_score += exception_handling_score * 0.4
        
        # 2. éŒ¯èª¤æ¢å¾©èƒ½åŠ›
        recovery_score = self._test_error_recovery(adapter_instance)
        error_tests["recovery"] = recovery_score
        total_score += recovery_score * 0.3
        
        # 3. éŒ¯èª¤å ±å‘Šè³ªé‡
        error_reporting_score = self._test_error_reporting(adapter_instance)
        error_tests["error_reporting"] = error_reporting_score
        total_score += error_reporting_score * 0.3
        
        passed = total_score >= 65
        execution_time = time.time() - start_time
        
        message = f"éŒ¯èª¤è™•ç†åˆè¦æ€§: {total_score:.1f}%"
        
        result = self.create_test_result(
            test_name="test_compliance_error_handling",
            adapter_name=adapter_name,
            passed=passed,
            score=total_score,
            execution_time=execution_time,
            message=message,
            details=error_tests,
            severity=TestSeverity.MEDIUM
        )
        
        self.add_test_result(result)
    
    def _test_exception_handling(self, adapter_instance: Any) -> float:
        """æ¸¬è©¦ç•°å¸¸è™•ç†"""
        score = 0
        test_cases = [None, {}, [], "", 0, -1, "invalid"]
        
        for test_case in test_cases:
            if hasattr(adapter_instance, "process"):
                try:
                    result = adapter_instance.process(test_case)
                    # èƒ½è™•ç†ç•°å¸¸è¼¸å…¥è€Œä¸å´©æ½°
                    score += 10
                except Exception:
                    # æ‹‹å‡ºç•°å¸¸ä¹Ÿæ˜¯ä¸€ç¨®è™•ç†æ–¹å¼
                    score += 8
        
        # æ¸¬è©¦æ–¹æ³•èª¿ç”¨ç•°å¸¸
        for method_name in ["get_name", "get_capabilities"]:
            if hasattr(adapter_instance, method_name):
                try:
                    method = getattr(adapter_instance, method_name)
                    result = method()
                    score += 10
                except Exception:
                    score += 5
        
        return min(score, 100)
    
    def _test_error_recovery(self, adapter_instance: Any) -> float:
        """æ¸¬è©¦éŒ¯èª¤æ¢å¾©"""
        score = 0
        
        if hasattr(adapter_instance, "process"):
            # å…ˆé€ æˆéŒ¯èª¤
            try:
                adapter_instance.process(None)
            except:
                pass
            
            # æ¸¬è©¦æ˜¯å¦èƒ½æ¢å¾©æ­£å¸¸å·¥ä½œ
            try:
                result = adapter_instance.process({"test": "recovery"})
                if result is not None:
                    score += 50
            except:
                pass
            
            # æ¸¬è©¦é€£çºŒéŒ¯èª¤å¾Œçš„æ¢å¾©
            for _ in range(3):
                try:
                    adapter_instance.process("invalid")
                except:
                    pass
            
            try:
                result = adapter_instance.process({"test": "final_recovery"})
                if result is not None:
                    score += 50
            except:
                pass
        else:
            score = 70  # æ²’æœ‰processæ–¹æ³•ï¼Œçµ¦äºˆåŸºç¤åˆ†æ•¸
        
        return score
    
    def _test_error_reporting(self, adapter_instance: Any) -> float:
        """æ¸¬è©¦éŒ¯èª¤å ±å‘Šè³ªé‡"""
        score = 0
        
        if hasattr(adapter_instance, "process"):
            try:
                adapter_instance.process(None)
                score += 30  # æ²’æœ‰æ‹‹å‡ºç•°å¸¸ï¼Œå¯èƒ½æœ‰å…§éƒ¨éŒ¯èª¤è™•ç†
            except Exception as e:
                error_msg = str(e)
                
                # æª¢æŸ¥éŒ¯èª¤æ¶ˆæ¯è³ªé‡
                if len(error_msg) > 10:
                    score += 40
                elif len(error_msg) > 0:
                    score += 20
                
                # æª¢æŸ¥æ˜¯å¦åŒ…å«æœ‰ç”¨ä¿¡æ¯
                if any(keyword in error_msg.lower() for keyword in ['invalid', 'error', 'none', 'null']):
                    score += 30
                
                # æª¢æŸ¥ç•°å¸¸é¡å‹
                if isinstance(e, (ValueError, TypeError, AttributeError)):
                    score += 30
        else:
            score = 60  # æ²’æœ‰processæ–¹æ³•ï¼Œçµ¦äºˆåŸºç¤åˆ†æ•¸
        
        return min(score, 100)
    
    def _test_capability_standardization(self, adapter_name: str, adapter_instance: Any):
        """æ¸¬è©¦èƒ½åŠ›è²æ˜æ¨™æº–åŒ–"""
        start_time = time.time()
        
        capability_tests = {}
        total_score = 0
        
        # 1. èƒ½åŠ›æ ¼å¼é©—è­‰
        format_score = self._test_capability_format(adapter_instance)
        capability_tests["format"] = format_score
        total_score += format_score * 0.4
        
        # 2. èƒ½åŠ›å…§å®¹é©—è­‰
        content_score = self._test_capability_content(adapter_instance)
        capability_tests["content"] = content_score
        total_score += content_score * 0.3
        
        # 3. èƒ½åŠ›ä¸€è‡´æ€§é©—è­‰
        consistency_score = self._test_capability_consistency(adapter_instance)
        capability_tests["consistency"] = consistency_score
        total_score += consistency_score * 0.3
        
        passed = total_score >= 70
        execution_time = time.time() - start_time
        
        message = f"èƒ½åŠ›è²æ˜æ¨™æº–åŒ–: {total_score:.1f}%"
        
        result = self.create_test_result(
            test_name="test_compliance_capability_standardization",
            adapter_name=adapter_name,
            passed=passed,
            score=total_score,
            execution_time=execution_time,
            message=message,
            details=capability_tests,
            severity=TestSeverity.LOW
        )
        
        self.add_test_result(result)
    
    def _test_capability_format(self, adapter_instance: Any) -> float:
        """æ¸¬è©¦èƒ½åŠ›æ ¼å¼"""
        if not hasattr(adapter_instance, "get_capabilities"):
            return 30  # æ²’æœ‰èƒ½åŠ›æ–¹æ³•ï¼Œçµ¦äºˆä½åˆ†
        
        try:
            capabilities = adapter_instance.get_capabilities()
            
            # æª¢æŸ¥æ ¼å¼
            if isinstance(capabilities, list):
                if all(isinstance(cap, str) for cap in capabilities):
                    return 100  # æ¨™æº–æ ¼å¼
                else:
                    return 70   # åˆ—è¡¨ä½†ä¸å…¨æ˜¯å­—ç¬¦ä¸²
            elif isinstance(capabilities, dict):
                return 80       # å­—å…¸æ ¼å¼ä¹Ÿå¯æ¥å—
            elif isinstance(capabilities, str):
                return 60       # å­—ç¬¦ä¸²æ ¼å¼
            else:
                return 40       # å…¶ä»–æ ¼å¼
                
        except Exception:
            return 20
    
    def _test_capability_content(self, adapter_instance: Any) -> float:
        """æ¸¬è©¦èƒ½åŠ›å…§å®¹"""
        if not hasattr(adapter_instance, "get_capabilities"):
            return 30
        
        try:
            capabilities = adapter_instance.get_capabilities()
            
            if not capabilities:
                return 20  # ç©ºèƒ½åŠ›
            
            score = 50  # åŸºç¤åˆ†æ•¸
            
            # æª¢æŸ¥èƒ½åŠ›æè¿°æ€§
            if isinstance(capabilities, list):
                descriptive_caps = sum(1 for cap in capabilities 
                                     if isinstance(cap, str) and len(cap) > 3)
                if len(capabilities) > 0:
                    score += (descriptive_caps / len(capabilities)) * 50
            elif isinstance(capabilities, dict):
                if len(capabilities) > 0:
                    score += 50
            
            return min(score, 100)
            
        except Exception:
            return 20
    
    def _test_capability_consistency(self, adapter_instance: Any) -> float:
        """æ¸¬è©¦èƒ½åŠ›ä¸€è‡´æ€§"""
        if not hasattr(adapter_instance, "get_capabilities"):
            return 30
        
        try:
            # å¤šæ¬¡èª¿ç”¨æª¢æŸ¥ä¸€è‡´æ€§
            caps1 = adapter_instance.get_capabilities()
            caps2 = adapter_instance.get_capabilities()
            
            if caps1 == caps2:
                return 100
            elif type(caps1) == type(caps2):
                return 70
            else:
                return 40
                
        except Exception:
            return 20

def run_level3_tests(adapter_name: Optional[str] = None) -> List[TestResult]:
    """é‹è¡ŒLevel 3æ¸¬è©¦çš„ä¾¿æ·å‡½æ•¸"""
    framework = Level3MCPComplianceFramework()
    return framework.run_tests(adapter_name)

if __name__ == "__main__":
    # é‹è¡ŒLevel 3æ¸¬è©¦
    print("ğŸ“‹ é–‹å§‹Level 3 MCPåˆè¦æ¸¬è©¦...")
    
    framework = Level3MCPComplianceFramework()
    results = framework.run_tests()
    
    # é¡¯ç¤ºçµæœ
    summary = framework.get_test_summary()
    print(f"ğŸ“Š æ¸¬è©¦å®Œæˆ:")
    print(f"   ç¸½æ¸¬è©¦æ•¸: {summary['total_tests']}")
    print(f"   é€šéæ¸¬è©¦: {summary['passed_tests']}")
    print(f"   å¤±æ•—æ¸¬è©¦: {summary['failed_tests']}")
    print(f"   é€šéç‡: {summary['pass_rate']:.1%}")
    print(f"   ç¸½é«”åˆ†æ•¸: {summary['overall_score']:.1f}")
    print(f"   åŸ·è¡Œæ™‚é–“: {summary['total_time']:.2f}ç§’")
    
    # ä¿å­˜çµæœ
    framework.save_results("test/level3")
    print("ğŸ“„ æ¸¬è©¦å ±å‘Šå·²ä¿å­˜åˆ° test/level3/")
    
    # ç”Ÿæˆç°¡è¦å ±å‘Š
    report = framework.generate_report()
    print("\nğŸ“‹ æ¸¬è©¦å ±å‘Šé è¦½:")
    print(report[:1000] + "..." if len(report) > 1000 else report)

