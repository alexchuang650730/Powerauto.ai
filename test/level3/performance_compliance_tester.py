#!/usr/bin/env python3
"""
MCPæ€§èƒ½åˆè¦æ€§æ¸¬è©¦æ¨¡çµ„
æ¸¬è©¦MCPé©é…å™¨çš„æ€§èƒ½æŒ‡æ¨™å’Œåˆè¦æ€§è¦æ±‚

åŒ…æ‹¬ï¼š
- éŸ¿æ‡‰æ™‚é–“æ¸¬è©¦
- ååé‡æ¸¬è©¦
- ä½µç™¼æ€§èƒ½æ¸¬è©¦
- è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦
- CPUä½¿ç”¨ç‡æ¸¬è©¦
- æ€§èƒ½åŸºæº–å°æ¯”
"""

import sys
import os
import time
import psutil
import threading
import asyncio
import statistics
import logging
from typing import Dict, Any, List, Optional, Callable
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import json

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ¨™"""
    response_time_ms: float
    throughput_rps: float
    memory_usage_mb: float
    cpu_usage_percent: float
    success_rate: float
    error_count: int
    concurrent_requests: int
    test_duration_seconds: float

@dataclass
class PerformanceTestResult:
    """æ€§èƒ½æ¸¬è©¦çµæœ"""
    test_name: str
    adapter_name: str
    passed: bool
    metrics: PerformanceMetrics
    benchmark_comparison: Dict[str, Any]
    details: Dict[str, Any]
    execution_time: float

class MCPPerformanceComplianceTester:
    """MCPæ€§èƒ½åˆè¦æ€§æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.test_results = []
        self.performance_standards = self._load_performance_standards()
        self.baseline_metrics = {}
        
    def _load_performance_standards(self) -> Dict[str, Any]:
        """è¼‰å…¥æ€§èƒ½æ¨™æº–"""
        return {
            "response_time": {
                "excellent": 100,  # ms
                "good": 500,
                "acceptable": 1000,
                "poor": 2000
            },
            "throughput": {
                "excellent": 100,  # requests per second
                "good": 50,
                "acceptable": 20,
                "poor": 10
            },
            "memory_usage": {
                "excellent": 50,   # MB
                "good": 100,
                "acceptable": 200,
                "poor": 500
            },
            "cpu_usage": {
                "excellent": 10,   # percent
                "good": 25,
                "acceptable": 50,
                "poor": 80
            },
            "success_rate": {
                "excellent": 99.9,  # percent
                "good": 99.0,
                "acceptable": 95.0,
                "poor": 90.0
            },
            "concurrent_users": {
                "light": 10,
                "medium": 50,
                "heavy": 100,
                "extreme": 200
            }
        }
    
    def measure_response_time(self, adapter_instance: Any, test_data: Dict[str, Any], iterations: int = 100) -> Dict[str, float]:
        """æ¸¬é‡éŸ¿æ‡‰æ™‚é–“"""
        response_times = []
        errors = 0
        
        for i in range(iterations):
            start_time = time.time()
            try:
                if hasattr(adapter_instance, 'process'):
                    result = adapter_instance.process(test_data)
                else:
                    # å¦‚æœæ²’æœ‰processæ–¹æ³•ï¼Œå˜—è©¦å…¶ä»–æ–¹æ³•
                    if hasattr(adapter_instance, '__call__'):
                        result = adapter_instance(test_data)
                    else:
                        raise AttributeError("No callable method found")
                
                end_time = time.time()
                response_time = (end_time - start_time) * 1000  # è½‰æ›ç‚ºæ¯«ç§’
                response_times.append(response_time)
                
            except Exception as e:
                errors += 1
                logger.warning(f"éŸ¿æ‡‰æ™‚é–“æ¸¬è©¦ç¬¬{i+1}æ¬¡è¿­ä»£å¤±æ•—: {e}")
        
        if not response_times:
            return {
                "avg_response_time": float('inf'),
                "min_response_time": float('inf'),
                "max_response_time": float('inf'),
                "p95_response_time": float('inf'),
                "p99_response_time": float('inf'),
                "error_rate": 100.0
            }
        
        return {
            "avg_response_time": statistics.mean(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "p95_response_time": statistics.quantiles(response_times, n=20)[18] if len(response_times) >= 20 else max(response_times),
            "p99_response_time": statistics.quantiles(response_times, n=100)[98] if len(response_times) >= 100 else max(response_times),
            "error_rate": (errors / iterations) * 100
        }
    
    def measure_throughput(self, adapter_instance: Any, test_data: Dict[str, Any], duration_seconds: int = 30) -> Dict[str, float]:
        """æ¸¬é‡ååé‡"""
        start_time = time.time()
        end_time = start_time + duration_seconds
        request_count = 0
        error_count = 0
        
        while time.time() < end_time:
            try:
                if hasattr(adapter_instance, 'process'):
                    result = adapter_instance.process(test_data)
                else:
                    if hasattr(adapter_instance, '__call__'):
                        result = adapter_instance(test_data)
                    else:
                        raise AttributeError("No callable method found")
                
                request_count += 1
                
            except Exception as e:
                error_count += 1
                logger.warning(f"ååé‡æ¸¬è©¦è«‹æ±‚å¤±æ•—: {e}")
        
        actual_duration = time.time() - start_time
        throughput = request_count / actual_duration if actual_duration > 0 else 0
        
        return {
            "requests_per_second": throughput,
            "total_requests": request_count,
            "total_errors": error_count,
            "test_duration": actual_duration,
            "success_rate": ((request_count - error_count) / request_count * 100) if request_count > 0 else 0
        }
    
    def measure_concurrent_performance(self, adapter_instance: Any, test_data: Dict[str, Any], concurrent_users: int = 10, requests_per_user: int = 10) -> Dict[str, Any]:
        """æ¸¬é‡ä½µç™¼æ€§èƒ½"""
        def worker_function(user_id: int) -> Dict[str, Any]:
            """å·¥ä½œç·šç¨‹å‡½æ•¸"""
            response_times = []
            errors = 0
            
            for i in range(requests_per_user):
                start_time = time.time()
                try:
                    if hasattr(adapter_instance, 'process'):
                        result = adapter_instance.process(test_data)
                    else:
                        if hasattr(adapter_instance, '__call__'):
                            result = adapter_instance(test_data)
                        else:
                            raise AttributeError("No callable method found")
                    
                    end_time = time.time()
                    response_time = (end_time - start_time) * 1000
                    response_times.append(response_time)
                    
                except Exception as e:
                    errors += 1
            
            return {
                "user_id": user_id,
                "response_times": response_times,
                "errors": errors,
                "avg_response_time": statistics.mean(response_times) if response_times else float('inf')
            }
        
        # åŸ·è¡Œä½µç™¼æ¸¬è©¦
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = [executor.submit(worker_function, i) for i in range(concurrent_users)]
            results = [future.result() for future in as_completed(futures)]
        
        end_time = time.time()
        
        # å½™ç¸½çµæœ
        all_response_times = []
        total_errors = 0
        total_requests = 0
        
        for result in results:
            all_response_times.extend(result["response_times"])
            total_errors += result["errors"]
            total_requests += len(result["response_times"]) + result["errors"]
        
        return {
            "concurrent_users": concurrent_users,
            "total_requests": total_requests,
            "total_errors": total_errors,
            "success_rate": ((total_requests - total_errors) / total_requests * 100) if total_requests > 0 else 0,
            "avg_response_time": statistics.mean(all_response_times) if all_response_times else float('inf'),
            "max_response_time": max(all_response_times) if all_response_times else float('inf'),
            "min_response_time": min(all_response_times) if all_response_times else float('inf'),
            "test_duration": end_time - start_time,
            "throughput": total_requests / (end_time - start_time) if (end_time - start_time) > 0 else 0
        }
    
    def measure_resource_usage(self, adapter_instance: Any, test_data: Dict[str, Any], duration_seconds: int = 60) -> Dict[str, float]:
        """æ¸¬é‡è³‡æºä½¿ç”¨æƒ…æ³"""
        process = psutil.Process()
        
        # è¨˜éŒ„åˆå§‹è³‡æºä½¿ç”¨
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        initial_cpu = process.cpu_percent()
        
        memory_samples = []
        cpu_samples = []
        
        start_time = time.time()
        end_time = start_time + duration_seconds
        
        # åœ¨èƒŒæ™¯åŸ·è¡Œè«‹æ±‚
        def background_requests():
            while time.time() < end_time:
                try:
                    if hasattr(adapter_instance, 'process'):
                        result = adapter_instance.process(test_data)
                    else:
                        if hasattr(adapter_instance, '__call__'):
                            result = adapter_instance(test_data)
                    time.sleep(0.1)  # é¿å…éåº¦è² è¼‰
                except Exception:
                    pass
        
        # å•Ÿå‹•èƒŒæ™¯è«‹æ±‚ç·šç¨‹
        request_thread = threading.Thread(target=background_requests)
        request_thread.start()
        
        # ç›£æ§è³‡æºä½¿ç”¨
        while time.time() < end_time:
            try:
                memory_usage = process.memory_info().rss / 1024 / 1024  # MB
                cpu_usage = process.cpu_percent()
                
                memory_samples.append(memory_usage)
                cpu_samples.append(cpu_usage)
                
                time.sleep(1)  # æ¯ç§’æ¡æ¨£ä¸€æ¬¡
            except Exception as e:
                logger.warning(f"è³‡æºç›£æ§å¤±æ•—: {e}")
        
        # ç­‰å¾…èƒŒæ™¯ç·šç¨‹çµæŸ
        request_thread.join(timeout=5)
        
        return {
            "initial_memory_mb": initial_memory,
            "peak_memory_mb": max(memory_samples) if memory_samples else initial_memory,
            "avg_memory_mb": statistics.mean(memory_samples) if memory_samples else initial_memory,
            "memory_growth_mb": (max(memory_samples) - initial_memory) if memory_samples else 0,
            "avg_cpu_percent": statistics.mean(cpu_samples) if cpu_samples else 0,
            "peak_cpu_percent": max(cpu_samples) if cpu_samples else 0,
            "samples_count": len(memory_samples)
        }
    
    def test_performance_compliance(self, adapter_name: str, adapter_instance: Any) -> PerformanceTestResult:
        """æ¸¬è©¦æ€§èƒ½åˆè¦æ€§"""
        start_time = time.time()
        
        # æº–å‚™æ¸¬è©¦æ•¸æ“š
        test_data = {
            "test_type": "performance_compliance",
            "data": "test data for performance testing",
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"é–‹å§‹æ€§èƒ½åˆè¦æ€§æ¸¬è©¦: {adapter_name}")
        
        # 1. éŸ¿æ‡‰æ™‚é–“æ¸¬è©¦
        logger.info("æ¸¬è©¦éŸ¿æ‡‰æ™‚é–“...")
        response_time_metrics = self.measure_response_time(adapter_instance, test_data, iterations=50)
        
        # 2. ååé‡æ¸¬è©¦
        logger.info("æ¸¬è©¦ååé‡...")
        throughput_metrics = self.measure_throughput(adapter_instance, test_data, duration_seconds=15)
        
        # 3. ä½µç™¼æ€§èƒ½æ¸¬è©¦
        logger.info("æ¸¬è©¦ä½µç™¼æ€§èƒ½...")
        concurrent_metrics = self.measure_concurrent_performance(adapter_instance, test_data, concurrent_users=5, requests_per_user=5)
        
        # 4. è³‡æºä½¿ç”¨æ¸¬è©¦
        logger.info("æ¸¬è©¦è³‡æºä½¿ç”¨...")
        resource_metrics = self.measure_resource_usage(adapter_instance, test_data, duration_seconds=30)
        
        # å½™ç¸½æ€§èƒ½æŒ‡æ¨™
        metrics = PerformanceMetrics(
            response_time_ms=response_time_metrics["avg_response_time"],
            throughput_rps=throughput_metrics["requests_per_second"],
            memory_usage_mb=resource_metrics["peak_memory_mb"],
            cpu_usage_percent=resource_metrics["avg_cpu_percent"],
            success_rate=min(throughput_metrics["success_rate"], concurrent_metrics["success_rate"]),
            error_count=throughput_metrics["total_errors"] + concurrent_metrics["total_errors"],
            concurrent_requests=concurrent_metrics["concurrent_users"],
            test_duration_seconds=time.time() - start_time
        )
        
        # æ€§èƒ½åŸºæº–å°æ¯”
        benchmark_comparison = self._compare_with_benchmarks(metrics)
        
        # åˆ¤æ–·æ˜¯å¦é€šé
        passed = self._evaluate_performance_compliance(metrics, benchmark_comparison)
        
        # è©³ç´°ä¿¡æ¯
        details = {
            "response_time_details": response_time_metrics,
            "throughput_details": throughput_metrics,
            "concurrent_details": concurrent_metrics,
            "resource_details": resource_metrics,
            "test_configuration": {
                "response_time_iterations": 50,
                "throughput_duration": 15,
                "concurrent_users": 5,
                "resource_monitoring_duration": 30
            }
        }
        
        execution_time = time.time() - start_time
        
        return PerformanceTestResult(
            test_name="Performance Compliance",
            adapter_name=adapter_name,
            passed=passed,
            metrics=metrics,
            benchmark_comparison=benchmark_comparison,
            details=details,
            execution_time=execution_time
        )
    
    def _compare_with_benchmarks(self, metrics: PerformanceMetrics) -> Dict[str, Any]:
        """èˆ‡åŸºæº–é€²è¡Œæ¯”è¼ƒ"""
        standards = self.performance_standards
        
        def get_performance_level(value: float, thresholds: Dict[str, float], reverse: bool = False) -> str:
            """ç²å–æ€§èƒ½ç­‰ç´š"""
            if reverse:  # å°æ–¼éŒ¯èª¤ç‡ç­‰æŒ‡æ¨™ï¼Œå€¼è¶Šå°è¶Šå¥½
                if value <= thresholds["excellent"]:
                    return "excellent"
                elif value <= thresholds["good"]:
                    return "good"
                elif value <= thresholds["acceptable"]:
                    return "acceptable"
                else:
                    return "poor"
            else:  # å°æ–¼ååé‡ç­‰æŒ‡æ¨™ï¼Œå€¼è¶Šå¤§è¶Šå¥½
                if value >= thresholds["excellent"]:
                    return "excellent"
                elif value >= thresholds["good"]:
                    return "good"
                elif value >= thresholds["acceptable"]:
                    return "acceptable"
                else:
                    return "poor"
        
        return {
            "response_time": {
                "value": metrics.response_time_ms,
                "level": get_performance_level(metrics.response_time_ms, standards["response_time"], reverse=True),
                "benchmark": standards["response_time"]
            },
            "throughput": {
                "value": metrics.throughput_rps,
                "level": get_performance_level(metrics.throughput_rps, standards["throughput"]),
                "benchmark": standards["throughput"]
            },
            "memory_usage": {
                "value": metrics.memory_usage_mb,
                "level": get_performance_level(metrics.memory_usage_mb, standards["memory_usage"], reverse=True),
                "benchmark": standards["memory_usage"]
            },
            "cpu_usage": {
                "value": metrics.cpu_usage_percent,
                "level": get_performance_level(metrics.cpu_usage_percent, standards["cpu_usage"], reverse=True),
                "benchmark": standards["cpu_usage"]
            },
            "success_rate": {
                "value": metrics.success_rate,
                "level": get_performance_level(metrics.success_rate, standards["success_rate"]),
                "benchmark": standards["success_rate"]
            }
        }
    
    def _evaluate_performance_compliance(self, metrics: PerformanceMetrics, benchmark_comparison: Dict[str, Any]) -> bool:
        """è©•ä¼°æ€§èƒ½åˆè¦æ€§"""
        # æª¢æŸ¥é—œéµæŒ‡æ¨™æ˜¯å¦é”åˆ°å¯æ¥å—æ°´å¹³
        critical_checks = [
            benchmark_comparison["response_time"]["level"] in ["excellent", "good", "acceptable"],
            benchmark_comparison["success_rate"]["level"] in ["excellent", "good", "acceptable"],
            benchmark_comparison["memory_usage"]["level"] in ["excellent", "good", "acceptable"]
        ]
        
        # è‡³å°‘80%çš„é—œéµæª¢æŸ¥é€šé
        return sum(critical_checks) >= len(critical_checks) * 0.8
    
    def run_performance_tests(self, adapter_name: str = None) -> List[PerformanceTestResult]:
        """é‹è¡Œæ€§èƒ½æ¸¬è©¦"""
        logger.info("é–‹å§‹MCPæ€§èƒ½åˆè¦æ€§æ¸¬è©¦...")
        
        # ç™¼ç¾é©é…å™¨
        try:
            from mcptool.adapters.core.safe_mcp_registry import SafeMCPRegistry
            registry = SafeMCPRegistry()
            adapters = registry.list_adapters()
            
            # è½‰æ›ç‚ºå­—å…¸æ ¼å¼
            adapter_dict = {}
            for adapter_name_reg, adapter_instance in adapters:
                adapter_dict[adapter_name_reg] = {
                    "name": adapter_name_reg,
                    "instance": adapter_instance
                }
                
        except Exception as e:
            logger.error(f"ç„¡æ³•å¾è¨»å†Šè¡¨ç²å–é©é…å™¨: {e}")
            return []
        
        # é¸æ“‡è¦æ¸¬è©¦çš„é©é…å™¨
        if adapter_name:
            if adapter_name not in adapter_dict:
                logger.error(f"æœªæ‰¾åˆ°é©é…å™¨: {adapter_name}")
                return []
            test_adapters = {adapter_name: adapter_dict[adapter_name]}
        else:
            # é™åˆ¶æ¸¬è©¦æ•¸é‡ä»¥é¿å…éé•·æ™‚é–“
            test_adapters = dict(list(adapter_dict.items())[:5])
        
        results = []
        
        for name, adapter_info in test_adapters.items():
            adapter_instance = adapter_info.get("instance")
            if not adapter_instance:
                logger.warning(f"ç„¡æ³•ç²å–é©é…å™¨å¯¦ä¾‹: {name}")
                continue
            
            try:
                result = self.test_performance_compliance(name, adapter_instance)
                results.append(result)
                
            except Exception as e:
                logger.error(f"æ€§èƒ½æ¸¬è©¦å¤±æ•— {name}: {e}")
        
        self.test_results = results
        logger.info(f"æ€§èƒ½åˆè¦æ€§æ¸¬è©¦å®Œæˆï¼Œæ¸¬è©¦äº† {len(results)} å€‹é©é…å™¨")
        
        return results
    
    def generate_performance_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æ¸¬è©¦å ±å‘Š"""
        if not self.test_results:
            return "# MCPæ€§èƒ½åˆè¦æ€§æ¸¬è©¦å ±å‘Š\n\nç„¡æ¸¬è©¦çµæœå¯ç”¨ã€‚"
        
        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r.passed)
        
        avg_response_time = statistics.mean([r.metrics.response_time_ms for r in self.test_results])
        avg_throughput = statistics.mean([r.metrics.throughput_rps for r in self.test_results])
        avg_memory = statistics.mean([r.metrics.memory_usage_mb for r in self.test_results])
        avg_cpu = statistics.mean([r.metrics.cpu_usage_percent for r in self.test_results])
        avg_success_rate = statistics.mean([r.metrics.success_rate for r in self.test_results])
        
        report = f"""
# MCPæ€§èƒ½åˆè¦æ€§æ¸¬è©¦å ±å‘Š

## ğŸ“Š ç¸½é«”çµ±è¨ˆ
- **æ¸¬è©¦æ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æ¸¬è©¦é©é…å™¨æ•¸**: {total_tests}
- **é€šéæ¸¬è©¦æ•¸**: {passed_tests}
- **é€šéç‡**: {(passed_tests/total_tests*100):.1f}%

## ğŸ“ˆ å¹³å‡æ€§èƒ½æŒ‡æ¨™
- **å¹³å‡éŸ¿æ‡‰æ™‚é–“**: {avg_response_time:.1f} ms
- **å¹³å‡ååé‡**: {avg_throughput:.1f} RPS
- **å¹³å‡è¨˜æ†¶é«”ä½¿ç”¨**: {avg_memory:.1f} MB
- **å¹³å‡CPUä½¿ç”¨ç‡**: {avg_cpu:.1f}%
- **å¹³å‡æˆåŠŸç‡**: {avg_success_rate:.1f}%

## ğŸ” è©³ç´°æ¸¬è©¦çµæœ
"""
        
        for result in self.test_results:
            status = "âœ…" if result.passed else "âŒ"
            metrics = result.metrics
            
            report += f"""
### {status} {result.adapter_name}
- **éŸ¿æ‡‰æ™‚é–“**: {metrics.response_time_ms:.1f} ms ({result.benchmark_comparison['response_time']['level']})
- **ååé‡**: {metrics.throughput_rps:.1f} RPS ({result.benchmark_comparison['throughput']['level']})
- **è¨˜æ†¶é«”ä½¿ç”¨**: {metrics.memory_usage_mb:.1f} MB ({result.benchmark_comparison['memory_usage']['level']})
- **CPUä½¿ç”¨ç‡**: {metrics.cpu_usage_percent:.1f}% ({result.benchmark_comparison['cpu_usage']['level']})
- **æˆåŠŸç‡**: {metrics.success_rate:.1f}% ({result.benchmark_comparison['success_rate']['level']})
- **æ¸¬è©¦æ™‚é•·**: {result.execution_time:.1f} ç§’
"""
        
        report += f"""
## ğŸ¯ æ€§èƒ½åŸºæº–

### éŸ¿æ‡‰æ™‚é–“æ¨™æº– (æ¯«ç§’)
- **å„ªç§€**: â‰¤ {self.performance_standards['response_time']['excellent']}
- **è‰¯å¥½**: â‰¤ {self.performance_standards['response_time']['good']}
- **å¯æ¥å—**: â‰¤ {self.performance_standards['response_time']['acceptable']}
- **è¼ƒå·®**: > {self.performance_standards['response_time']['poor']}

### ååé‡æ¨™æº– (RPS)
- **å„ªç§€**: â‰¥ {self.performance_standards['throughput']['excellent']}
- **è‰¯å¥½**: â‰¥ {self.performance_standards['throughput']['good']}
- **å¯æ¥å—**: â‰¥ {self.performance_standards['throughput']['acceptable']}
- **è¼ƒå·®**: < {self.performance_standards['throughput']['poor']}

## ğŸ“‹ æ”¹é€²å»ºè­°

### æ€§èƒ½å„ªåŒ–
1. å„ªåŒ–éŸ¿æ‡‰æ™‚é–“è¶…é1ç§’çš„é©é…å™¨
2. æå‡ååé‡ä½æ–¼20 RPSçš„é©é…å™¨
3. æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨è¶…é200MBçš„é©é…å™¨
4. é™ä½CPUä½¿ç”¨ç‡è¶…é50%çš„é©é…å™¨

### åˆè¦æ€§æ”¹é€²
1. ç¢ºä¿æ‰€æœ‰é©é…å™¨æˆåŠŸç‡é”åˆ°95%ä»¥ä¸Š
2. å¯¦æ–½æ€§èƒ½ç›£æ§å’Œå‘Šè­¦æ©Ÿåˆ¶
3. å»ºç«‹æ€§èƒ½å›æ­¸æ¸¬è©¦
4. å„ªåŒ–è³‡æºä½¿ç”¨æ•ˆç‡

## ğŸ† åˆè¦æ€§è©•ä¼°

{'âœ… ç³»çµ±é”åˆ°æ€§èƒ½åˆè¦è¦æ±‚' if (passed_tests/total_tests) >= 0.8 else 'âš ï¸ ç³»çµ±éœ€è¦æ€§èƒ½å„ªåŒ–' if (passed_tests/total_tests) >= 0.6 else 'âŒ ç³»çµ±æœªé”åˆ°æ€§èƒ½åˆè¦è¦æ±‚'}

**æ•´é«”é€šéç‡**: {(passed_tests/total_tests*100):.1f}%
"""
        
        return report

if __name__ == "__main__":
    tester = MCPPerformanceComplianceTester()
    results = tester.run_performance_tests()
    
    # ç”Ÿæˆå ±å‘Š
    report = tester.generate_performance_report()
    
    # ä¿å­˜å ±å‘Š
    report_file = Path("mcp_performance_compliance_report.md")
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… MCPæ€§èƒ½åˆè¦æ€§æ¸¬è©¦å®Œæˆ")
    print(f"ğŸ“„ å ±å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    if results:
        passed_count = sum(1 for r in results if r.passed)
        print(f"ğŸ¯ æ¸¬è©¦çµæœ: {passed_count}/{len(results)} å€‹é©é…å™¨é€šéæ€§èƒ½æ¸¬è©¦")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°å¯æ¸¬è©¦çš„é©é…å™¨")

