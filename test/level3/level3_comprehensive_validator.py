#!/usr/bin/env python3
"""
ç¬¬3å±¤MCPåˆè¦æ¸¬è©¦é›†æˆé©—è­‰å™¨
æ•´åˆæ‰€æœ‰MCPåˆè¦æ¸¬è©¦æ¨¡çµ„ï¼Œæä¾›çµ±ä¸€çš„æ¸¬è©¦å’Œé©—è­‰æ¥å£

åŒ…æ‹¬ï¼š
- å”è­°é©—è­‰æ¸¬è©¦
- æ¨™æº–åŒ–æ¸¬è©¦æ¡†æ¶
- æ€§èƒ½åˆè¦æ€§æ¸¬è©¦
- éŒ¯èª¤è™•ç†æ¸¬è©¦
- èƒ½åŠ›è²æ˜æ¨™æº–åŒ–
"""

import sys
import os
import time
import json
import logging
from typing import Dict, Any, List, Optional
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, asdict

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å°å…¥å„å€‹æ¸¬è©¦æ¨¡çµ„
try:
    from enhanced_protocol_validation import MCPProtocolValidator
    from standardized_test_framework import MCPStandardizedTestFramework
    from performance_compliance_tester import MCPPerformanceComplianceTester
    from error_handling_tester import MCPErrorHandlingTester
    from capability_standardizer import MCPCapabilityStandardizer
except ImportError as e:
    print(f"è­¦å‘Š: ç„¡æ³•å°å…¥æ¸¬è©¦æ¨¡çµ„: {e}")

logger = logging.getLogger(__name__)

@dataclass
class Level3TestSummary:
    """ç¬¬3å±¤æ¸¬è©¦ç¸½çµ"""
    test_time: str
    total_adapters: int
    tested_adapters: int
    passed_adapters: int
    overall_compliance: float
    test_results: Dict[str, Any]
    recommendations: List[str]

class Level3MCPComplianceValidator:
    """ç¬¬3å±¤MCPåˆè¦æ¸¬è©¦é›†æˆé©—è­‰å™¨"""
    
    def __init__(self):
        self.test_modules = {}
        self.test_results = {}
        self.summary = None
        
        # åˆå§‹åŒ–æ¸¬è©¦æ¨¡çµ„
        self._initialize_test_modules()
        
    def _initialize_test_modules(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¸¬è©¦æ¨¡çµ„"""
        try:
            self.test_modules['protocol'] = MCPProtocolValidator()
            logger.info("âœ… å”è­°é©—è­‰æ¨¡çµ„åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ å”è­°é©—è­‰æ¨¡çµ„åˆå§‹åŒ–å¤±æ•—: {e}")
        
        try:
            self.test_modules['standardization'] = MCPStandardizedTestFramework()
            logger.info("âœ… æ¨™æº–åŒ–æ¸¬è©¦æ¨¡çµ„åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æ¨™æº–åŒ–æ¸¬è©¦æ¨¡çµ„åˆå§‹åŒ–å¤±æ•—: {e}")
        
        try:
            self.test_modules['performance'] = MCPPerformanceComplianceTester()
            logger.info("âœ… æ€§èƒ½æ¸¬è©¦æ¨¡çµ„åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æ¸¬è©¦æ¨¡çµ„åˆå§‹åŒ–å¤±æ•—: {e}")
        
        try:
            self.test_modules['error_handling'] = MCPErrorHandlingTester()
            logger.info("âœ… éŒ¯èª¤è™•ç†æ¸¬è©¦æ¨¡çµ„åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ éŒ¯èª¤è™•ç†æ¸¬è©¦æ¨¡çµ„åˆå§‹åŒ–å¤±æ•—: {e}")
        
        try:
            self.test_modules['capability'] = MCPCapabilityStandardizer()
            logger.info("âœ… èƒ½åŠ›æ¨™æº–åŒ–æ¨¡çµ„åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ èƒ½åŠ›æ¨™æº–åŒ–æ¨¡çµ„åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def run_protocol_validation(self) -> Dict[str, Any]:
        """é‹è¡Œå”è­°é©—è­‰æ¸¬è©¦"""
        logger.info("ğŸ” é–‹å§‹å”è­°é©—è­‰æ¸¬è©¦...")
        
        if 'protocol' not in self.test_modules:
            return {"error": "å”è­°é©—è­‰æ¨¡çµ„æœªåˆå§‹åŒ–"}
        
        try:
            validator = self.test_modules['protocol']
            results = validator.run_validation_tests()
            
            # ç”Ÿæˆå ±å‘Š
            report = validator.generate_validation_report()
            
            # ä¿å­˜å ±å‘Š
            report_file = Path("protocol_validation_report.md")
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            
            return {
                "status": "completed",
                "results_count": len(results),
                "compliance_score": validator.calculate_overall_compliance(),
                "report_file": str(report_file)
            }
            
        except Exception as e:
            logger.error(f"å”è­°é©—è­‰æ¸¬è©¦å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def run_standardization_tests(self) -> Dict[str, Any]:
        """é‹è¡Œæ¨™æº–åŒ–æ¸¬è©¦"""
        logger.info("ğŸ“‹ é–‹å§‹æ¨™æº–åŒ–æ¸¬è©¦...")
        
        if 'standardization' not in self.test_modules:
            return {"error": "æ¨™æº–åŒ–æ¸¬è©¦æ¨¡çµ„æœªåˆå§‹åŒ–"}
        
        try:
            framework = self.test_modules['standardization']
            results = framework.run_standardization_tests()
            
            # ç”Ÿæˆå ±å‘Š
            report = framework.generate_standardization_report()
            
            # ä¿å­˜å ±å‘Š
            report_file = Path("standardization_test_report.md")
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            
            return {
                "status": "completed",
                "results_count": len(results),
                "compliance_score": framework.stats.get('overall_compliance', 0),
                "report_file": str(report_file)
            }
            
        except Exception as e:
            logger.error(f"æ¨™æº–åŒ–æ¸¬è©¦å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def run_performance_tests(self) -> Dict[str, Any]:
        """é‹è¡Œæ€§èƒ½æ¸¬è©¦"""
        logger.info("âš¡ é–‹å§‹æ€§èƒ½åˆè¦æ€§æ¸¬è©¦...")
        
        if 'performance' not in self.test_modules:
            return {"error": "æ€§èƒ½æ¸¬è©¦æ¨¡çµ„æœªåˆå§‹åŒ–"}
        
        try:
            tester = self.test_modules['performance']
            results = tester.run_performance_tests()
            
            # ç”Ÿæˆå ±å‘Š
            report = tester.generate_performance_report()
            
            # ä¿å­˜å ±å‘Š
            report_file = Path("performance_compliance_report.md")
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            
            passed_count = sum(1 for r in results if r.passed) if results else 0
            
            return {
                "status": "completed",
                "results_count": len(results),
                "passed_count": passed_count,
                "pass_rate": passed_count / len(results) if results else 0,
                "report_file": str(report_file)
            }
            
        except Exception as e:
            logger.error(f"æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def run_error_handling_tests(self) -> Dict[str, Any]:
        """é‹è¡ŒéŒ¯èª¤è™•ç†æ¸¬è©¦"""
        logger.info("ğŸ›¡ï¸ é–‹å§‹éŒ¯èª¤è™•ç†æ¸¬è©¦...")
        
        if 'error_handling' not in self.test_modules:
            return {"error": "éŒ¯èª¤è™•ç†æ¸¬è©¦æ¨¡çµ„æœªåˆå§‹åŒ–"}
        
        try:
            tester = self.test_modules['error_handling']
            results = tester.run_error_handling_tests()
            
            # ç”Ÿæˆå ±å‘Š
            report = tester.generate_error_handling_report()
            
            # ä¿å­˜å ±å‘Š
            report_file = Path("error_handling_test_report.md")
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            
            passed_count = sum(1 for r in results if r.passed) if results else 0
            handled_count = sum(1 for r in results if r.error_handled) if results else 0
            
            return {
                "status": "completed",
                "results_count": len(results),
                "passed_count": passed_count,
                "handled_count": handled_count,
                "pass_rate": passed_count / len(results) if results else 0,
                "handle_rate": handled_count / len(results) if results else 0,
                "report_file": str(report_file)
            }
            
        except Exception as e:
            logger.error(f"éŒ¯èª¤è™•ç†æ¸¬è©¦å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def run_capability_standardization(self) -> Dict[str, Any]:
        """é‹è¡Œèƒ½åŠ›æ¨™æº–åŒ–"""
        logger.info("ğŸ¯ é–‹å§‹èƒ½åŠ›è²æ˜æ¨™æº–åŒ–...")
        
        if 'capability' not in self.test_modules:
            return {"error": "èƒ½åŠ›æ¨™æº–åŒ–æ¨¡çµ„æœªåˆå§‹åŒ–"}
        
        try:
            standardizer = self.test_modules['capability']
            results = standardizer.run_capability_standardization()
            
            # ç”Ÿæˆå ±å‘Š
            report = standardizer.generate_standardization_report()
            
            # ä¿å­˜å ±å‘Š
            report_file = Path("capability_standardization_report.md")
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            
            valid_count = sum(1 for r in results if r.is_valid) if results else 0
            avg_compliance = sum(r.compliance_score for r in results) / len(results) if results else 0
            
            return {
                "status": "completed",
                "results_count": len(results),
                "valid_count": valid_count,
                "validity_rate": valid_count / len(results) if results else 0,
                "avg_compliance": avg_compliance,
                "report_file": str(report_file)
            }
            
        except Exception as e:
            logger.error(f"èƒ½åŠ›æ¨™æº–åŒ–å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def run_comprehensive_tests(self) -> Level3TestSummary:
        """é‹è¡Œå®Œæ•´çš„ç¬¬3å±¤æ¸¬è©¦"""
        logger.info("ğŸš€ é–‹å§‹ç¬¬3å±¤MCPåˆè¦æ¸¬è©¦å®Œæ•´é©—è­‰...")
        
        start_time = time.time()
        test_start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # é‹è¡Œæ‰€æœ‰æ¸¬è©¦
        self.test_results['protocol'] = self.run_protocol_validation()
        self.test_results['standardization'] = self.run_standardization_tests()
        self.test_results['performance'] = self.run_performance_tests()
        self.test_results['error_handling'] = self.run_error_handling_tests()
        self.test_results['capability'] = self.run_capability_standardization()
        
        # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
        total_adapters = 0
        tested_adapters = 0
        passed_tests = 0
        total_tests = 0
        
        for test_type, result in self.test_results.items():
            if 'error' not in result:
                results_count = result.get('results_count', 0)
                tested_adapters = max(tested_adapters, results_count)
                
                if test_type == 'protocol':
                    compliance_score = result.get('compliance_score', 0)
                    if compliance_score >= 0.8:
                        passed_tests += 1
                    total_tests += 1
                elif test_type in ['performance', 'error_handling']:
                    pass_rate = result.get('pass_rate', 0)
                    if pass_rate >= 0.8:
                        passed_tests += 1
                    total_tests += 1
                elif test_type == 'capability':
                    validity_rate = result.get('validity_rate', 0)
                    if validity_rate >= 0.8:
                        passed_tests += 1
                    total_tests += 1
        
        # è¨ˆç®—æ•´é«”åˆè¦æ€§
        overall_compliance = passed_tests / total_tests if total_tests > 0 else 0
        
        # ç”Ÿæˆå»ºè­°
        recommendations = self._generate_recommendations()
        
        # å‰µå»ºç¸½çµ
        self.summary = Level3TestSummary(
            test_time=test_start_time,
            total_adapters=tested_adapters,
            tested_adapters=tested_adapters,
            passed_adapters=passed_tests,
            overall_compliance=overall_compliance,
            test_results=self.test_results,
            recommendations=recommendations
        )
        
        execution_time = time.time() - start_time
        logger.info(f"âœ… ç¬¬3å±¤æ¸¬è©¦å®Œæˆï¼Œè€—æ™‚ {execution_time:.1f} ç§’")
        
        return self.summary
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        # åˆ†ææ¸¬è©¦çµæœä¸¦ç”Ÿæˆå»ºè­°
        for test_type, result in self.test_results.items():
            if 'error' in result:
                recommendations.append(f"ä¿®å¾© {test_type} æ¸¬è©¦æ¨¡çµ„çš„å•é¡Œ: {result['error']}")
                continue
            
            if test_type == 'protocol':
                compliance = result.get('compliance_score', 0)
                if compliance < 0.8:
                    recommendations.append("æ”¹é€²MCPå”è­°åˆè¦æ€§ï¼Œç¢ºä¿æ‰€æœ‰é©é…å™¨ç¬¦åˆå”è­°æ¨™æº–")
            
            elif test_type == 'standardization':
                compliance = result.get('compliance_score', 0)
                if compliance < 0.8:
                    recommendations.append("çµ±ä¸€é©é…å™¨æ¥å£æ¨™æº–ï¼Œå¯¦æ–½æ¨™æº–åŒ–æ¸¬è©¦æ¡†æ¶")
            
            elif test_type == 'performance':
                pass_rate = result.get('pass_rate', 0)
                if pass_rate < 0.8:
                    recommendations.append("å„ªåŒ–é©é…å™¨æ€§èƒ½ï¼Œç¢ºä¿éŸ¿æ‡‰æ™‚é–“å’Œååé‡é”åˆ°æ¨™æº–")
            
            elif test_type == 'error_handling':
                pass_rate = result.get('pass_rate', 0)
                handle_rate = result.get('handle_rate', 0)
                if pass_rate < 0.8:
                    recommendations.append("å®Œå–„éŒ¯èª¤è™•ç†æ©Ÿåˆ¶ï¼Œæé«˜éŒ¯èª¤è™•ç†çš„æ­£ç¢ºæ€§")
                if handle_rate < 0.9:
                    recommendations.append("å¢å¼·éŒ¯èª¤æª¢æ¸¬å’Œè™•ç†èƒ½åŠ›ï¼Œç¢ºä¿ç³»çµ±ç©©å®šæ€§")
            
            elif test_type == 'capability':
                validity_rate = result.get('validity_rate', 0)
                if validity_rate < 0.8:
                    recommendations.append("æ¨™æº–åŒ–èƒ½åŠ›è²æ˜æ ¼å¼ï¼Œå®Œå–„èƒ½åŠ›æ–‡æª”å’Œé©—è­‰")
        
        # æ·»åŠ é€šç”¨å»ºè­°
        if self.summary and self.summary.overall_compliance < 0.8:
            recommendations.extend([
                "å»ºç«‹æŒçºŒé›†æˆæ¸¬è©¦æµç¨‹ï¼Œç¢ºä¿ä»£ç¢¼è³ªé‡",
                "å¯¦æ–½è‡ªå‹•åŒ–æ¸¬è©¦å’Œç›£æ§æ©Ÿåˆ¶",
                "å®šæœŸé€²è¡Œåˆè¦æ€§å¯©æŸ¥å’Œæ”¹é€²",
                "å»ºç«‹æ¸¬è©¦è¦†è“‹ç‡ç›£æ§å’Œå ±å‘Šæ©Ÿåˆ¶"
            ])
        
        return recommendations
    
    def generate_comprehensive_report(self) -> str:
        """ç”Ÿæˆå®Œæ•´çš„ç¬¬3å±¤æ¸¬è©¦å ±å‘Š"""
        if not self.summary:
            return "# ç¬¬3å±¤MCPåˆè¦æ¸¬è©¦å ±å‘Š\n\næ¸¬è©¦å°šæœªé‹è¡Œã€‚"
        
        summary = self.summary
        
        report = f"""
# ç¬¬3å±¤MCPåˆè¦æ¸¬è©¦å®Œæ•´å ±å‘Š

## ğŸ“Š åŸ·è¡Œç¸½çµ
- **æ¸¬è©¦æ™‚é–“**: {summary.test_time}
- **æ¸¬è©¦é©é…å™¨æ•¸**: {summary.tested_adapters}
- **é€šéæ¸¬è©¦æ¨¡çµ„æ•¸**: {summary.passed_adapters}/5
- **æ•´é«”åˆè¦æ€§**: {summary.overall_compliance:.1%}

## ğŸ¯ åˆè¦æ€§è©•ä¼°

{'âœ… ç¬¬3å±¤æ¸¬è©¦å…¨é¢é€šé' if summary.overall_compliance >= 0.8 else 'âš ï¸ ç¬¬3å±¤æ¸¬è©¦éœ€è¦æ”¹é€²' if summary.overall_compliance >= 0.6 else 'âŒ ç¬¬3å±¤æ¸¬è©¦æœªé”æ¨™æº–'}

## ğŸ“‹ è©³ç´°æ¸¬è©¦çµæœ

### 1. å”è­°é©—è­‰æ¸¬è©¦
"""
        
        protocol_result = summary.test_results.get('protocol', {})
        if 'error' in protocol_result:
            report += f"âŒ **å¤±æ•—**: {protocol_result['error']}\n"
        else:
            compliance = protocol_result.get('compliance_score', 0)
            status = "âœ…" if compliance >= 0.8 else "âš ï¸" if compliance >= 0.6 else "âŒ"
            report += f"{status} **åˆè¦åˆ†æ•¸**: {compliance:.1%}\n"
            report += f"- æ¸¬è©¦çµæœæ•¸: {protocol_result.get('results_count', 0)}\n"
            report += f"- å ±å‘Šæ–‡ä»¶: {protocol_result.get('report_file', 'N/A')}\n"
        
        report += f"""
### 2. æ¨™æº–åŒ–æ¸¬è©¦
"""
        
        std_result = summary.test_results.get('standardization', {})
        if 'error' in std_result:
            report += f"âŒ **å¤±æ•—**: {std_result['error']}\n"
        else:
            compliance = std_result.get('compliance_score', 0)
            status = "âœ…" if compliance >= 0.8 else "âš ï¸" if compliance >= 0.6 else "âŒ"
            report += f"{status} **åˆè¦åˆ†æ•¸**: {compliance:.1%}\n"
            report += f"- æ¸¬è©¦çµæœæ•¸: {std_result.get('results_count', 0)}\n"
            report += f"- å ±å‘Šæ–‡ä»¶: {std_result.get('report_file', 'N/A')}\n"
        
        report += f"""
### 3. æ€§èƒ½åˆè¦æ€§æ¸¬è©¦
"""
        
        perf_result = summary.test_results.get('performance', {})
        if 'error' in perf_result:
            report += f"âŒ **å¤±æ•—**: {perf_result['error']}\n"
        else:
            pass_rate = perf_result.get('pass_rate', 0)
            status = "âœ…" if pass_rate >= 0.8 else "âš ï¸" if pass_rate >= 0.6 else "âŒ"
            report += f"{status} **é€šéç‡**: {pass_rate:.1%}\n"
            report += f"- æ¸¬è©¦çµæœæ•¸: {perf_result.get('results_count', 0)}\n"
            report += f"- é€šéæ•¸: {perf_result.get('passed_count', 0)}\n"
            report += f"- å ±å‘Šæ–‡ä»¶: {perf_result.get('report_file', 'N/A')}\n"
        
        report += f"""
### 4. éŒ¯èª¤è™•ç†æ¸¬è©¦
"""
        
        error_result = summary.test_results.get('error_handling', {})
        if 'error' in error_result:
            report += f"âŒ **å¤±æ•—**: {error_result['error']}\n"
        else:
            pass_rate = error_result.get('pass_rate', 0)
            handle_rate = error_result.get('handle_rate', 0)
            status = "âœ…" if pass_rate >= 0.8 and handle_rate >= 0.9 else "âš ï¸" if pass_rate >= 0.6 else "âŒ"
            report += f"{status} **é€šéç‡**: {pass_rate:.1%}, **è™•ç†ç‡**: {handle_rate:.1%}\n"
            report += f"- æ¸¬è©¦çµæœæ•¸: {error_result.get('results_count', 0)}\n"
            report += f"- é€šéæ•¸: {error_result.get('passed_count', 0)}\n"
            report += f"- è™•ç†æ•¸: {error_result.get('handled_count', 0)}\n"
            report += f"- å ±å‘Šæ–‡ä»¶: {error_result.get('report_file', 'N/A')}\n"
        
        report += f"""
### 5. èƒ½åŠ›è²æ˜æ¨™æº–åŒ–
"""
        
        cap_result = summary.test_results.get('capability', {})
        if 'error' in cap_result:
            report += f"âŒ **å¤±æ•—**: {cap_result['error']}\n"
        else:
            validity_rate = cap_result.get('validity_rate', 0)
            avg_compliance = cap_result.get('avg_compliance', 0)
            status = "âœ…" if validity_rate >= 0.8 else "âš ï¸" if validity_rate >= 0.6 else "âŒ"
            report += f"{status} **æœ‰æ•ˆç‡**: {validity_rate:.1%}, **å¹³å‡åˆè¦**: {avg_compliance:.1%}\n"
            report += f"- æ¸¬è©¦çµæœæ•¸: {cap_result.get('results_count', 0)}\n"
            report += f"- æœ‰æ•ˆæ•¸: {cap_result.get('valid_count', 0)}\n"
            report += f"- å ±å‘Šæ–‡ä»¶: {cap_result.get('report_file', 'N/A')}\n"
        
        report += f"""
## ğŸ’¡ æ”¹é€²å»ºè­°

"""
        
        for i, recommendation in enumerate(summary.recommendations, 1):
            report += f"{i}. {recommendation}\n"
        
        report += f"""
## ğŸ“ˆ ä¸‹ä¸€æ­¥è¡Œå‹•

### ç«‹å³è¡Œå‹•
1. ä¿®å¾©æ‰€æœ‰å¤±æ•—çš„æ¸¬è©¦æ¨¡çµ„
2. æå‡åˆè¦æ€§ä½æ–¼80%çš„æ¸¬è©¦é …ç›®
3. å¯¦æ–½è‡ªå‹•åŒ–æ¸¬è©¦æµç¨‹

### ä¸­æœŸç›®æ¨™
1. å»ºç«‹æŒçºŒé›†æˆæ¸¬è©¦ç®¡é“
2. å¯¦æ–½æ€§èƒ½ç›£æ§å’Œå‘Šè­¦
3. å®Œå–„æ–‡æª”å’Œæ¨™æº–åŒ–æµç¨‹

### é•·æœŸè¦åŠƒ
1. å»ºç«‹ä¼æ¥­ç´šè³ªé‡ä¿è­‰é«”ç³»
2. å¯¦æ–½å…¨é¢çš„åˆè¦æ€§ç®¡ç†
3. æŒçºŒå„ªåŒ–å’Œæ”¹é€²æ¸¬è©¦æ¡†æ¶

## ğŸ† çµè«–

ç¬¬3å±¤MCPåˆè¦æ¸¬è©¦{'å·²é”åˆ°ä¼æ¥­ç´šæ¨™æº–' if summary.overall_compliance >= 0.8 else 'éœ€è¦é€²ä¸€æ­¥æ”¹é€²ä»¥é”åˆ°ä¼æ¥­ç´šæ¨™æº–'}ã€‚

**æ•´é«”åˆè¦æ€§è©•åˆ†**: {summary.overall_compliance:.1%}
"""
        
        return report

if __name__ == "__main__":
    # é…ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    validator = Level3MCPComplianceValidator()
    
    # é‹è¡Œå®Œæ•´æ¸¬è©¦
    summary = validator.run_comprehensive_tests()
    
    # ç”Ÿæˆå®Œæ•´å ±å‘Š
    report = validator.generate_comprehensive_report()
    
    # ä¿å­˜å ±å‘Š
    report_file = Path("level3_mcp_compliance_comprehensive_report.md")
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # ä¿å­˜æ¸¬è©¦çµæœJSON
    results_file = Path("level3_test_results.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(asdict(summary), f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ‰ ç¬¬3å±¤MCPåˆè¦æ¸¬è©¦å®Œæˆï¼")
    print(f"ğŸ“„ å®Œæ•´å ±å‘Š: {report_file}")
    print(f"ğŸ“Š æ¸¬è©¦çµæœ: {results_file}")
    print(f"ğŸ¯ æ•´é«”åˆè¦æ€§: {summary.overall_compliance:.1%}")
    print(f"ğŸ“‹ æ”¹é€²å»ºè­°æ•¸: {len(summary.recommendations)}")
    
    if summary.overall_compliance >= 0.8:
        print("âœ… æ­å–œï¼ç¬¬3å±¤æ¸¬è©¦é”åˆ°ä¼æ¥­ç´šæ¨™æº–")
    elif summary.overall_compliance >= 0.6:
        print("âš ï¸ ç¬¬3å±¤æ¸¬è©¦éœ€è¦æ”¹é€²ä»¥é”åˆ°æ¨™æº–")
    else:
        print("âŒ ç¬¬3å±¤æ¸¬è©¦æœªé”åˆ°åŸºæœ¬è¦æ±‚ï¼Œéœ€è¦é‡é»æ”¹é€²")

