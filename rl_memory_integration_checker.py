#!/usr/bin/env python3
"""
RL_SRT + ç•°æ­¥RL èˆ‡çµ±ä¸€è¨˜æ†¶æ¨¡å¡Šé›†æˆå®Œæ•´æ€§æª¢æŸ¥å™¨
"""

import os
import sys
import json
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime

logger = logging.getLogger(__name__)

class RLMemoryIntegrationChecker:
    """RL-è¨˜æ†¶é›†æˆå®Œæ•´æ€§æª¢æŸ¥å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æª¢æŸ¥å™¨"""
        self.base_path = "/home/ubuntu/Powerauto.ai"
        self.check_results = {}
        
    def check_integration_completeness(self) -> Dict[str, Any]:
        """æª¢æŸ¥é›†æˆå®Œæ•´æ€§"""
        results = {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "unknown",
            "components": {},
            "integration_points": {},
            "data_flow_analysis": {},
            "missing_components": [],
            "recommendations": []
        }
        
        # 1. æª¢æŸ¥æ ¸å¿ƒçµ„ä»¶
        results["components"] = self._check_core_components()
        
        # 2. æª¢æŸ¥é›†æˆé»
        results["integration_points"] = self._check_integration_points()
        
        # 3. åˆ†ææ•¸æ“šæµ
        results["data_flow_analysis"] = self._analyze_data_flow()
        
        # 4. æª¢æŸ¥è¨˜æ†¶æ•¸æ“šæ¥å£
        results["memory_interfaces"] = self._check_memory_interfaces()
        
        # 5. è©•ä¼°æ•´é«”å®Œæ•´æ€§
        results["overall_status"] = self._evaluate_overall_status(results)
        
        return results
    
    def _check_core_components(self) -> Dict[str, Any]:
        """æª¢æŸ¥æ ¸å¿ƒçµ„ä»¶"""
        components = {
            "rl_srt_mcp": {
                "path": "mcptool/adapters/rl_srt/rl_srt_mcp.py",
                "status": "unknown",
                "features": []
            },
            "rl_srt_dataflow_mcp": {
                "path": "mcptool/adapters/rl_srt_dataflow_mcp.py", 
                "status": "unknown",
                "features": []
            },
            "unified_memory_mcp": {
                "path": "mcptool/adapters/unified_memory_mcp.py",
                "status": "unknown",
                "features": []
            },
            "memory_query_engine": {
                "path": "memory-system/memory_query_engine.py",
                "status": "unknown",
                "features": []
            },
            "cloud_edge_data_mcp": {
                "path": "mcptool/adapters/cloud_edge_data_mcp.py",
                "status": "unknown",
                "features": []
            }
        }
        
        for component_name, component_info in components.items():
            full_path = os.path.join(self.base_path, component_info["path"])
            
            if os.path.exists(full_path):
                component_info["status"] = "exists"
                component_info["features"] = self._analyze_component_features(full_path)
            else:
                component_info["status"] = "missing"
        
        return components
    
    def _analyze_component_features(self, file_path: str) -> List[str]:
        """åˆ†æçµ„ä»¶åŠŸèƒ½ç‰¹æ€§"""
        features = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # æª¢æŸ¥é—œéµåŠŸèƒ½
            feature_patterns = {
                "async_training": ["async", "asyncio", "_async_training"],
                "memory_integration": ["memory", "get_training_data", "interaction_data"],
                "data_flow": ["data_flow", "stream", "batch"],
                "rl_training": ["rl", "reinforcement", "reward", "train"],
                "srt_training": ["srt", "self_reward", "self-reward"],
                "cloud_edge": ["cloud", "edge", "federated"],
                "performance_monitoring": ["metrics", "performance", "monitor"]
            }
            
            for feature, patterns in feature_patterns.items():
                if any(pattern.lower() in content.lower() for pattern in patterns):
                    features.append(feature)
                    
        except Exception as e:
            logger.warning(f"åˆ†ææ–‡ä»¶å¤±æ•— {file_path}: {e}")
        
        return features
    
    def _check_integration_points(self) -> Dict[str, Any]:
        """æª¢æŸ¥é›†æˆé»"""
        integration_points = {
            "rl_to_memory": {
                "description": "RLç³»çµ±å¾è¨˜æ†¶æ¨¡å¡Šç²å–äº¤äº’æ•¸æ“š",
                "status": "unknown",
                "implementation": []
            },
            "memory_to_rl": {
                "description": "è¨˜æ†¶æ¨¡å¡Šå‘RLç³»çµ±æä¾›è¨“ç·´æ•¸æ“š",
                "status": "unknown", 
                "implementation": []
            },
            "async_data_pipeline": {
                "description": "ç•°æ­¥æ•¸æ“šç®¡é“è™•ç†",
                "status": "unknown",
                "implementation": []
            },
            "cloud_edge_sync": {
                "description": "é›²ç«¯é‚Šç·£æ•¸æ“šåŒæ­¥",
                "status": "unknown",
                "implementation": []
            }
        }
        
        # æª¢æŸ¥RL_SRTæ•¸æ“šæµæ–‡ä»¶ä¸­çš„é›†æˆå¯¦ç¾
        dataflow_file = os.path.join(self.base_path, "mcptool/adapters/rl_srt_dataflow_mcp.py")
        if os.path.exists(dataflow_file):
            with open(dataflow_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # æª¢æŸ¥è¨˜æ†¶é›†æˆ
            if "cloud_edge_data" in content and "get_training_data" in content:
                integration_points["rl_to_memory"]["status"] = "implemented"
                integration_points["rl_to_memory"]["implementation"].append("CloudEdgeDataMCPé›†æˆ")
                
            if "async" in content and "training_loop" in content:
                integration_points["async_data_pipeline"]["status"] = "implemented"
                integration_points["async_data_pipeline"]["implementation"].append("ç•°æ­¥è¨“ç·´å¾ªç’°")
                
            if "federated" in content or "sync_with_cloud" in content:
                integration_points["cloud_edge_sync"]["status"] = "implemented"
                integration_points["cloud_edge_sync"]["implementation"].append("è¯é‚¦å­¸ç¿’æ”¯æŒ")
        
        return integration_points
    
    def _analyze_data_flow(self) -> Dict[str, Any]:
        """åˆ†ææ•¸æ“šæµ"""
        data_flow = {
            "memory_data_sources": [],
            "rl_training_pipeline": [],
            "data_transformation": [],
            "feedback_loop": []
        }
        
        # æª¢æŸ¥è¨˜æ†¶æ•¸æ“šæº
        memory_files = [
            "mcptool/adapters/unified_memory_mcp.py",
            "memory-system/memory_query_engine.py",
            "mcptool/adapters/supermemory_adapter/supermemory_mcp.py"
        ]
        
        for memory_file in memory_files:
            full_path = os.path.join(self.base_path, memory_file)
            if os.path.exists(full_path):
                data_flow["memory_data_sources"].append({
                    "file": memory_file,
                    "status": "available"
                })
        
        # æª¢æŸ¥RLè¨“ç·´ç®¡é“
        rl_files = [
            "mcptool/adapters/rl_srt/rl_srt_mcp.py",
            "mcptool/adapters/rl_srt_dataflow_mcp.py"
        ]
        
        for rl_file in rl_files:
            full_path = os.path.join(self.base_path, rl_file)
            if os.path.exists(full_path):
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                pipeline_features = []
                if "async" in content:
                    pipeline_features.append("ç•°æ­¥è™•ç†")
                if "batch" in content:
                    pipeline_features.append("æ‰¹æ¬¡è™•ç†")
                if "stream" in content:
                    pipeline_features.append("æµå¼è™•ç†")
                    
                data_flow["rl_training_pipeline"].append({
                    "file": rl_file,
                    "features": pipeline_features
                })
        
        return data_flow
    
    def _check_memory_interfaces(self) -> Dict[str, Any]:
        """æª¢æŸ¥è¨˜æ†¶æ•¸æ“šæ¥å£"""
        interfaces = {
            "interaction_data_api": {
                "description": "ç²å–ç”¨æˆ¶äº¤äº’æ•¸æ“šçš„API",
                "status": "unknown",
                "methods": []
            },
            "training_data_api": {
                "description": "ç²å–è¨“ç·´æ•¸æ“šçš„API", 
                "status": "unknown",
                "methods": []
            },
            "feedback_data_api": {
                "description": "ç²å–åé¥‹æ•¸æ“šçš„API",
                "status": "unknown",
                "methods": []
            }
        }
        
        # æª¢æŸ¥æ•¸æ“šæµæ–‡ä»¶ä¸­çš„æ¥å£å¯¦ç¾
        dataflow_file = os.path.join(self.base_path, "mcptool/adapters/rl_srt_dataflow_mcp.py")
        if os.path.exists(dataflow_file):
            with open(dataflow_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # æª¢æŸ¥è¨“ç·´æ•¸æ“šAPI
            if "get_training_data" in content:
                interfaces["training_data_api"]["status"] = "implemented"
                interfaces["training_data_api"]["methods"].append("get_training_data")
                
            # æª¢æŸ¥äº¤äº’æ•¸æ“šè™•ç†
            if "_prepare_rl_data" in content:
                interfaces["interaction_data_api"]["status"] = "implemented"
                interfaces["interaction_data_api"]["methods"].append("_prepare_rl_data")
                
            # æª¢æŸ¥åé¥‹è™•ç†
            if "reward" in content and "quality_score" in content:
                interfaces["feedback_data_api"]["status"] = "implemented"
                interfaces["feedback_data_api"]["methods"].append("reward_extraction")
        
        return interfaces
    
    def _evaluate_overall_status(self, results: Dict[str, Any]) -> str:
        """è©•ä¼°æ•´é«”ç‹€æ…‹"""
        # æª¢æŸ¥æ ¸å¿ƒçµ„ä»¶å®Œæ•´æ€§
        components = results["components"]
        existing_components = sum(1 for comp in components.values() if comp["status"] == "exists")
        total_components = len(components)
        
        # æª¢æŸ¥é›†æˆé»å¯¦ç¾
        integration_points = results["integration_points"]
        implemented_integrations = sum(1 for point in integration_points.values() if point["status"] == "implemented")
        total_integrations = len(integration_points)
        
        # æª¢æŸ¥æ¥å£å®Œæ•´æ€§
        interfaces = results["memory_interfaces"]
        implemented_interfaces = sum(1 for iface in interfaces.values() if iface["status"] == "implemented")
        total_interfaces = len(interfaces)
        
        # è¨ˆç®—å®Œæ•´æ€§åˆ†æ•¸
        component_score = existing_components / total_components
        integration_score = implemented_integrations / total_integrations if total_integrations > 0 else 0
        interface_score = implemented_interfaces / total_interfaces if total_interfaces > 0 else 0
        
        overall_score = (component_score + integration_score + interface_score) / 3
        
        if overall_score >= 0.8:
            return "complete"
        elif overall_score >= 0.6:
            return "mostly_complete"
        elif overall_score >= 0.4:
            return "partially_complete"
        else:
            return "incomplete"


def main():
    """ä¸»æª¢æŸ¥å‡½æ•¸"""
    print("ğŸ” RL_SRT + ç•°æ­¥RL èˆ‡çµ±ä¸€è¨˜æ†¶æ¨¡å¡Šé›†æˆå®Œæ•´æ€§æª¢æŸ¥\\n")
    
    checker = RLMemoryIntegrationChecker()
    results = checker.check_integration_completeness()
    
    # é¡¯ç¤ºçµæœ
    print(f"ğŸ“Š æ•´é«”ç‹€æ…‹: {results['overall_status']}")
    print()
    
    print("ğŸ§© æ ¸å¿ƒçµ„ä»¶:")
    for name, info in results["components"].items():
        status_emoji = "âœ…" if info["status"] == "exists" else "âŒ"
        print(f"  {status_emoji} {name}: {info['status']}")
        if info["features"]:
            print(f"     åŠŸèƒ½: {', '.join(info['features'])}")
    print()
    
    print("ğŸ”— é›†æˆé»:")
    for name, info in results["integration_points"].items():
        status_emoji = "âœ…" if info["status"] == "implemented" else "â“"
        print(f"  {status_emoji} {name}: {info['status']}")
        print(f"     {info['description']}")
        if info["implementation"]:
            print(f"     å¯¦ç¾: {', '.join(info['implementation'])}")
    print()
    
    print("ğŸ“¡ è¨˜æ†¶æ•¸æ“šæ¥å£:")
    for name, info in results["memory_interfaces"].items():
        status_emoji = "âœ…" if info["status"] == "implemented" else "â“"
        print(f"  {status_emoji} {name}: {info['status']}")
        print(f"     {info['description']}")
        if info["methods"]:
            print(f"     æ–¹æ³•: {', '.join(info['methods'])}")
    print()
    
    print("ğŸ“ˆ æ•¸æ“šæµåˆ†æ:")
    data_flow = results["data_flow_analysis"]
    print(f"  è¨˜æ†¶æ•¸æ“šæº: {len(data_flow['memory_data_sources'])} å€‹")
    print(f"  RLè¨“ç·´ç®¡é“: {len(data_flow['rl_training_pipeline'])} å€‹")
    
    # ä¿å­˜è©³ç´°çµæœ
    with open("/home/ubuntu/Powerauto.ai/rl_memory_integration_check.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("\\nğŸ“„ è©³ç´°çµæœå·²ä¿å­˜åˆ°: rl_memory_integration_check.json")

if __name__ == "__main__":
    main()

