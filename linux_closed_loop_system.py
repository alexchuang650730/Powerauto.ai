#!/usr/bin/env python3
"""
PowerAutomation Linuxç’°å¢ƒé–‰ç’°ç³»çµ±

æ•´åˆManusäº¤äº’åˆ†é¡ã€KiloCodeå·¥å…·ç”Ÿæˆã€æ–‡æª”åˆ†æç­‰åŠŸèƒ½
å¯¦ç¾å®Œæ•´çš„Linuxç’°å¢ƒé–‰ç’°å­¸ç¿’å’Œæ”¹é€²ç³»çµ±
"""

import os
import json
import time
import hashlib
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import logging
import subprocess
import shutil
import pandas as pd
import numpy as np
from PIL import Image
import requests

# å°å…¥ä¹‹å‰çš„ç³»çµ±
from interaction_log_manager import InteractionLogManager, InteractionType, DeliverableType
from rl_srt_learning_system import RLSRTLearningEngine, LearningExperience, LearningMode

class DocumentType(Enum):
    """æ–‡æª”é¡å‹æšèˆ‰"""
    BUSINESS_PPT = "business_ppt"
    TECHNICAL_DOC = "technical_doc"
    FINANCIAL_REPORT = "financial_report"
    MARKET_ANALYSIS = "market_analysis"
    PRODUCT_SPEC = "product_spec"
    INVESTMENT_PITCH = "investment_pitch"

class AnalysisTemplate(Enum):
    """åˆ†ææ¨¡æ¿æšèˆ‰"""
    BUSINESS_STRATEGY = "business_strategy"
    TECHNICAL_REVIEW = "technical_review"
    FINANCIAL_ANALYSIS = "financial_analysis"
    COMPETITIVE_ANALYSIS = "competitive_analysis"
    RISK_ASSESSMENT = "risk_assessment"
    PERFORMANCE_METRICS = "performance_metrics"

@dataclass
class DocumentAnalysisTask:
    """æ–‡æª”åˆ†æä»»å‹™"""
    task_id: str
    document_path: str
    document_type: DocumentType
    analysis_template: AnalysisTemplate
    user_requirements: str
    created_at: str
    status: str
    results: Dict[str, Any]

@dataclass
class KiloCodeTool:
    """KiloCodeå·¥å…·"""
    tool_id: str
    name: str
    description: str
    generated_code: str
    input_parameters: Dict[str, Any]
    output_format: str
    execution_count: int
    success_rate: float
    created_at: str

class LinuxClosedLoopSystem:
    """Linuxç’°å¢ƒé–‰ç’°ç³»çµ±"""
    
    def __init__(self, base_dir: str = "/home/ubuntu/Powerauto.ai/linux_closed_loop"):
        self.base_dir = Path(base_dir)
        self.setup_system()
        
        # åˆå§‹åŒ–å­ç³»çµ±
        self.log_manager = InteractionLogManager()
        self.rl_srt_engine = RLSRTLearningEngine(self.log_manager)
        
        # ç³»çµ±ç‹€æ…‹
        self.kilocode_tools = {}
        self.analysis_templates = {}
        self.execution_history = []
        self.learning_metrics = {}
        
    def setup_system(self):
        """è¨­ç½®ç³»çµ±"""
        directories = [
            "documents",
            "analysis_results",
            "kilocode_tools",
            "templates",
            "execution_logs",
            "learning_data",
            "performance_metrics",
            "closed_loop_reports"
        ]
        
        for directory in directories:
            (self.base_dir / directory).mkdir(parents=True, exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"âœ… Linuxé–‰ç’°ç³»çµ±å·²è¨­ç½®: {self.base_dir}")
        
        # åˆå§‹åŒ–åˆ†ææ¨¡æ¿
        self.initialize_analysis_templates()
    
    def initialize_analysis_templates(self):
        """åˆå§‹åŒ–åˆ†ææ¨¡æ¿"""
        templates = {
            AnalysisTemplate.BUSINESS_STRATEGY: {
                "name": "å•†æ¥­ç­–ç•¥åˆ†æ",
                "description": "åˆ†æå•†æ¥­è¨ˆåŠƒã€å¸‚å ´ç­–ç•¥ã€ç«¶çˆ­å„ªå‹¢",
                "analysis_points": [
                    "å¸‚å ´æ©Ÿæœƒè©•ä¼°",
                    "ç«¶çˆ­å„ªå‹¢åˆ†æ", 
                    "å•†æ¥­æ¨¡å¼è©•ä¼°",
                    "é¢¨éšªå› ç´ è­˜åˆ¥",
                    "è²¡å‹™å¯è¡Œæ€§",
                    "åŸ·è¡Œè¨ˆåŠƒè©•ä¼°"
                ],
                "output_format": "structured_report",
                "kilocode_tools": ["document_parser", "data_analyzer", "report_generator"]
            },
            AnalysisTemplate.TECHNICAL_REVIEW: {
                "name": "æŠ€è¡“æ–‡æª”å¯©æŸ¥",
                "description": "å¯©æŸ¥æŠ€è¡“æ¶æ§‹ã€ä»£ç¢¼è³ªé‡ã€ç³»çµ±è¨­è¨ˆ",
                "analysis_points": [
                    "æ¶æ§‹è¨­è¨ˆè©•ä¼°",
                    "æŠ€è¡“é¸å‹åˆ†æ",
                    "æ€§èƒ½æŒ‡æ¨™è©•ä¼°",
                    "å®‰å…¨æ€§å¯©æŸ¥",
                    "å¯æ“´å±•æ€§åˆ†æ",
                    "ç¶­è­·æ€§è©•ä¼°"
                ],
                "output_format": "technical_report",
                "kilocode_tools": ["code_analyzer", "architecture_reviewer", "security_scanner"]
            },
            AnalysisTemplate.FINANCIAL_ANALYSIS: {
                "name": "è²¡å‹™åˆ†æ",
                "description": "åˆ†æè²¡å‹™å ±è¡¨ã€æŠ•è³‡å›å ±ã€æˆæœ¬æ•ˆç›Š",
                "analysis_points": [
                    "æ”¶å…¥åˆ†æ",
                    "æˆæœ¬çµæ§‹åˆ†æ",
                    "ç›ˆåˆ©èƒ½åŠ›è©•ä¼°",
                    "ç¾é‡‘æµåˆ†æ",
                    "æŠ•è³‡å›å ±ç‡",
                    "è²¡å‹™é¢¨éšªè©•ä¼°"
                ],
                "output_format": "financial_dashboard",
                "kilocode_tools": ["financial_calculator", "trend_analyzer", "risk_assessor"]
            }
        }
        
        for template_type, template_data in templates.items():
            template_file = self.base_dir / "templates" / f"{template_type.value}.json"
            with open(template_file, 'w', encoding='utf-8') as f:
                json.dump(template_data, f, indent=2, ensure_ascii=False)
        
        self.analysis_templates = templates
        self.logger.info(f"âœ… åˆå§‹åŒ–äº† {len(templates)} å€‹åˆ†ææ¨¡æ¿")
    
    def create_kilocode_tool(self, name: str, description: str, 
                           task_requirements: str) -> KiloCodeTool:
        """å‰µå»ºKiloCodeå·¥å…·"""
        tool_id = hashlib.md5(f"{name}{time.time()}".encode()).hexdigest()[:12]
        
        # æ ¹æ“šéœ€æ±‚ç”Ÿæˆä»£ç¢¼
        generated_code = self.generate_tool_code(name, description, task_requirements)
        
        tool = KiloCodeTool(
            tool_id=tool_id,
            name=name,
            description=description,
            generated_code=generated_code,
            input_parameters=self.extract_input_parameters(generated_code),
            output_format="json",
            execution_count=0,
            success_rate=0.0,
            created_at=datetime.now().isoformat()
        )
        
        # ä¿å­˜å·¥å…·
        tool_file = self.base_dir / "kilocode_tools" / f"{tool_id}.py"
        with open(tool_file, 'w', encoding='utf-8') as f:
            f.write(generated_code)
        
        # ä¿å­˜å·¥å…·å…ƒæ•¸æ“š
        metadata_file = self.base_dir / "kilocode_tools" / f"{tool_id}_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            tool_dict = asdict(tool)
            json.dump(tool_dict, f, indent=2, ensure_ascii=False)
        
        self.kilocode_tools[tool_id] = tool
        self.logger.info(f"âœ… KiloCodeå·¥å…·å·²å‰µå»º: {tool_id} - {name}")
        
        return tool
    
    def generate_tool_code(self, name: str, description: str, requirements: str) -> str:
        """ç”Ÿæˆå·¥å…·ä»£ç¢¼"""
        if "æ–‡æª”åˆ†æ" in description or "document" in description.lower():
            return self.generate_document_analyzer_code(name, requirements)
        elif "æ•¸æ“šåˆ†æ" in description or "data" in description.lower():
            return self.generate_data_analyzer_code(name, requirements)
        elif "å ±å‘Šç”Ÿæˆ" in description or "report" in description.lower():
            return self.generate_report_generator_code(name, requirements)
        else:
            return self.generate_generic_tool_code(name, description, requirements)
    
    def generate_document_analyzer_code(self, name: str, requirements: str) -> str:
        """ç”Ÿæˆæ–‡æª”åˆ†æå™¨ä»£ç¢¼"""
        return f'''#!/usr/bin/env python3
"""
{name} - è‡ªå‹•ç”Ÿæˆçš„æ–‡æª”åˆ†æå·¥å…·
éœ€æ±‚: {requirements}
"""

import json
import os
from pathlib import Path
from datetime import datetime
import pandas as pd

def analyze_document(document_path: str, analysis_type: str = "general") -> dict:
    """åˆ†ææ–‡æª”å…§å®¹"""
    results = {{
        "tool_name": "{name}",
        "document_path": document_path,
        "analysis_type": analysis_type,
        "timestamp": datetime.now().isoformat(),
        "analysis_results": {{}},
        "success": False
    }}
    
    try:
        # æª¢æŸ¥æ–‡æª”æ˜¯å¦å­˜åœ¨
        if not os.path.exists(document_path):
            results["error"] = "æ–‡æª”ä¸å­˜åœ¨"
            return results
        
        # ç²å–æ–‡æª”åŸºæœ¬ä¿¡æ¯
        file_info = os.stat(document_path)
        results["analysis_results"]["file_info"] = {{
            "size_mb": file_info.st_size / (1024 * 1024),
            "modified_time": datetime.fromtimestamp(file_info.st_mtime).isoformat(),
            "file_extension": Path(document_path).suffix
        }}
        
        # æ ¹æ“šæ–‡ä»¶é¡å‹é€²è¡Œåˆ†æ
        file_ext = Path(document_path).suffix.lower()
        
        if file_ext in ['.txt', '.md']:
            results["analysis_results"]["content_analysis"] = analyze_text_content(document_path)
        elif file_ext in ['.json']:
            results["analysis_results"]["json_analysis"] = analyze_json_content(document_path)
        elif file_ext in ['.csv']:
            results["analysis_results"]["data_analysis"] = analyze_csv_content(document_path)
        else:
            results["analysis_results"]["general_info"] = "æ–‡æª”é¡å‹å·²è­˜åˆ¥ï¼Œä½†éœ€è¦å°ˆé–€çš„è§£æå™¨"
        
        results["success"] = True
        
    except Exception as e:
        results["error"] = str(e)
    
    return results

def analyze_text_content(file_path: str) -> dict:
    """åˆ†ææ–‡æœ¬å…§å®¹"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    return {{
        "character_count": len(content),
        "word_count": len(content.split()),
        "line_count": len(content.split('\\n')),
        "has_chinese": any('\\u4e00' <= char <= '\\u9fff' for char in content),
        "sample_content": content[:200] + "..." if len(content) > 200 else content
    }}

def analyze_json_content(file_path: str) -> dict:
    """åˆ†æJSONå…§å®¹"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    return {{
        "data_type": type(data).__name__,
        "keys_count": len(data.keys()) if isinstance(data, dict) else "N/A",
        "items_count": len(data) if isinstance(data, (list, dict)) else "N/A",
        "structure_sample": str(data)[:200] + "..." if len(str(data)) > 200 else str(data)
    }}

def analyze_csv_content(file_path: str) -> dict:
    """åˆ†æCSVå…§å®¹"""
    df = pd.read_csv(file_path)
    
    return {{
        "rows_count": len(df),
        "columns_count": len(df.columns),
        "columns": df.columns.tolist(),
        "data_types": df.dtypes.to_dict(),
        "sample_data": df.head().to_dict()
    }}

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        result = analyze_document(sys.argv[1])
        print(json.dumps(result, indent=2, ensure_ascii=False))
    else:
        print("ä½¿ç”¨æ–¹æ³•: python {{}} <document_path>".format(sys.argv[0]))
'''
    
    def generate_data_analyzer_code(self, name: str, requirements: str) -> str:
        """ç”Ÿæˆæ•¸æ“šåˆ†æå™¨ä»£ç¢¼"""
        return f'''#!/usr/bin/env python3
"""
{name} - è‡ªå‹•ç”Ÿæˆçš„æ•¸æ“šåˆ†æå·¥å…·
éœ€æ±‚: {requirements}
"""

import json
import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_data(data_source: str, analysis_type: str = "descriptive") -> dict:
    """åˆ†ææ•¸æ“š"""
    results = {{
        "tool_name": "{name}",
        "data_source": data_source,
        "analysis_type": analysis_type,
        "timestamp": datetime.now().isoformat(),
        "analysis_results": {{}},
        "success": False
    }}
    
    try:
        # åŠ è¼‰æ•¸æ“š
        if data_source.endswith('.csv'):
            df = pd.read_csv(data_source)
        elif data_source.endswith('.json'):
            df = pd.read_json(data_source)
        else:
            results["error"] = "ä¸æ”¯æŒçš„æ•¸æ“šæ ¼å¼"
            return results
        
        # åŸºæœ¬çµ±è¨ˆåˆ†æ
        results["analysis_results"]["basic_stats"] = {{
            "shape": df.shape,
            "columns": df.columns.tolist(),
            "data_types": df.dtypes.to_dict(),
            "missing_values": df.isnull().sum().to_dict(),
            "memory_usage": df.memory_usage(deep=True).sum()
        }}
        
        # æ•¸å€¼åˆ—çµ±è¨ˆ
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) > 0:
            results["analysis_results"]["numeric_analysis"] = {{
                "descriptive_stats": df[numeric_columns].describe().to_dict(),
                "correlation_matrix": df[numeric_columns].corr().to_dict()
            }}
        
        # åˆ†é¡åˆ—åˆ†æ
        categorical_columns = df.select_dtypes(include=['object']).columns
        if len(categorical_columns) > 0:
            results["analysis_results"]["categorical_analysis"] = {{}}
            for col in categorical_columns[:5]:  # é™åˆ¶å‰5åˆ—
                results["analysis_results"]["categorical_analysis"][col] = {{
                    "unique_count": df[col].nunique(),
                    "top_values": df[col].value_counts().head().to_dict()
                }}
        
        results["success"] = True
        
    except Exception as e:
        results["error"] = str(e)
    
    return results

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        result = analyze_data(sys.argv[1])
        print(json.dumps(result, indent=2, ensure_ascii=False))
    else:
        print("ä½¿ç”¨æ–¹æ³•: python {{}} <data_file>".format(sys.argv[0]))
'''
    
    def generate_report_generator_code(self, name: str, requirements: str) -> str:
        """ç”Ÿæˆå ±å‘Šç”Ÿæˆå™¨ä»£ç¢¼"""
        return f'''#!/usr/bin/env python3
"""
{name} - è‡ªå‹•ç”Ÿæˆçš„å ±å‘Šç”Ÿæˆå·¥å…·
éœ€æ±‚: {requirements}
"""

import json
from datetime import datetime
from pathlib import Path

def generate_report(data: dict, report_type: str = "analysis", output_path: str = None) -> dict:
    """ç”Ÿæˆå ±å‘Š"""
    results = {{
        "tool_name": "{name}",
        "report_type": report_type,
        "timestamp": datetime.now().isoformat(),
        "success": False
    }}
    
    try:
        # ç”Ÿæˆå ±å‘Šå…§å®¹
        if report_type == "analysis":
            report_content = generate_analysis_report(data)
        elif report_type == "summary":
            report_content = generate_summary_report(data)
        else:
            report_content = generate_generic_report(data)
        
        # ä¿å­˜å ±å‘Š
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            results["output_file"] = output_path
        
        results["report_content"] = report_content
        results["content_length"] = len(report_content)
        results["success"] = True
        
    except Exception as e:
        results["error"] = str(e)
    
    return results

def generate_analysis_report(data: dict) -> str:
    """ç”Ÿæˆåˆ†æå ±å‘Š"""
    report = f"""# åˆ†æå ±å‘Š

## å ±å‘Šæ¦‚æ³
- ç”Ÿæˆæ™‚é–“: {{datetime.now().isoformat()}}
- æ•¸æ“šä¾†æº: {{data.get('source', 'Unknown')}}

## åˆ†æçµæœ

"""
    
    for key, value in data.items():
        if isinstance(value, dict):
            report += f"### {{key}}\\n"
            for sub_key, sub_value in value.items():
                report += f"- **{{sub_key}}**: {{sub_value}}\\n"
            report += "\\n"
        else:
            report += f"- **{{key}}**: {{value}}\\n"
    
    return report

def generate_summary_report(data: dict) -> str:
    """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
    return f"""# æ‘˜è¦å ±å‘Š

**ç”Ÿæˆæ™‚é–“**: {{datetime.now().isoformat()}}

## é—œéµæŒ‡æ¨™
{{json.dumps(data, indent=2, ensure_ascii=False)}}

---
*æ­¤å ±å‘Šç”±PowerAutomationè‡ªå‹•ç”Ÿæˆ*
"""

def generate_generic_report(data: dict) -> str:
    """ç”Ÿæˆé€šç”¨å ±å‘Š"""
    return f"""# é€šç”¨å ±å‘Š

{{json.dumps(data, indent=2, ensure_ascii=False)}}
"""

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 2:
        with open(sys.argv[1], 'r') as f:
            data = json.load(f)
        result = generate_report(data, sys.argv[2] if len(sys.argv) > 2 else "analysis")
        print(json.dumps(result, indent=2, ensure_ascii=False))
    else:
        print("ä½¿ç”¨æ–¹æ³•: python {{}} <data_file> [report_type]".format(sys.argv[0]))
'''
    
    def generate_generic_tool_code(self, name: str, description: str, requirements: str) -> str:
        """ç”Ÿæˆé€šç”¨å·¥å…·ä»£ç¢¼"""
        return f'''#!/usr/bin/env python3
"""
{name} - è‡ªå‹•ç”Ÿæˆçš„å·¥å…·
æè¿°: {description}
éœ€æ±‚: {requirements}
"""

import json
from datetime import datetime

def execute_tool(input_data: dict) -> dict:
    """åŸ·è¡Œå·¥å…·"""
    results = {{
        "tool_name": "{name}",
        "description": "{description}",
        "timestamp": datetime.now().isoformat(),
        "input_data": input_data,
        "success": False
    }}
    
    try:
        # å·¥å…·é‚è¼¯
        processed_data = process_input(input_data)
        results["output_data"] = processed_data
        results["success"] = True
        
    except Exception as e:
        results["error"] = str(e)
    
    return results

def process_input(data: dict) -> dict:
    """è™•ç†è¼¸å…¥æ•¸æ“š"""
    # åŸºæœ¬è™•ç†é‚è¼¯
    return {{
        "processed": True,
        "original_keys": list(data.keys()),
        "processing_time": datetime.now().isoformat()
    }}

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        with open(sys.argv[1], 'r') as f:
            input_data = json.load(f)
        result = execute_tool(input_data)
        print(json.dumps(result, indent=2, ensure_ascii=False))
    else:
        print("ä½¿ç”¨æ–¹æ³•: python {{}} <input_file>".format(sys.argv[0]))
'''
    
    def extract_input_parameters(self, code: str) -> Dict[str, Any]:
        """æå–è¼¸å…¥åƒæ•¸"""
        # ç°¡åŒ–çš„åƒæ•¸æå–é‚è¼¯
        parameters = {}
        
        if "document_path" in code:
            parameters["document_path"] = {"type": "string", "description": "æ–‡æª”è·¯å¾‘"}
        if "data_source" in code:
            parameters["data_source"] = {"type": "string", "description": "æ•¸æ“šæºè·¯å¾‘"}
        if "analysis_type" in code:
            parameters["analysis_type"] = {"type": "string", "description": "åˆ†æé¡å‹"}
        if "output_path" in code:
            parameters["output_path"] = {"type": "string", "description": "è¼¸å‡ºè·¯å¾‘"}
        
        return parameters
    
    def execute_kilocode_tool(self, tool_id: str, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡ŒKiloCodeå·¥å…·"""
        if tool_id not in self.kilocode_tools:
            return {"error": f"å·¥å…·ä¸å­˜åœ¨: {tool_id}"}
        
        tool = self.kilocode_tools[tool_id]
        tool_file = self.base_dir / "kilocode_tools" / f"{tool_id}.py"
        
        execution_result = {
            "tool_id": tool_id,
            "tool_name": tool.name,
            "execution_time": None,
            "success": False,
            "output": {},
            "error": None
        }
        
        try:
            start_time = time.time()
            
            # æº–å‚™è¼¸å…¥æ–‡ä»¶
            input_file = self.base_dir / "execution_logs" / f"input_{tool_id}_{int(time.time())}.json"
            with open(input_file, 'w', encoding='utf-8') as f:
                json.dump(input_data, f, indent=2, ensure_ascii=False)
            
            # åŸ·è¡Œå·¥å…·
            result = subprocess.run(
                ["python3", str(tool_file), str(input_file)],
                capture_output=True,
                text=True,
                timeout=60
            )
            
            execution_time = time.time() - start_time
            execution_result["execution_time"] = execution_time
            
            if result.returncode == 0:
                try:
                    output_data = json.loads(result.stdout)
                    execution_result["output"] = output_data
                    execution_result["success"] = True
                    
                    # æ›´æ–°å·¥å…·çµ±è¨ˆ
                    tool.execution_count += 1
                    tool.success_rate = (tool.success_rate * (tool.execution_count - 1) + 1) / tool.execution_count
                    
                except json.JSONDecodeError:
                    execution_result["output"] = {"raw_output": result.stdout}
                    execution_result["success"] = True
            else:
                execution_result["error"] = result.stderr
                
                # æ›´æ–°å·¥å…·çµ±è¨ˆ
                tool.execution_count += 1
                tool.success_rate = (tool.success_rate * (tool.execution_count - 1)) / tool.execution_count
            
        except subprocess.TimeoutExpired:
            execution_result["error"] = "åŸ·è¡Œè¶…æ™‚"
        except Exception as e:
            execution_result["error"] = str(e)
        
        # è¨˜éŒ„åŸ·è¡Œæ­·å²
        self.execution_history.append(execution_result)
        
        # ä¿å­˜åŸ·è¡Œæ—¥èªŒ
        log_file = self.base_dir / "execution_logs" / f"execution_{tool_id}_{int(time.time())}.json"
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(execution_result, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"âœ… å·¥å…·åŸ·è¡Œå®Œæˆ: {tool_id} - æˆåŠŸ: {execution_result['success']}")
        
        return execution_result
    
    def analyze_document_with_template(self, document_path: str, 
                                     document_type: DocumentType,
                                     analysis_template: AnalysisTemplate) -> DocumentAnalysisTask:
        """ä½¿ç”¨æ¨¡æ¿åˆ†ææ–‡æª”"""
        task_id = hashlib.md5(f"{document_path}{time.time()}".encode()).hexdigest()[:12]
        
        task = DocumentAnalysisTask(
            task_id=task_id,
            document_path=document_path,
            document_type=document_type,
            analysis_template=analysis_template,
            user_requirements=f"ä½¿ç”¨{analysis_template.value}æ¨¡æ¿åˆ†æ{document_type.value}æ–‡æª”",
            created_at=datetime.now().isoformat(),
            status="processing",
            results={}
        )
        
        try:
            # ç²å–åˆ†ææ¨¡æ¿
            template_data = self.analysis_templates.get(analysis_template, {})
            required_tools = template_data.get("kilocode_tools", [])
            
            # ç‚ºæ¯å€‹æ‰€éœ€å·¥å…·å‰µå»ºæˆ–ç²å–KiloCodeå·¥å…·
            analysis_results = {}
            
            for tool_name in required_tools:
                # æª¢æŸ¥æ˜¯å¦å·²æœ‰ç›¸æ‡‰å·¥å…·
                existing_tool = self.find_tool_by_name(tool_name)
                
                if not existing_tool:
                    # å‰µå»ºæ–°å·¥å…·
                    tool_description = f"{tool_name}ç”¨æ–¼{analysis_template.value}åˆ†æ"
                    tool_requirements = f"åˆ†æ{document_type.value}æ–‡æª”çš„{tool_name}åŠŸèƒ½"
                    existing_tool = self.create_kilocode_tool(tool_name, tool_description, tool_requirements)
                
                # åŸ·è¡Œå·¥å…·
                input_data = {
                    "document_path": document_path,
                    "analysis_type": analysis_template.value,
                    "document_type": document_type.value
                }
                
                execution_result = self.execute_kilocode_tool(existing_tool.tool_id, input_data)
                analysis_results[tool_name] = execution_result
            
            task.results = analysis_results
            task.status = "completed"
            
        except Exception as e:
            task.status = "failed"
            task.results = {"error": str(e)}
        
        # ä¿å­˜ä»»å‹™çµæœ
        task_file = self.base_dir / "analysis_results" / f"{task_id}.json"
        with open(task_file, 'w', encoding='utf-8') as f:
            task_dict = asdict(task)
            # è½‰æ›æšèˆ‰ç‚ºå­—ç¬¦ä¸²
            task_dict['document_type'] = task.document_type.value
            task_dict['analysis_template'] = task.analysis_template.value
            json.dump(task_dict, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"âœ… æ–‡æª”åˆ†æä»»å‹™å®Œæˆ: {task_id} - ç‹€æ…‹: {task.status}")
        
        return task
    
    def find_tool_by_name(self, tool_name: str) -> Optional[KiloCodeTool]:
        """æ ¹æ“šåç¨±æŸ¥æ‰¾å·¥å…·"""
        for tool in self.kilocode_tools.values():
            if tool_name in tool.name or tool.name in tool_name:
                return tool
        return None
    
    def create_learning_experience_from_execution(self, execution_result: Dict[str, Any]) -> LearningExperience:
        """å¾åŸ·è¡Œçµæœå‰µå»ºå­¸ç¿’ç¶“é©—"""
        experience_id = hashlib.md5(f"{execution_result.get('tool_id', '')}{time.time()}".encode()).hexdigest()[:12]
        
        # æ§‹å»ºç‹€æ…‹
        state = {
            "tool_id": execution_result.get("tool_id", ""),
            "tool_name": execution_result.get("tool_name", ""),
            "input_complexity": len(str(execution_result.get("input_data", {}))),
            "execution_context": "linux_environment"
        }
        
        # æ§‹å»ºå‹•ä½œ
        action = {
            "tool_execution": True,
            "execution_strategy": "kilocode_generated",
            "resource_usage": execution_result.get("execution_time", 0)
        }
        
        # è¨ˆç®—çå‹µ
        reward = 0.0
        if execution_result.get("success", False):
            reward += 0.5  # åŸºç¤æˆåŠŸçå‹µ
            
            # åŸºæ–¼åŸ·è¡Œæ™‚é–“çš„æ•ˆç‡çå‹µ
            exec_time = execution_result.get("execution_time", 0)
            if exec_time < 5:  # 5ç§’å…§å®Œæˆ
                reward += 0.3
            elif exec_time < 10:  # 10ç§’å…§å®Œæˆ
                reward += 0.2
            
            # åŸºæ–¼è¼¸å‡ºè³ªé‡çš„çå‹µ
            output = execution_result.get("output", {})
            if isinstance(output, dict) and len(output) > 0:
                reward += 0.2
        
        # æ§‹å»ºä¸‹ä¸€ç‹€æ…‹
        next_state = {
            "execution_completed": execution_result.get("success", False),
            "output_quality": len(str(execution_result.get("output", {}))),
            "error_occurred": execution_result.get("error") is not None,
            "tool_performance": execution_result.get("success", False)
        }
        
        experience = LearningExperience(
            experience_id=experience_id,
            timestamp=datetime.now().isoformat(),
            state=state,
            action=action,
            reward=reward,
            next_state=next_state,
            metadata={
                "source": "kilocode_execution",
                "tool_id": execution_result.get("tool_id", ""),
                "execution_time": execution_result.get("execution_time", 0)
            },
            learning_mode=LearningMode.SYNCHRONOUS
        )
        
        return experience
    
    def run_closed_loop_cycle(self, user_request: str, document_path: str = None) -> Dict[str, Any]:
        """é‹è¡Œé–‰ç’°å¾ªç’°"""
        cycle_id = hashlib.md5(f"{user_request}{time.time()}".encode()).hexdigest()[:12]
        
        cycle_result = {
            "cycle_id": cycle_id,
            "user_request": user_request,
            "timestamp": datetime.now().isoformat(),
            "phases": {},
            "learning_experiences": [],
            "performance_metrics": {},
            "success": False
        }
        
        try:
            # Phase 1: Manusåˆ†æå’Œç†è§£
            self.logger.info(f"ğŸ§  Phase 1: Manusåˆ†æç”¨æˆ¶è«‹æ±‚")
            manus_analysis = self.analyze_user_request(user_request, document_path)
            cycle_result["phases"]["manus_analysis"] = manus_analysis
            
            # Phase 2: KiloCodeå·¥å…·ç”Ÿæˆ
            self.logger.info(f"ğŸ”§ Phase 2: KiloCodeå·¥å…·ç”Ÿæˆ")
            required_tools = manus_analysis.get("required_tools", [])
            generated_tools = []
            
            for tool_spec in required_tools:
                tool = self.create_kilocode_tool(
                    tool_spec["name"],
                    tool_spec["description"],
                    tool_spec["requirements"]
                )
                generated_tools.append(tool)
            
            cycle_result["phases"]["tool_generation"] = {
                "tools_created": len(generated_tools),
                "tool_ids": [t.tool_id for t in generated_tools]
            }
            
            # Phase 3: Linuxç’°å¢ƒåŸ·è¡Œ
            self.logger.info(f"âš¡ Phase 3: Linuxç’°å¢ƒåŸ·è¡Œ")
            execution_results = []
            
            for tool in generated_tools:
                input_data = manus_analysis.get("tool_inputs", {}).get(tool.name, {})
                if document_path:
                    input_data["document_path"] = document_path
                
                exec_result = self.execute_kilocode_tool(tool.tool_id, input_data)
                execution_results.append(exec_result)
                
                # å‰µå»ºå­¸ç¿’ç¶“é©—
                learning_exp = self.create_learning_experience_from_execution(exec_result)
                cycle_result["learning_experiences"].append(learning_exp)
            
            cycle_result["phases"]["execution"] = {
                "total_executions": len(execution_results),
                "successful_executions": len([r for r in execution_results if r.get("success", False)]),
                "execution_results": execution_results
            }
            
            # Phase 4: RL-SRTå­¸ç¿’
            self.logger.info(f"ğŸ§  Phase 4: RL-SRTå­¸ç¿’")
            if cycle_result["learning_experiences"]:
                learning_results = self.rl_srt_engine.synchronous_learning(cycle_result["learning_experiences"])
                cycle_result["phases"]["learning"] = learning_results
            
            # Phase 5: æ€§èƒ½è©•ä¼°å’Œåé¥‹
            self.logger.info(f"ğŸ“Š Phase 5: æ€§èƒ½è©•ä¼°")
            performance_metrics = self.calculate_cycle_performance(cycle_result)
            cycle_result["performance_metrics"] = performance_metrics
            
            cycle_result["success"] = True
            
        except Exception as e:
            cycle_result["error"] = str(e)
            self.logger.error(f"âŒ é–‰ç’°å¾ªç’°å¤±æ•—: {e}")
        
        # ä¿å­˜å¾ªç’°çµæœ
        cycle_file = self.base_dir / "closed_loop_reports" / f"cycle_{cycle_id}.json"
        with open(cycle_file, 'w', encoding='utf-8') as f:
            # è™•ç†ä¸å¯åºåˆ—åŒ–çš„å°è±¡
            serializable_result = self.make_serializable(cycle_result)
            json.dump(serializable_result, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"âœ… é–‰ç’°å¾ªç’°å®Œæˆ: {cycle_id} - æˆåŠŸ: {cycle_result['success']}")
        
        return cycle_result
    
    def analyze_user_request(self, user_request: str, document_path: str = None) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ¶è«‹æ±‚"""
        analysis = {
            "request": user_request,
            "document_path": document_path,
            "intent": "unknown",
            "required_tools": [],
            "tool_inputs": {},
            "complexity": "medium"
        }
        
        # ç°¡åŒ–çš„æ„åœ–è­˜åˆ¥
        request_lower = user_request.lower()
        
        if any(word in request_lower for word in ["åˆ†æ", "analyze", "review"]):
            analysis["intent"] = "analysis"
            if document_path:
                analysis["required_tools"].append({
                    "name": "document_analyzer",
                    "description": "åˆ†ææ–‡æª”å…§å®¹",
                    "requirements": f"åˆ†ææ–‡æª”: {user_request}"
                })
                analysis["tool_inputs"]["document_analyzer"] = {"document_path": document_path}
        
        if any(word in request_lower for word in ["å ±å‘Š", "report", "ç¸½çµ"]):
            analysis["intent"] = "reporting"
            analysis["required_tools"].append({
                "name": "report_generator",
                "description": "ç”Ÿæˆå ±å‘Š",
                "requirements": f"ç”Ÿæˆå ±å‘Š: {user_request}"
            })
        
        if any(word in request_lower for word in ["æ•¸æ“š", "data", "çµ±è¨ˆ"]):
            analysis["intent"] = "data_analysis"
            analysis["required_tools"].append({
                "name": "data_analyzer",
                "description": "æ•¸æ“šåˆ†æ",
                "requirements": f"æ•¸æ“šåˆ†æ: {user_request}"
            })
        
        # å¦‚æœæ²’æœ‰è­˜åˆ¥å‡ºç‰¹å®šæ„åœ–ï¼Œå‰µå»ºé€šç”¨å·¥å…·
        if not analysis["required_tools"]:
            analysis["required_tools"].append({
                "name": "general_processor",
                "description": "é€šç”¨è™•ç†å·¥å…·",
                "requirements": user_request
            })
        
        return analysis
    
    def calculate_cycle_performance(self, cycle_result: Dict[str, Any]) -> Dict[str, Any]:
        """è¨ˆç®—å¾ªç’°æ€§èƒ½"""
        metrics = {
            "overall_success_rate": 0.0,
            "average_execution_time": 0.0,
            "tool_efficiency": 0.0,
            "learning_improvement": 0.0,
            "user_satisfaction_estimate": 0.0
        }
        
        # è¨ˆç®—æ•´é«”æˆåŠŸç‡
        execution_phase = cycle_result.get("phases", {}).get("execution", {})
        total_executions = execution_phase.get("total_executions", 0)
        successful_executions = execution_phase.get("successful_executions", 0)
        
        if total_executions > 0:
            metrics["overall_success_rate"] = successful_executions / total_executions
        
        # è¨ˆç®—å¹³å‡åŸ·è¡Œæ™‚é–“
        execution_results = execution_phase.get("execution_results", [])
        execution_times = [r.get("execution_time", 0) for r in execution_results if r.get("execution_time")]
        
        if execution_times:
            metrics["average_execution_time"] = np.mean(execution_times)
        
        # è¨ˆç®—å·¥å…·æ•ˆç‡
        if successful_executions > 0 and metrics["average_execution_time"] > 0:
            metrics["tool_efficiency"] = successful_executions / metrics["average_execution_time"]
        
        # ä¼°ç®—å­¸ç¿’æ”¹é€²
        learning_phase = cycle_result.get("phases", {}).get("learning", {})
        metrics["learning_improvement"] = learning_phase.get("performance_improvement", 0)
        
        # ä¼°ç®—ç”¨æˆ¶æ»¿æ„åº¦
        if metrics["overall_success_rate"] > 0.8 and metrics["average_execution_time"] < 10:
            metrics["user_satisfaction_estimate"] = 0.9
        elif metrics["overall_success_rate"] > 0.6:
            metrics["user_satisfaction_estimate"] = 0.7
        else:
            metrics["user_satisfaction_estimate"] = 0.5
        
        return metrics
    
    def make_serializable(self, obj: Any) -> Any:
        """ä½¿å°è±¡å¯åºåˆ—åŒ–"""
        if isinstance(obj, dict):
            return {k: self.make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self.make_serializable(item) for item in obj]
        elif hasattr(obj, '__dict__'):
            return self.make_serializable(obj.__dict__)
        elif isinstance(obj, (str, int, float, bool, type(None))):
            return obj
        else:
            return str(obj)
    
    def generate_closed_loop_report(self, cycle_results: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆé–‰ç’°å ±å‘Š"""
        report_content = f"""# PowerAutomation Linuxç’°å¢ƒé–‰ç’°ç³»çµ±å ±å‘Š

## ğŸ“Š ç³»çµ±æ¦‚æ³

- **å ±å‘Šç”Ÿæˆæ™‚é–“**: {datetime.now().isoformat()}
- **ç¸½å¾ªç’°æ•¸**: {len(cycle_results)}
- **ç³»çµ±ç‰ˆæœ¬**: Linuxé–‰ç’° v1.0

## ğŸ”„ é–‰ç’°æ€§èƒ½åˆ†æ

"""
        
        if cycle_results:
            # è¨ˆç®—ç¸½é«”æŒ‡æ¨™
            total_success = len([c for c in cycle_results if c.get("success", False)])
            success_rate = (total_success / len(cycle_results)) * 100
            
            avg_tools_per_cycle = np.mean([
                c.get("phases", {}).get("tool_generation", {}).get("tools_created", 0) 
                for c in cycle_results
            ])
            
            avg_execution_time = np.mean([
                c.get("performance_metrics", {}).get("average_execution_time", 0)
                for c in cycle_results
            ])
            
            report_content += f"""### ç¸½é«”æŒ‡æ¨™
- **æˆåŠŸç‡**: {success_rate:.1f}%
- **å¹³å‡æ¯å¾ªç’°å·¥å…·æ•¸**: {avg_tools_per_cycle:.1f}
- **å¹³å‡åŸ·è¡Œæ™‚é–“**: {avg_execution_time:.2f} ç§’

### å­¸ç¿’æ”¹é€²è¶¨å‹¢
"""
            
            for i, cycle in enumerate(cycle_results):
                metrics = cycle.get("performance_metrics", {})
                report_content += f"- **å¾ªç’° {i+1}**: æˆåŠŸç‡ {metrics.get('overall_success_rate', 0):.1%}, "
                report_content += f"ç”¨æˆ¶æ»¿æ„åº¦ {metrics.get('user_satisfaction_estimate', 0):.1%}\n"
        
        report_content += f"""
## ğŸ”§ KiloCodeå·¥å…·çµ±è¨ˆ

- **ç¸½å·¥å…·æ•¸**: {len(self.kilocode_tools)}
- **å·¥å…·é¡å‹**: æ–‡æª”åˆ†æå™¨ã€æ•¸æ“šåˆ†æå™¨ã€å ±å‘Šç”Ÿæˆå™¨ç­‰

### å·¥å…·æ€§èƒ½æ’è¡Œ
"""
        
        # å·¥å…·æ€§èƒ½æ’è¡Œ
        sorted_tools = sorted(
            self.kilocode_tools.values(),
            key=lambda t: t.success_rate,
            reverse=True
        )
        
        for i, tool in enumerate(sorted_tools[:5]):
            report_content += f"{i+1}. **{tool.name}**: æˆåŠŸç‡ {tool.success_rate:.1%}, åŸ·è¡Œæ¬¡æ•¸ {tool.execution_count}\n"
        
        report_content += f"""
## ğŸ§  RL-SRTå­¸ç¿’æ´å¯Ÿ

### å­¸ç¿’æ•ˆæœ
- **å­¸ç¿’ç¶“é©—ç¸½æ•¸**: {len(self.execution_history)}
- **å¹³å‡çå‹µ**: {np.mean([h.get('reward', 0) for h in self.execution_history if 'reward' in h]):.3f}

### æ”¹é€²å»ºè­°
1. **å·¥å…·å„ªåŒ–**: æå‡ä½æˆåŠŸç‡å·¥å…·çš„ç©©å®šæ€§
2. **å­¸ç¿’åŠ é€Ÿ**: å¢åŠ ç•°æ­¥å­¸ç¿’æ©Ÿåˆ¶
3. **æ¨¡æ¿æ“´å±•**: æ·»åŠ æ›´å¤šå°ˆæ¥­é ˜åŸŸæ¨¡æ¿

## ğŸ’¡ ç³»çµ±åƒ¹å€¼

### é–‰ç’°å„ªå‹¢
- âœ… **è‡ªæˆ‘æ”¹é€²**: æ¯æ¬¡åŸ·è¡Œéƒ½åœ¨å­¸ç¿’
- âœ… **å‹•æ…‹é©æ‡‰**: æ ¹æ“šéœ€æ±‚ç”Ÿæˆå·¥å…·
- âœ… **çœŸå¯¦åé¥‹**: åŸºæ–¼å¯¦éš›åŸ·è¡Œçµæœ
- âœ… **æŒçºŒå„ªåŒ–**: RL-SRTæŒçºŒæ”¹é€²æ€§èƒ½

---

*å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().isoformat()}*
"""
        
        return report_content

def main():
    """ä¸»å‡½æ•¸ - æ¼”ç¤ºLinuxé–‰ç’°ç³»çµ±"""
    
    # åˆå§‹åŒ–ç³»çµ±
    closed_loop_system = LinuxClosedLoopSystem()
    
    print("ğŸ”„ PowerAutomation Linuxé–‰ç’°ç³»çµ±æ¼”ç¤ºé–‹å§‹...")
    
    # å‰µå»ºç¤ºä¾‹æ–‡æª”
    sample_doc_path = closed_loop_system.base_dir / "documents" / "sample_business_plan.json"
    sample_data = {
        "company": "PowerAutomation",
        "market_size": "10B USD",
        "revenue_projection": [1000000, 5000000, 15000000],
        "key_features": ["KiloCode MCP", "RL-SRT Learning", "Cross-platform Testing"],
        "competitive_advantages": ["Dynamic tool creation", "Self-learning AI", "Real-time adaptation"]
    }
    
    with open(sample_doc_path, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ å‰µå»ºç¤ºä¾‹æ–‡æª”: {sample_doc_path}")
    
    # é‹è¡Œé–‰ç’°å¾ªç’°
    cycle_results = []
    
    # å¾ªç’°1: å•†æ¥­è¨ˆåŠƒåˆ†æ
    print("\nğŸ”„ é‹è¡Œé–‰ç’°å¾ªç’° 1: å•†æ¥­è¨ˆåŠƒåˆ†æ")
    cycle1 = closed_loop_system.run_closed_loop_cycle(
        "è«‹åˆ†æé€™å€‹å•†æ¥­è¨ˆåŠƒçš„å¯è¡Œæ€§å’Œç«¶çˆ­å„ªå‹¢",
        str(sample_doc_path)
    )
    cycle_results.append(cycle1)
    
    # å¾ªç’°2: æ•¸æ“šåˆ†æ
    print("\nğŸ”„ é‹è¡Œé–‰ç’°å¾ªç’° 2: æ•¸æ“šåˆ†æ")
    cycle2 = closed_loop_system.run_closed_loop_cycle(
        "åˆ†ææ”¶å…¥é æ¸¬æ•¸æ“šä¸¦ç”Ÿæˆè¶¨å‹¢å ±å‘Š",
        str(sample_doc_path)
    )
    cycle_results.append(cycle2)
    
    # å¾ªç’°3: ç«¶çˆ­åˆ†æ
    print("\nğŸ”„ é‹è¡Œé–‰ç’°å¾ªç’° 3: ç«¶çˆ­åˆ†æ")
    cycle3 = closed_loop_system.run_closed_loop_cycle(
        "è©•ä¼°ç«¶çˆ­å„ªå‹¢ä¸¦æä¾›æ”¹é€²å»ºè­°"
    )
    cycle_results.append(cycle3)
    
    # ç”Ÿæˆé–‰ç’°å ±å‘Š
    print("\nğŸ“Š ç”Ÿæˆé–‰ç’°ç³»çµ±å ±å‘Š...")
    report_content = closed_loop_system.generate_closed_loop_report(cycle_results)
    
    report_file = closed_loop_system.base_dir / "closed_loop_reports" / f"system_report_{int(time.time())}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"âœ… é–‰ç’°ç³»çµ±å ±å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    # é¡¯ç¤ºé—œéµæŒ‡æ¨™
    print(f"\nğŸ¯ Linuxé–‰ç’°ç³»çµ±æ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“ ç³»çµ±ç›®éŒ„: {closed_loop_system.base_dir}")
    print(f"ğŸ”§ å‰µå»ºå·¥å…·æ•¸: {len(closed_loop_system.kilocode_tools)}")
    print(f"âš¡ åŸ·è¡Œæ­·å²æ•¸: {len(closed_loop_system.execution_history)}")
    print(f"ğŸ”„ é–‰ç’°å¾ªç’°æ•¸: {len(cycle_results)}")
    print(f"ğŸ“Š ç³»çµ±å ±å‘Š: {report_file}")
    
    # é¡¯ç¤ºæˆåŠŸç‡
    successful_cycles = len([c for c in cycle_results if c.get("success", False)])
    success_rate = (successful_cycles / len(cycle_results)) * 100 if cycle_results else 0
    print(f"âœ… é–‰ç’°æˆåŠŸç‡: {success_rate:.1f}%")
    
    return closed_loop_system, cycle_results, report_file

if __name__ == "__main__":
    main()

