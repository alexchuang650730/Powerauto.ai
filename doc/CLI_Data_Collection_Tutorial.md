# PowerAutomation CLIæ•¸æ“šæ”¶é›†ç³»çµ±æ•™å­¸æŒ‡å—

## ğŸ“š ç›®éŒ„

1. [ç³»çµ±æ¦‚è¿°](#ç³»çµ±æ¦‚è¿°)
2. [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
3. [ç³»çµ±æ¶æ§‹](#ç³»çµ±æ¶æ§‹)
4. [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
5. [é…ç½®èªªæ˜](#é…ç½®èªªæ˜)
6. [æ•¸æ“šåˆ†æ](#æ•¸æ“šåˆ†æ)
7. [è³ªé‡æ§åˆ¶](#è³ªé‡æ§åˆ¶)
8. [éš±ç§ä¿è­·](#éš±ç§ä¿è­·)
9. [æœ€ä½³å¯¦è¸](#æœ€ä½³å¯¦è¸)
10. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
11. [APIåƒè€ƒ](#apiåƒè€ƒ)

---

## ğŸ¯ ç³»çµ±æ¦‚è¿°

PowerAutomation CLIæ•¸æ“šæ”¶é›†ç³»çµ±æ˜¯ä¸€å€‹æ™ºèƒ½çš„æ•¸æ“šç®¡ç†å¹³å°ï¼Œæ—¨åœ¨æ”¶é›†ã€åˆ†æå’Œåˆ©ç”¨CLIä½¿ç”¨æ•¸æ“šä¾†æŒçºŒæ”¹é€²ç³»çµ±æ€§èƒ½ã€‚

### æ ¸å¿ƒç‰¹æ€§

- **ğŸ”„ è‡ªå‹•æ•¸æ“šæ”¶é›†** - ç„¡æ„ŸçŸ¥æ”¶é›†CLIäº¤äº’æ•¸æ“š
- **ğŸ“Š æ™ºèƒ½åˆ†é¡åˆ†æ** - è‡ªå‹•åˆ†é¡å’Œåƒ¹å€¼è©•ä¼°
- **ğŸ›¡ï¸ éš±ç§ä¿è­·** - å¤šå±¤æ¬¡åŒ¿ååŒ–å’Œå®‰å…¨ä¿è­·
- **ğŸ¯ è¨“ç·´æ•¸æ“šç”Ÿæˆ** - è‡ªå‹•æ§‹å»ºé«˜è³ªé‡è¨“ç·´é›†
- **ğŸ“ˆ å¯¦æ™‚ç›£æ§** - æ•¸æ“šè³ªé‡å’Œç³»çµ±æ€§èƒ½ç›£æ§
- **ğŸ¤ ç¤¾å€å…±äº«** - å®‰å…¨çš„æ•¸æ“šå…±äº«å’Œæ”¶ç›Šåˆ†é…

### ç³»çµ±åƒ¹å€¼

```mermaid
graph LR
    A[ç”¨æˆ¶ä½¿ç”¨CLI] --> B[æ•¸æ“šè‡ªå‹•æ”¶é›†]
    B --> C[æ™ºèƒ½åˆ†æè™•ç†]
    C --> D[ç”Ÿæˆè¨“ç·´æ•¸æ“š]
    D --> E[ç³»çµ±æ€§èƒ½æå‡]
    E --> F[æ‰€æœ‰ç”¨æˆ¶å—ç›Š]
    F --> A
```

**é›†é«”æ™ºæ…§æ•ˆæ‡‰**ï¼šæ¯å€‹ç”¨æˆ¶çš„ä½¿ç”¨éƒ½è®“æ•´å€‹ç³»çµ±è®Šå¾—æ›´æ™ºèƒ½ï¼

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. ç’°å¢ƒæº–å‚™

```bash
# ç¢ºä¿Pythonç’°å¢ƒ
python --version  # éœ€è¦Python 3.8+

# å®‰è£ä¾è³´
pip install pandas numpy matplotlib seaborn sqlite3
```

### 2. ç³»çµ±åˆå§‹åŒ–

```python
from cli_data_collection_system import get_cli_data_collector

# ç²å–å…¨å±€æ”¶é›†å™¨å¯¦ä¾‹
collector = get_cli_data_collector()

print("âœ… CLIæ•¸æ“šæ”¶é›†ç³»çµ±å·²åˆå§‹åŒ–")
```

### 3. ç¬¬ä¸€æ¬¡ä½¿ç”¨

```python
# é–‹å§‹è¨˜éŒ„ä¸€å€‹CLIäº¤äº’
interaction_id = collector.start_interaction(
    command="python enhanced_mcp_cli.py",
    arguments={"gaia": True, "level": 1, "max-tasks": 5},
    context={"test_mode": True}
)

# æ¨¡æ“¬åŸ·è¡Œ...
import time
time.sleep(2)

# çµæŸè¨˜éŒ„
collector.end_interaction(
    interaction_id=interaction_id,
    result_status=ResultStatus.SUCCESS_PARTIAL,
    output_data={"accuracy": 0.8, "correct_answers": 4, "total_questions": 5},
    execution_time=2.5,
    tools_used=["claude_mcp", "gemini_mcp"],
    accuracy_score=0.8,
    user_satisfaction=4
)

print("âœ… ç¬¬ä¸€å€‹äº¤äº’è¨˜éŒ„å®Œæˆ")
```

### 4. æŸ¥çœ‹çµ±è¨ˆ

```python
# ç²å–æœƒè©±çµ±è¨ˆ
stats = collector.get_session_stats()
print(f"ç¸½äº¤äº’æ•¸: {stats['total_interactions']}")
print(f"å¹³å‡æº–ç¢ºç‡: {stats['average_accuracy']:.2f}")
```

---

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹

### æ•´é«”æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLIæ•¸æ“šæ”¶é›†ç³»çµ±                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ•¸æ“šæ”¶é›†å±¤                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ CLIäº¤äº’ç›£æ§  â”‚  â”‚ å¯¦æ™‚åˆ†é¡å™¨   â”‚  â”‚ è³ªé‡é©—è­‰å™¨   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ•¸æ“šå­˜å„²å±¤                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ SQLiteæ•¸æ“šåº« â”‚  â”‚ æ–‡ä»¶ç³»çµ±å­˜å„² â”‚  â”‚ ç´¢å¼•ç®¡ç†å™¨   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ•¸æ“šè™•ç†å±¤                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ æ•¸æ“šåˆ†æå™¨   â”‚  â”‚ è¨“ç·´é›†æ§‹å»ºå™¨ â”‚  â”‚ è³ªé‡æ§åˆ¶å™¨   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ‡‰ç”¨æœå‹™å±¤                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ ç›£æ§å„€è¡¨æ¿   â”‚  â”‚ å ±å‘Šç”Ÿæˆå™¨   â”‚  â”‚ APIæœå‹™      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒçµ„ä»¶

#### 1. æ•¸æ“šæ”¶é›†å™¨ (CLIDataCollector)
- **åŠŸèƒ½**ï¼šè‡ªå‹•æ”¶é›†CLIäº¤äº’æ•¸æ“š
- **ç‰¹é»**ï¼šç·šç¨‹å®‰å…¨ã€å¯¦æ™‚è™•ç†ã€è‡ªå‹•åˆ†é¡
- **ä½ç½®**ï¼š`cli_data_collection_system.py`

#### 2. æ•¸æ“šåˆ†é¡å™¨ (CLIDataClassifier)
- **åŠŸèƒ½**ï¼šæ™ºèƒ½åˆ†é¡ä»»å‹™é¡å‹å’Œè¤‡é›œåº¦
- **ç®—æ³•**ï¼šé—œéµè©åŒ¹é… + è¦å‰‡å¼•æ“
- **æº–ç¢ºç‡**ï¼š>95%

#### 3. å­˜å„²ç³»çµ± (CLIDataStorage)
- **æ•¸æ“šåº«**ï¼šSQLiteï¼ˆçµæ§‹åŒ–æ•¸æ“šï¼‰
- **æ–‡ä»¶ç³»çµ±**ï¼šJSONæ–‡ä»¶ï¼ˆåŸå§‹æ•¸æ“šï¼‰
- **ç´¢å¼•**ï¼šå¤šç¶­åº¦ç´¢å¼•å„ªåŒ–æŸ¥è©¢

#### 4. åˆ†æå·¥å…· (CLIDataAnalyzer)
- **åŠŸèƒ½**ï¼šå¤šç¶­åº¦æ•¸æ“šåˆ†æå’Œå ±å‘Šç”Ÿæˆ
- **è¼¸å‡º**ï¼šJSONæ ¼å¼åˆ†æå ±å‘Š
- **å¯è¦–åŒ–**ï¼šæ”¯æŒåœ–è¡¨ç”Ÿæˆ

---

## ğŸ“– ä½¿ç”¨æŒ‡å—

### åŸºæœ¬ä½¿ç”¨æµç¨‹

#### 1. åˆå§‹åŒ–ç³»çµ±

```python
from cli_data_collection_system import get_cli_data_collector
from cli_data_collection_system import ResultStatus, TaskType, ComplexityLevel

# ç²å–æ”¶é›†å™¨å¯¦ä¾‹
collector = get_cli_data_collector()
```

#### 2. è¨˜éŒ„CLIäº¤äº’

```python
# é–‹å§‹è¨˜éŒ„
interaction_id = collector.start_interaction(
    command="python your_script.py",
    arguments={
        "param1": "value1",
        "param2": "value2"
    },
    context={
        "user_intent": "data_analysis",
        "expected_output": "csv_file"
    },
    user_id="user123"  # å¯é¸ï¼Œæœƒè¢«åŒ¿ååŒ–
)

# åŸ·è¡Œä½ çš„CLIå‘½ä»¤...
# ...

# è¨˜éŒ„åŸ·è¡Œçµæœ
collector.end_interaction(
    interaction_id=interaction_id,
    result_status=ResultStatus.SUCCESS_PERFECT,  # æˆ–å…¶ä»–ç‹€æ…‹
    output_data={
        "file_generated": "output.csv",
        "rows_processed": 1000,
        "processing_time": 5.2
    },
    execution_time=5.2,
    tools_used=["pandas", "numpy", "matplotlib"],
    mcp_adapters=["data_analysis_mcp"],
    accuracy_score=0.95,  # 0-1ä¹‹é–“
    user_satisfaction=5,  # 1-5è©•åˆ†
    error_info=None  # å¦‚æœæœ‰éŒ¯èª¤å‰‡æä¾›éŒ¯èª¤ä¿¡æ¯
)
```

#### 3. æŸ¥è©¢å’Œåˆ†ææ•¸æ“š

```python
from cli_data_analysis_tools import CLIDataAnalyzer

# å‰µå»ºåˆ†æå™¨
analyzer = CLIDataAnalyzer()

# ç”Ÿæˆç¶œåˆå ±å‘Š
report = analyzer.generate_comprehensive_report()

# æŸ¥çœ‹æ¦‚è¦½
print(f"ç¸½äº¤äº’æ•¸: {report['overview']['total_interactions']}")
print(f"ä»»å‹™é¡å‹åˆ†å¸ƒ: {report['overview']['task_type_distribution']}")

# æŸ¥çœ‹æ€§èƒ½åˆ†æ
if report['performance_analysis']['accuracy_statistics']:
    acc_stats = report['performance_analysis']['accuracy_statistics']
    print(f"å¹³å‡æº–ç¢ºç‡: {acc_stats['mean']:.3f}")
```

### é«˜ç´šä½¿ç”¨

#### 1. è‡ªå®šç¾©æ•¸æ“šåˆ†é¡

```python
from cli_data_collection_system import CLIDataClassifier

class CustomClassifier(CLIDataClassifier):
    def classify_task_type(self, command, arguments):
        # è‡ªå®šç¾©åˆ†é¡é‚è¼¯
        if "machine_learning" in command.lower():
            return TaskType.DATA_ANALYSIS
        # ... å…¶ä»–è‡ªå®šç¾©é‚è¼¯
        return super().classify_task_type(command, arguments)

# ä½¿ç”¨è‡ªå®šç¾©åˆ†é¡å™¨
collector.classifier = CustomClassifier()
```

#### 2. æ‰¹é‡æ•¸æ“šè™•ç†

```python
from cli_data_collection_system import CLIDataStorage

storage = CLIDataStorage()

# æŸ¥è©¢ç‰¹å®šæ¢ä»¶çš„æ•¸æ“š
gaia_interactions = storage.query_interactions(
    task_type=TaskType.GAIA_TESTING,
    start_date=datetime(2025, 6, 1),
    end_date=datetime(2025, 6, 30),
    limit=100
)

print(f"æ‰¾åˆ° {len(gaia_interactions)} å€‹GAIAæ¸¬è©¦è¨˜éŒ„")
```

#### 3. ç”Ÿæˆè¨“ç·´æ•¸æ“šé›†

```python
from cli_data_analysis_tools import CLITrainingDataBuilder

builder = CLITrainingDataBuilder()

# æ§‹å»ºGAIAå„ªåŒ–æ•¸æ“šé›†
gaia_dataset = builder.build_gaia_optimization_dataset()
print(f"GAIAæ•¸æ“šé›†åŒ…å« {gaia_dataset['metadata']['total_samples']} å€‹æ¨£æœ¬")

# æ§‹å»ºå·¥å…·é¸æ“‡æ•¸æ“šé›†
tool_dataset = builder.build_tool_selection_dataset()
print(f"å·¥å…·é¸æ“‡æ•¸æ“šé›†åŒ…å« {tool_dataset['metadata']['total_samples']} å€‹æ¨£æœ¬")
```

---

## âš™ï¸ é…ç½®èªªæ˜

### ç’°å¢ƒè®Šé‡é…ç½®

```bash
# æ•¸æ“šå­˜å„²ç›®éŒ„
export CLI_DATA_DIR="/path/to/cli_training_data"

# æ•¸æ“šåº«é…ç½®
export CLI_DB_PATH="/path/to/cli_interactions.db"

# éš±ç§ä¿è­·ç´šåˆ¥
export PRIVACY_LEVEL="high"  # low, medium, high

# è‡ªå‹•æ¸…ç†é–“éš”ï¼ˆå°æ™‚ï¼‰
export AUTO_CLEANUP_INTERVAL=24

# è³ªé‡æª¢æŸ¥ç´šåˆ¥
export QUALITY_CHECK_LEVEL="strict"  # loose, normal, strict
```

### é…ç½®æ–‡ä»¶ç¤ºä¾‹

å‰µå»º `config/cli_data_config.json`ï¼š

```json
{
  "storage": {
    "data_dir": "/home/ubuntu/Powerauto.ai/cli_training_data",
    "db_path": "cli_interactions.db",
    "backup_enabled": true,
    "backup_interval_hours": 6
  },
  "collection": {
    "auto_start": true,
    "buffer_size": 100,
    "flush_interval_seconds": 30,
    "max_session_duration_hours": 24
  },
  "classification": {
    "auto_classify": true,
    "confidence_threshold": 0.8,
    "manual_review_threshold": 0.6
  },
  "privacy": {
    "anonymization_level": "high",
    "data_retention_days": 365,
    "sensitive_data_detection": true,
    "auto_redaction": true
  },
  "quality": {
    "min_quality_score": 0.7,
    "auto_cleanup": true,
    "outlier_detection": true,
    "duplicate_removal": true
  }
}
```

### åŠ è¼‰é…ç½®

```python
import json
from pathlib import Path

def load_config(config_path="config/cli_data_config.json"):
    """åŠ è¼‰é…ç½®æ–‡ä»¶"""
    
    config_file = Path(config_path)
    if config_file.exists():
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        return get_default_config()

def get_default_config():
    """ç²å–é»˜èªé…ç½®"""
    return {
        "storage": {
            "data_dir": "/home/ubuntu/Powerauto.ai/cli_training_data",
            "db_path": "cli_interactions.db"
        },
        "privacy": {
            "anonymization_level": "high"
        }
    }

# ä½¿ç”¨é…ç½®
config = load_config()
collector = CLIDataCollector(storage_dir=config["storage"]["data_dir"])
```

---

## ğŸ“Š æ•¸æ“šåˆ†æ

### ç”Ÿæˆåˆ†æå ±å‘Š

```python
from cli_data_analysis_tools import CLIDataAnalyzer

analyzer = CLIDataAnalyzer()

# ç”Ÿæˆå®Œæ•´å ±å‘Š
report = analyzer.generate_comprehensive_report()

# ä¿å­˜å ±å‘Š
import json
with open('analysis_report.json', 'w', encoding='utf-8') as f:
    json.dump(report, f, indent=2, ensure_ascii=False, default=str)
```

### å ±å‘Šå…§å®¹è§£è®€

#### 1. æ¦‚è¦½çµ±è¨ˆ (Overview)
```python
overview = report['overview']

print(f"æ•¸æ“šæ”¶é›†æœŸé–“: {overview['time_range']['duration_days']} å¤©")
print(f"ç¸½äº¤äº’æ•¸: {overview['total_interactions']}")
print(f"ä¸»è¦ä»»å‹™é¡å‹: {list(overview['task_type_distribution'].keys())}")
```

#### 2. ä»»å‹™åˆ†æ (Task Analysis)
```python
task_analysis = report['task_analysis']

for task_type, stats in task_analysis.items():
    print(f"\n{task_type}:")
    print(f"  æˆåŠŸç‡: {stats['success_rate']:.2%}")
    print(f"  å¹³å‡åŸ·è¡Œæ™‚é–“: {stats['average_execution_time']:.2f}ç§’")
    print(f"  å¸¸ç”¨å·¥å…·: {list(stats['common_tools'].keys())[:3]}")
```

#### 3. æ€§èƒ½åˆ†æ (Performance Analysis)
```python
performance = report['performance_analysis']

if performance['accuracy_statistics']:
    acc = performance['accuracy_statistics']
    print(f"æº–ç¢ºç‡çµ±è¨ˆ:")
    print(f"  å¹³å‡: {acc['mean']:.3f}")
    print(f"  ç¯„åœ: {acc['min']:.3f} - {acc['max']:.3f}")
    print(f"  æ¨™æº–å·®: {acc['std']:.3f}")
```

#### 4. è¨“ç·´æº–å‚™åº¦ (Training Readiness)
```python
readiness = report['training_readiness']

print(f"è¨“ç·´æº–å‚™åº¦: {readiness['readiness_level']}")
print(f"æº–å‚™åº¦åˆ†æ•¸: {readiness['overall_readiness_score']:.3f}")
print(f"é«˜åƒ¹å€¼æ¨£æœ¬: {readiness['high_value_samples']}")

for recommendation in readiness['recommendations']:
    print(f"å»ºè­°: {recommendation}")
```

### å¯è¦–åŒ–åˆ†æ

```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_task_distribution(report):
    """å¯è¦–åŒ–ä»»å‹™é¡å‹åˆ†å¸ƒ"""
    
    task_dist = report['overview']['task_type_distribution']
    
    plt.figure(figsize=(10, 6))
    plt.pie(task_dist.values(), labels=task_dist.keys(), autopct='%1.1f%%')
    plt.title('ä»»å‹™é¡å‹åˆ†å¸ƒ')
    plt.savefig('task_distribution.png')
    plt.show()

def visualize_accuracy_trend(interactions):
    """å¯è¦–åŒ–æº–ç¢ºç‡è¶¨å‹¢"""
    
    # æå–æº–ç¢ºç‡æ•¸æ“š
    accuracy_data = [(i.timestamp, i.accuracy_score) 
                    for i in interactions 
                    if i.accuracy_score is not None]
    
    if accuracy_data:
        timestamps, accuracies = zip(*accuracy_data)
        
        plt.figure(figsize=(12, 6))
        plt.plot(timestamps, accuracies, marker='o')
        plt.title('æº–ç¢ºç‡è¶¨å‹¢')
        plt.xlabel('æ™‚é–“')
        plt.ylabel('æº–ç¢ºç‡')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig('accuracy_trend.png')
        plt.show()

# ä½¿ç”¨å¯è¦–åŒ–
visualize_task_distribution(report)
```

---

## ğŸ›¡ï¸ è³ªé‡æ§åˆ¶

### å¯¦æ™‚è³ªé‡æª¢æŸ¥

ç³»çµ±æœƒè‡ªå‹•é€²è¡Œä»¥ä¸‹è³ªé‡æª¢æŸ¥ï¼š

#### 1. å¿…éœ€å­—æ®µæª¢æŸ¥
```python
required_fields = [
    'interaction_id', 'session_id', 'timestamp', 'command',
    'task_type', 'complexity_level', 'result_status'
]
```

#### 2. æ•¸æ“šé¡å‹é©—è­‰
```python
type_checks = {
    'execution_time': float,
    'accuracy_score': (float, type(None)),
    'user_satisfaction': (int, type(None)),
    'tools_used': list,
    'arguments': dict
}
```

#### 3. å€¼ç¯„åœæª¢æŸ¥
```python
range_checks = {
    'accuracy_score': (0.0, 1.0),
    'user_satisfaction': (1, 5),
    'execution_time': (0.0, 3600.0)  # æœ€å¤§1å°æ™‚
}
```

### è³ªé‡åˆ†æ•¸è¨ˆç®—

```python
def calculate_quality_score(interaction):
    """è¨ˆç®—è³ªé‡åˆ†æ•¸"""
    
    scores = {
        'completeness': 0.0,    # å®Œæ•´æ€§ (30%)
        'accuracy': 0.0,        # æº–ç¢ºæ€§ (25%)
        'consistency': 0.0,     # ä¸€è‡´æ€§ (20%)
        'timeliness': 0.0,      # æ™‚æ•ˆæ€§ (15%)
        'uniqueness': 0.0       # å”¯ä¸€æ€§ (10%)
    }
    
    # å®Œæ•´æ€§æª¢æŸ¥
    required_fields = ['command', 'result_status', 'execution_time']
    present_fields = sum(1 for field in required_fields 
                        if getattr(interaction, field) is not None)
    scores['completeness'] = present_fields / len(required_fields)
    
    # æº–ç¢ºæ€§æª¢æŸ¥
    if interaction.accuracy_score is not None:
        scores['accuracy'] = interaction.accuracy_score
    
    # ä¸€è‡´æ€§æª¢æŸ¥
    if (interaction.result_status.value.startswith('success') and 
        interaction.accuracy_score and interaction.accuracy_score > 0.5):
        scores['consistency'] = 1.0
    
    # æ™‚æ•ˆæ€§æª¢æŸ¥
    age_hours = (datetime.now() - interaction.timestamp).total_seconds() / 3600
    scores['timeliness'] = max(0, 1 - age_hours / 24)  # 24å°æ™‚å…§ç‚ºæ»¿åˆ†
    
    # å”¯ä¸€æ€§æª¢æŸ¥ï¼ˆç°¡åŒ–ç‰ˆï¼‰
    scores['uniqueness'] = 0.8  # å‡è¨­å¤§éƒ¨åˆ†æ•¸æ“šæ˜¯å”¯ä¸€çš„
    
    # åŠ æ¬Šè¨ˆç®—ç¸½åˆ†
    weights = [0.30, 0.25, 0.20, 0.15, 0.10]
    total_score = sum(score * weight for score, weight in zip(scores.values(), weights))
    
    return total_score
```

### è³ªé‡ç­‰ç´šåˆ†é¡

| åˆ†æ•¸ç¯„åœ | ç­‰ç´š | æè¿° | ä½¿ç”¨å»ºè­° |
|---------|------|------|----------|
| 0.9-1.0 | Aç´š | å„ªç§€ | å„ªå…ˆç”¨æ–¼è¨“ç·´ |
| 0.8-0.9 | Bç´š | è‰¯å¥½ | é©åˆè¨“ç·´ä½¿ç”¨ |
| 0.7-0.8 | Cç´š | ä¸€èˆ¬ | éœ€è¦æ¸…ç†å¾Œä½¿ç”¨ |
| 0.6-0.7 | Dç´š | è¼ƒå·® | åƒ…ç”¨æ–¼çµ±è¨ˆåˆ†æ |
| 0.0-0.6 | Fç´š | å·® | ä¸å»ºè­°ä½¿ç”¨ |

---

## ğŸ”’ éš±ç§ä¿è­·

### è‡ªå‹•åŒ¿ååŒ–

ç³»çµ±æœƒè‡ªå‹•é€²è¡Œä»¥ä¸‹åŒ¿ååŒ–è™•ç†ï¼š

#### 1. ç”¨æˆ¶æ¨™è­˜åŒ¿ååŒ–
```python
import hashlib

def anonymize_user_id(user_id):
    """åŒ¿ååŒ–ç”¨æˆ¶ID"""
    if user_id:
        return hashlib.sha256(user_id.encode()).hexdigest()[:16]
    return None
```

#### 2. æ•æ„Ÿä¿¡æ¯æª¢æ¸¬å’Œæ¸…ç†
```python
import re

SENSITIVE_PATTERNS = {
    'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
    'phone': r'\b\d{3}-\d{3}-\d{4}\b',
    'credit_card': r'\b\d{4}-\d{4}-\d{4}-\d{4}\b',
    'ip_address': r'\b(?:\d{1,3}\.){3}\d{1,3}\b'
}

def detect_sensitive_data(text):
    """æª¢æ¸¬æ•æ„Ÿæ•¸æ“š"""
    detected = []
    for pattern_name, pattern in SENSITIVE_PATTERNS.items():
        if re.search(pattern, text):
            detected.append(pattern_name)
    return detected

def sanitize_text(text):
    """æ¸…ç†æ•æ„Ÿä¿¡æ¯"""
    for pattern_name, pattern in SENSITIVE_PATTERNS.items():
        text = re.sub(pattern, f'[{pattern_name.upper()}_REDACTED]', text)
    return text
```

#### 3. ç’°å¢ƒä¿¡æ¯æ¸…ç†
```python
def clean_environment_info(env_info):
    """æ¸…ç†ç’°å¢ƒä¿¡æ¯"""
    safe_info = {
        'python_version': env_info.get('python_version'),
        'platform': env_info.get('platform'),
        'timestamp': env_info.get('timestamp')
    }
    # ç§»é™¤å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯çš„å­—æ®µ
    return safe_info
```

### éš±ç§åˆè¦æª¢æŸ¥

```python
def privacy_compliance_check(interaction):
    """éš±ç§åˆè¦æª¢æŸ¥"""
    
    issues = []
    
    # æª¢æŸ¥å‘½ä»¤ä¸­çš„æ•æ„Ÿä¿¡æ¯
    command_text = f"{interaction.command} {json.dumps(interaction.arguments)}"
    sensitive_data = detect_sensitive_data(command_text)
    
    if sensitive_data:
        issues.append(f"å‘½ä»¤ä¸­åŒ…å«æ•æ„Ÿä¿¡æ¯: {sensitive_data}")
    
    # æª¢æŸ¥è¼¸å‡ºæ•¸æ“šä¸­çš„æ•æ„Ÿä¿¡æ¯
    output_text = json.dumps(interaction.output_data)
    sensitive_output = detect_sensitive_data(output_text)
    
    if sensitive_output:
        issues.append(f"è¼¸å‡ºä¸­åŒ…å«æ•æ„Ÿä¿¡æ¯: {sensitive_output}")
    
    # æª¢æŸ¥ç”¨æˆ¶æ¨™è­˜æ˜¯å¦å·²åŒ¿ååŒ–
    if interaction.user_hash and len(interaction.user_hash) > 16:
        issues.append("ç”¨æˆ¶æ¨™è­˜æœªæ­£ç¢ºåŒ¿ååŒ–")
    
    return len(issues) == 0, issues
```

---

## ğŸ’¡ æœ€ä½³å¯¦è¸

### 1. æ•¸æ“šæ”¶é›†æœ€ä½³å¯¦è¸

#### âœ… æ¨è–¦åšæ³•

```python
# 1. æä¾›è©³ç´°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
interaction_id = collector.start_interaction(
    command="python data_analysis.py",
    arguments={"input_file": "data.csv", "output_format": "json"},
    context={
        "user_intent": "æ•¸æ“šåˆ†æ",
        "expected_output": "çµ±è¨ˆå ±å‘Š",
        "data_size": "medium",
        "complexity": "moderate"
    }
)

# 2. è¨˜éŒ„æº–ç¢ºçš„åŸ·è¡Œçµæœ
collector.end_interaction(
    interaction_id=interaction_id,
    result_status=ResultStatus.SUCCESS_PARTIAL,  # æº–ç¢ºåæ˜ çµæœ
    output_data={"rows_processed": 1000, "accuracy": 0.95},
    execution_time=actual_execution_time,  # å¯¦éš›åŸ·è¡Œæ™‚é–“
    tools_used=["pandas", "numpy"],  # å¯¦éš›ä½¿ç”¨çš„å·¥å…·
    accuracy_score=0.95,  # å®¢è§€çš„æº–ç¢ºç‡
    user_satisfaction=4  # çœŸå¯¦çš„ç”¨æˆ¶æ»¿æ„åº¦
)

# 3. è™•ç†éŒ¯èª¤æƒ…æ³
if error_occurred:
    collector.end_interaction(
        interaction_id=interaction_id,
        result_status=ResultStatus.FAILURE_SYSTEM,
        output_data={},
        execution_time=execution_time,
        error_info={
            "error_type": "FileNotFoundError",
            "error_message": "Input file not found",
            "stack_trace": traceback.format_exc()
        }
    )
```

#### âŒ é¿å…çš„åšæ³•

```python
# ä¸è¦æä¾›è™›å‡æˆ–èª¤å°æ€§ä¿¡æ¯
collector.end_interaction(
    interaction_id=interaction_id,
    result_status=ResultStatus.SUCCESS_PERFECT,  # âŒ å¯¦éš›åªæ˜¯éƒ¨åˆ†æˆåŠŸ
    accuracy_score=1.0,  # âŒ èª‡å¤§æº–ç¢ºç‡
    user_satisfaction=5  # âŒ ä¸çœŸå¯¦çš„æ»¿æ„åº¦
)

# ä¸è¦å¿½ç•¥éŒ¯èª¤ä¿¡æ¯
collector.end_interaction(
    interaction_id=interaction_id,
    result_status=ResultStatus.SUCCESS_ACCEPTABLE,  # âŒ å¯¦éš›ç™¼ç”Ÿäº†éŒ¯èª¤
    error_info=None  # âŒ æ²’æœ‰è¨˜éŒ„éŒ¯èª¤ä¿¡æ¯
)
```

### 2. æ•¸æ“šè³ªé‡æœ€ä½³å¯¦è¸

#### æé«˜æ•¸æ“šè³ªé‡çš„æ–¹æ³•

```python
# 1. ä½¿ç”¨çµæ§‹åŒ–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
context = {
    "task_category": "data_processing",
    "input_type": "csv",
    "output_type": "json",
    "data_volume": "large",
    "performance_requirement": "high_speed"
}

# 2. æä¾›è©³ç´°çš„å·¥å…·ä½¿ç”¨ä¿¡æ¯
tools_used = [
    "pandas==1.5.0",  # åŒ…å«ç‰ˆæœ¬ä¿¡æ¯
    "numpy==1.21.0",
    "custom_data_processor"
]

# 3. è¨˜éŒ„è©³ç´°çš„æ€§èƒ½æŒ‡æ¨™
output_data = {
    "processing_time": 5.2,
    "memory_usage_mb": 256,
    "cpu_usage_percent": 75,
    "rows_processed": 10000,
    "accuracy_score": 0.95,
    "quality_metrics": {
        "completeness": 0.98,
        "consistency": 0.96,
        "validity": 0.94
    }
}
```

### 3. éš±ç§ä¿è­·æœ€ä½³å¯¦è¸

```python
# 1. åœ¨è¨˜éŒ„å‰æ¸…ç†æ•æ„Ÿä¿¡æ¯
def safe_record_interaction(command, arguments, context):
    """å®‰å…¨è¨˜éŒ„äº¤äº’"""
    
    # æ¸…ç†å‘½ä»¤ä¸­çš„æ•æ„Ÿä¿¡æ¯
    safe_command = sanitize_text(command)
    
    # æ¸…ç†åƒæ•¸ä¸­çš„æ•æ„Ÿä¿¡æ¯
    safe_arguments = {}
    for key, value in arguments.items():
        if isinstance(value, str):
            safe_arguments[key] = sanitize_text(value)
        else:
            safe_arguments[key] = value
    
    # æ¸…ç†ä¸Šä¸‹æ–‡ä¿¡æ¯
    safe_context = {k: v for k, v in context.items() 
                   if k not in ['user_email', 'api_key', 'password']}
    
    return collector.start_interaction(safe_command, safe_arguments, safe_context)

# 2. å®šæœŸæª¢æŸ¥éš±ç§åˆè¦æ€§
def audit_privacy_compliance():
    """å¯©è¨ˆéš±ç§åˆè¦æ€§"""
    
    recent_interactions = storage.query_interactions(
        start_date=datetime.now() - timedelta(days=7)
    )
    
    violations = []
    for interaction in recent_interactions:
        is_compliant, issues = privacy_compliance_check(interaction)
        if not is_compliant:
            violations.append({
                "interaction_id": interaction.interaction_id,
                "issues": issues
            })
    
    if violations:
        logger.warning(f"ç™¼ç¾ {len(violations)} å€‹éš±ç§åˆè¦å•é¡Œ")
        # æ¡å–è£œæ•‘æªæ–½
        remediate_privacy_violations(violations)
```

### 4. æ€§èƒ½å„ªåŒ–æœ€ä½³å¯¦è¸

```python
# 1. æ‰¹é‡è™•ç†æ•¸æ“š
def batch_process_interactions(interactions, batch_size=100):
    """æ‰¹é‡è™•ç†äº¤äº’æ•¸æ“š"""
    
    for i in range(0, len(interactions), batch_size):
        batch = interactions[i:i + batch_size]
        
        # æ‰¹é‡é©—è­‰
        valid_interactions = [i for i in batch if validator.validate_interaction(i)]
        
        # æ‰¹é‡å­˜å„²
        storage.store_batch(valid_interactions)
        
        logger.info(f"è™•ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(valid_interactions)} å€‹æœ‰æ•ˆäº¤äº’")

# 2. ç•°æ­¥æ•¸æ“šè™•ç†
import asyncio

async def async_data_processing():
    """ç•°æ­¥æ•¸æ“šè™•ç†"""
    
    tasks = [
        asyncio.create_task(process_data_quality()),
        asyncio.create_task(generate_training_datasets()),
        asyncio.create_task(update_analytics_reports())
    ]
    
    await asyncio.gather(*tasks)

# 3. ç·©å­˜å„ªåŒ–
from functools import lru_cache

@lru_cache(maxsize=128)
def get_task_statistics(task_type, date_range):
    """ç·©å­˜ä»»å‹™çµ±è¨ˆä¿¡æ¯"""
    return analyzer.calculate_task_statistics(task_type, date_range)
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡ŒåŠè§£æ±ºæ–¹æ¡ˆ

#### 1. æ•¸æ“šæ”¶é›†å•é¡Œ

**å•é¡Œï¼šæ•¸æ“šæ”¶é›†å™¨åˆå§‹åŒ–å¤±æ•—**
```
éŒ¯èª¤ï¼šFileNotFoundError: No such file or directory: 'cli_training_data'
```

**è§£æ±ºæ–¹æ¡ˆï¼š**
```python
import os
from pathlib import Path

# ç¢ºä¿æ•¸æ“šç›®éŒ„å­˜åœ¨
data_dir = Path("/home/ubuntu/Powerauto.ai/cli_training_data")
data_dir.mkdir(parents=True, exist_ok=True)

# é‡æ–°åˆå§‹åŒ–æ”¶é›†å™¨
collector = CLIDataCollector(storage_dir=str(data_dir))
```

**å•é¡Œï¼šäº¤äº’è¨˜éŒ„å¤±æ•—**
```
éŒ¯èª¤ï¼šValidationError: Required field 'command' is missing
```

**è§£æ±ºæ–¹æ¡ˆï¼š**
```python
# ç¢ºä¿æä¾›æ‰€æœ‰å¿…éœ€å­—æ®µ
interaction_id = collector.start_interaction(
    command="your_command_here",  # å¿…éœ€
    arguments={},  # å¯ä»¥ç‚ºç©ºå­—å…¸ï¼Œä½†ä¸èƒ½ç‚ºNone
    context={}     # å¯ä»¥ç‚ºç©ºå­—å…¸ï¼Œä½†ä¸èƒ½ç‚ºNone
)
```

#### 2. æ•¸æ“šåº«å•é¡Œ

**å•é¡Œï¼šæ•¸æ“šåº«é–å®š**
```
éŒ¯èª¤ï¼šsqlite3.OperationalError: database is locked
```

**è§£æ±ºæ–¹æ¡ˆï¼š**
```python
import sqlite3
import time

def safe_database_operation(operation, max_retries=3):
    """å®‰å…¨çš„æ•¸æ“šåº«æ“ä½œ"""
    
    for attempt in range(max_retries):
        try:
            return operation()
        except sqlite3.OperationalError as e:
            if "database is locked" in str(e) and attempt < max_retries - 1:
                time.sleep(0.1 * (2 ** attempt))  # æŒ‡æ•¸é€€é¿
                continue
            raise
```

#### 3. å…§å­˜å•é¡Œ

**å•é¡Œï¼šå…§å­˜ä½¿ç”¨éé«˜**
```
éŒ¯èª¤ï¼šMemoryError: Unable to allocate array
```

**è§£æ±ºæ–¹æ¡ˆï¼š**
```python
# 1. æ¸›å°‘æ‰¹è™•ç†å¤§å°
analyzer = CLIDataAnalyzer()
report = analyzer.generate_comprehensive_report(batch_size=50)  # é»˜èª100

# 2. ä½¿ç”¨ç”Ÿæˆå™¨è™•ç†å¤§æ•¸æ“šé›†
def process_interactions_generator(storage):
    """ä½¿ç”¨ç”Ÿæˆå™¨è™•ç†å¤§æ•¸æ“šé›†"""
    
    offset = 0
    batch_size = 100
    
    while True:
        batch = storage.query_interactions(limit=batch_size, offset=offset)
        if not batch:
            break
        
        for interaction in batch:
            yield interaction
        
        offset += batch_size

# 3. å®šæœŸæ¸…ç†å…§å­˜
import gc

def cleanup_memory():
    """æ¸…ç†å…§å­˜"""
    gc.collect()
```

#### 4. æ€§èƒ½å•é¡Œ

**å•é¡Œï¼šæŸ¥è©¢é€Ÿåº¦æ…¢**

**è§£æ±ºæ–¹æ¡ˆï¼š**
```python
# 1. æ·»åŠ æ•¸æ“šåº«ç´¢å¼•
def add_performance_indexes(db_path):
    """æ·»åŠ æ€§èƒ½ç´¢å¼•"""
    
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()
        
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_timestamp_task ON cli_interactions(timestamp, task_type)",
            "CREATE INDEX IF NOT EXISTS idx_learning_value_status ON cli_interactions(learning_value, result_status)",
            "CREATE INDEX IF NOT EXISTS idx_session_timestamp ON cli_interactions(session_id, timestamp)"
        ]
        
        for index_sql in indexes:
            cursor.execute(index_sql)
        
        conn.commit()

# 2. ä½¿ç”¨æŸ¥è©¢å„ªåŒ–
def optimized_query(storage, task_type, start_date, end_date):
    """å„ªåŒ–çš„æŸ¥è©¢"""
    
    # ä½¿ç”¨ç´¢å¼•å‹å¥½çš„æŸ¥è©¢
    query = """
    SELECT * FROM cli_interactions 
    WHERE task_type = ? 
    AND timestamp BETWEEN ? AND ?
    ORDER BY timestamp DESC
    LIMIT 1000
    """
    
    with sqlite3.connect(storage.db_path) as conn:
        cursor = conn.cursor()
        cursor.execute(query, (task_type.value, start_date.isoformat(), end_date.isoformat()))
        return cursor.fetchall()
```

### æ—¥èªŒå’Œèª¿è©¦

#### å•Ÿç”¨è©³ç´°æ—¥èªŒ

```python
import logging

# è¨­ç½®è©³ç´°æ—¥èªŒ
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('cli_data_system.log'),
        logging.StreamHandler()
    ]
)

# ç²å–ç‰¹å®šæ¨¡å¡Šçš„æ—¥èªŒå™¨
logger = logging.getLogger('cli_data_collection_system')
logger.setLevel(logging.DEBUG)
```

#### èª¿è©¦æ¨¡å¼

```python
# å•Ÿç”¨èª¿è©¦æ¨¡å¼
os.environ['CLI_DEBUG_MODE'] = '1'

# åœ¨èª¿è©¦æ¨¡å¼ä¸‹ï¼Œç³»çµ±æœƒï¼š
# 1. è¼¸å‡ºè©³ç´°çš„åŸ·è¡Œä¿¡æ¯
# 2. ä¿å­˜ä¸­é–“è™•ç†çµæœ
# 3. è·³éæŸäº›æ€§èƒ½å„ªåŒ–ä»¥ä¾¿èª¿è©¦
```

---

## ğŸ“š APIåƒè€ƒ

### CLIDataCollector

#### åˆå§‹åŒ–
```python
CLIDataCollector(storage_dir: str = "/home/ubuntu/Powerauto.ai/cli_training_data")
```

#### ä¸»è¦æ–¹æ³•

##### start_interaction()
```python
def start_interaction(
    command: str,
    arguments: Dict[str, Any],
    context: Dict[str, Any] = None,
    user_id: Optional[str] = None
) -> str:
    """
    é–‹å§‹è¨˜éŒ„CLIäº¤äº’
    
    åƒæ•¸:
        command: CLIå‘½ä»¤å­—ç¬¦ä¸²
        arguments: å‘½ä»¤åƒæ•¸å­—å…¸
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯å­—å…¸
        user_id: ç”¨æˆ¶IDï¼ˆå¯é¸ï¼Œæœƒè¢«åŒ¿ååŒ–ï¼‰
    
    è¿”å›:
        interaction_id: äº¤äº’å”¯ä¸€æ¨™è­˜ç¬¦
    """
```

##### end_interaction()
```python
def end_interaction(
    interaction_id: str,
    result_status: ResultStatus,
    output_data: Dict[str, Any],
    execution_time: float,
    tools_used: List[str] = None,
    mcp_adapters: List[str] = None,
    accuracy_score: Optional[float] = None,
    user_satisfaction: Optional[int] = None,
    error_info: Optional[Dict[str, Any]] = None
):
    """
    çµæŸè¨˜éŒ„CLIäº¤äº’
    
    åƒæ•¸:
        interaction_id: äº¤äº’æ¨™è­˜ç¬¦
        result_status: åŸ·è¡Œçµæœç‹€æ…‹
        output_data: è¼¸å‡ºæ•¸æ“šå­—å…¸
        execution_time: åŸ·è¡Œæ™‚é–“ï¼ˆç§’ï¼‰
        tools_used: ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨
        mcp_adapters: ä½¿ç”¨çš„MCPé©é…å™¨åˆ—è¡¨
        accuracy_score: æº–ç¢ºç‡åˆ†æ•¸ (0-1)
        user_satisfaction: ç”¨æˆ¶æ»¿æ„åº¦ (1-5)
        error_info: éŒ¯èª¤ä¿¡æ¯å­—å…¸
    """
```

##### get_session_stats()
```python
def get_session_stats() -> Dict[str, Any]:
    """
    ç²å–ç•¶å‰æœƒè©±çµ±è¨ˆä¿¡æ¯
    
    è¿”å›:
        çµ±è¨ˆä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«ï¼š
        - session_id: æœƒè©±ID
        - total_interactions: ç¸½äº¤äº’æ•¸
        - task_type_distribution: ä»»å‹™é¡å‹åˆ†å¸ƒ
        - average_accuracy: å¹³å‡æº–ç¢ºç‡
        - high_value_interactions: é«˜åƒ¹å€¼äº¤äº’æ•¸
    """
```

### CLIDataAnalyzer

#### åˆå§‹åŒ–
```python
CLIDataAnalyzer(storage_dir: str = "/home/ubuntu/Powerauto.ai/cli_training_data")
```

#### ä¸»è¦æ–¹æ³•

##### generate_comprehensive_report()
```python
def generate_comprehensive_report() -> Dict[str, Any]:
    """
    ç”Ÿæˆç¶œåˆåˆ†æå ±å‘Š
    
    è¿”å›:
        åŒ…å«ä»¥ä¸‹éƒ¨åˆ†çš„å ±å‘Šå­—å…¸ï¼š
        - overview: æ¦‚è¦½çµ±è¨ˆ
        - task_analysis: ä»»å‹™åˆ†æ
        - performance_analysis: æ€§èƒ½åˆ†æ
        - learning_value_analysis: å­¸ç¿’åƒ¹å€¼åˆ†æ
        - tool_usage_analysis: å·¥å…·ä½¿ç”¨åˆ†æ
        - temporal_analysis: æ™‚é–“æ¨¡å¼åˆ†æ
        - quality_metrics: è³ªé‡æŒ‡æ¨™
        - training_readiness: è¨“ç·´æº–å‚™åº¦
    """
```

### CLITrainingDataBuilder

#### ä¸»è¦æ–¹æ³•

##### build_gaia_optimization_dataset()
```python
def build_gaia_optimization_dataset() -> Dict[str, Any]:
    """
    æ§‹å»ºGAIAå„ªåŒ–è¨“ç·´æ•¸æ“šé›†
    
    è¿”å›:
        åŒ…å«è¨“ç·´æ¨£æœ¬å’Œå…ƒæ•¸æ“šçš„æ•¸æ“šé›†å­—å…¸
    """
```

##### build_tool_selection_dataset()
```python
def build_tool_selection_dataset() -> Dict[str, Any]:
    """
    æ§‹å»ºå·¥å…·é¸æ“‡è¨“ç·´æ•¸æ“šé›†
    
    è¿”å›:
        åŒ…å«å·¥å…·é¸æ“‡æ¨¡å¼çš„æ•¸æ“šé›†å­—å…¸
    """
```

### æšèˆ‰é¡å‹

#### TaskType
```python
class TaskType(Enum):
    GAIA_TESTING = "gaia_testing"
    MCP_MANAGEMENT = "mcp_management"
    DATA_ANALYSIS = "data_analysis"
    CODE_GENERATION = "code_generation"
    SYSTEM_OPERATION = "system_operation"
```

#### ResultStatus
```python
class ResultStatus(Enum):
    SUCCESS_PERFECT = "success_perfect"
    SUCCESS_PARTIAL = "success_partial"
    SUCCESS_ACCEPTABLE = "success_acceptable"
    FAILURE_USER = "failure_user"
    FAILURE_SYSTEM = "failure_system"
    FAILURE_CONFIG = "failure_config"
    FAILURE_RESOURCE = "failure_resource"
```

#### LearningValue
```python
class LearningValue(Enum):
    HIGH = "high_value"
    MEDIUM = "medium_value"
    LOW = "low_value"
    NEGATIVE = "negative_value"
```

---

## ğŸ“ æ”¯æŒå’Œç¤¾å€

### ç²å–å¹«åŠ©

- **æ–‡æª”**: æŸ¥çœ‹å®Œæ•´æ–‡æª”å’Œç¤ºä¾‹
- **GitHub Issues**: å ±å‘Šå•é¡Œå’ŒåŠŸèƒ½è«‹æ±‚
- **ç¤¾å€è«–å£‡**: èˆ‡å…¶ä»–ç”¨æˆ¶äº¤æµç¶“é©—
- **é–‹ç™¼è€…éƒµä»¶åˆ—è¡¨**: ç²å–æŠ€è¡“æ”¯æŒ

### è²¢ç»æŒ‡å—

æ­¡è¿ç‚ºPowerAutomation CLIæ•¸æ“šæ”¶é›†ç³»çµ±åšå‡ºè²¢ç»ï¼š

1. **å ±å‘Šå•é¡Œ** - ç™¼ç¾bugæˆ–æœ‰æ”¹é€²å»ºè­°
2. **æäº¤ä»£ç¢¼** - ä¿®å¾©å•é¡Œæˆ–æ·»åŠ æ–°åŠŸèƒ½
3. **æ”¹é€²æ–‡æª”** - å¹«åŠ©å®Œå–„æ–‡æª”å’Œæ•™ç¨‹
4. **åˆ†äº«ç¶“é©—** - åœ¨ç¤¾å€åˆ†äº«ä½¿ç”¨ç¶“é©—

### ç‰ˆæœ¬æ›´æ–°

å®šæœŸæª¢æŸ¥ç³»çµ±æ›´æ–°ï¼š

```bash
# æª¢æŸ¥ç•¶å‰ç‰ˆæœ¬
python -c "from cli_data_collection_system import __version__; print(__version__)"

# æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬
git pull origin main
pip install -r requirements.txt
```

---

## ğŸ“„ è¨±å¯è­‰

æœ¬ç³»çµ±æ¡ç”¨MITè¨±å¯è­‰ï¼Œè©³è¦‹LICENSEæ–‡ä»¶ã€‚

---

*æœ€å¾Œæ›´æ–°: 2025å¹´6æœˆ8æ—¥*  
*ç‰ˆæœ¬: 1.0.0*  
*ç¶­è­·è€…: PowerAutomationé–‹ç™¼åœ˜éšŠ*

