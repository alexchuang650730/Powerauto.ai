#!/usr/bin/env python3
"""
CLIæ•¸æ“šè™•ç†å’Œåˆ†æå·¥å…·
ç”¨æ–¼è™•ç†ã€åˆ†æå’Œç”Ÿæˆè¨“ç·´æ•¸æ“šé›†
"""

import json
import sqlite3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from collections import defaultdict, Counter
import logging

# å°å…¥æˆ‘å€‘çš„æ•¸æ“šçµæ§‹
from cli_data_collection_system import (
    CLIInteractionData, TaskType, ComplexityLevel, 
    ResultStatus, LearningValue, CLIDataStorage
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CLIDataAnalyzer:
    """CLIæ•¸æ“šåˆ†æå™¨"""
    
    def __init__(self, storage_dir: str = "/home/ubuntu/Powerauto.ai/cli_training_data"):
        self.storage = CLIDataStorage(storage_dir)
        self.storage_dir = Path(storage_dir)
        
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç¶œåˆåˆ†æå ±å‘Š"""
        
        # ç²å–æ‰€æœ‰æ•¸æ“š
        all_interactions = self.storage.query_interactions()
        
        if not all_interactions:
            return {"error": "æ²’æœ‰æ‰¾åˆ°ä»»ä½•äº¤äº’æ•¸æ“š"}
        
        report = {
            "overview": self._analyze_overview(all_interactions),
            "task_analysis": self._analyze_task_patterns(all_interactions),
            "performance_analysis": self._analyze_performance(all_interactions),
            "learning_value_analysis": self._analyze_learning_value(all_interactions),
            "tool_usage_analysis": self._analyze_tool_usage(all_interactions),
            "temporal_analysis": self._analyze_temporal_patterns(all_interactions),
            "quality_metrics": self._calculate_quality_metrics(all_interactions),
            "training_readiness": self._assess_training_readiness(all_interactions)
        }
        
        return report
    
    def _analyze_overview(self, interactions: List[CLIInteractionData]) -> Dict[str, Any]:
        """åˆ†ææ¦‚è¦½çµ±è¨ˆ"""
        
        total_interactions = len(interactions)
        
        # æ™‚é–“ç¯„åœ
        timestamps = [i.timestamp for i in interactions]
        time_range = {
            "start": min(timestamps).isoformat(),
            "end": max(timestamps).isoformat(),
            "duration_days": (max(timestamps) - min(timestamps)).days
        }
        
        # åŸºæœ¬çµ±è¨ˆ
        task_types = Counter(i.task_type.value for i in interactions)
        complexity_levels = Counter(i.complexity_level.value for i in interactions)
        result_statuses = Counter(i.result_status.value for i in interactions)
        learning_values = Counter(i.learning_value.value for i in interactions)
        
        return {
            "total_interactions": total_interactions,
            "time_range": time_range,
            "task_type_distribution": dict(task_types),
            "complexity_distribution": dict(complexity_levels),
            "result_status_distribution": dict(result_statuses),
            "learning_value_distribution": dict(learning_values)
        }
    
    def _analyze_task_patterns(self, interactions: List[CLIInteractionData]) -> Dict[str, Any]:
        """åˆ†æä»»å‹™æ¨¡å¼"""
        
        patterns = {}
        
        # æŒ‰ä»»å‹™é¡å‹åˆ†çµ„åˆ†æ
        task_groups = defaultdict(list)
        for interaction in interactions:
            task_groups[interaction.task_type.value].append(interaction)
        
        for task_type, task_interactions in task_groups.items():
            # æˆåŠŸç‡åˆ†æ
            success_count = sum(1 for i in task_interactions 
                              if i.result_status in [ResultStatus.SUCCESS_PERFECT, 
                                                   ResultStatus.SUCCESS_PARTIAL,
                                                   ResultStatus.SUCCESS_ACCEPTABLE])
            success_rate = success_count / len(task_interactions)
            
            # å¹³å‡åŸ·è¡Œæ™‚é–“
            execution_times = [i.execution_time for i in task_interactions if i.execution_time > 0]
            avg_execution_time = np.mean(execution_times) if execution_times else 0
            
            # å¹³å‡æº–ç¢ºç‡
            accuracy_scores = [i.accuracy_score for i in task_interactions if i.accuracy_score is not None]
            avg_accuracy = np.mean(accuracy_scores) if accuracy_scores else None
            
            # è¤‡é›œåº¦åˆ†å¸ƒ
            complexity_dist = Counter(i.complexity_level.value for i in task_interactions)
            
            # å¸¸ç”¨å·¥å…·
            all_tools = []
            for i in task_interactions:
                all_tools.extend(i.tools_used)
            common_tools = Counter(all_tools).most_common(5)
            
            patterns[task_type] = {
                "total_interactions": len(task_interactions),
                "success_rate": success_rate,
                "average_execution_time": avg_execution_time,
                "average_accuracy": avg_accuracy,
                "complexity_distribution": dict(complexity_dist),
                "common_tools": dict(common_tools)
            }
        
        return patterns
    
    def _analyze_performance(self, interactions: List[CLIInteractionData]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æŒ‡æ¨™"""
        
        # æº–ç¢ºç‡åˆ†æ
        accuracy_scores = [i.accuracy_score for i in interactions if i.accuracy_score is not None]
        
        accuracy_stats = {}
        if accuracy_scores:
            accuracy_stats = {
                "mean": np.mean(accuracy_scores),
                "median": np.median(accuracy_scores),
                "std": np.std(accuracy_scores),
                "min": np.min(accuracy_scores),
                "max": np.max(accuracy_scores),
                "count": len(accuracy_scores)
            }
        
        # åŸ·è¡Œæ™‚é–“åˆ†æ
        execution_times = [i.execution_time for i in interactions if i.execution_time > 0]
        
        execution_stats = {}
        if execution_times:
            execution_stats = {
                "mean": np.mean(execution_times),
                "median": np.median(execution_times),
                "std": np.std(execution_times),
                "min": np.min(execution_times),
                "max": np.max(execution_times),
                "count": len(execution_times)
            }
        
        # ç”¨æˆ¶æ»¿æ„åº¦åˆ†æ
        satisfaction_scores = [i.user_satisfaction for i in interactions if i.user_satisfaction is not None]
        
        satisfaction_stats = {}
        if satisfaction_scores:
            satisfaction_stats = {
                "mean": np.mean(satisfaction_scores),
                "median": np.median(satisfaction_scores),
                "distribution": dict(Counter(satisfaction_scores)),
                "count": len(satisfaction_scores)
            }
        
        return {
            "accuracy_statistics": accuracy_stats,
            "execution_time_statistics": execution_stats,
            "user_satisfaction_statistics": satisfaction_stats
        }
    
    def _analyze_learning_value(self, interactions: List[CLIInteractionData]) -> Dict[str, Any]:
        """åˆ†æå­¸ç¿’åƒ¹å€¼"""
        
        # æŒ‰å­¸ç¿’åƒ¹å€¼åˆ†çµ„
        value_groups = defaultdict(list)
        for interaction in interactions:
            value_groups[interaction.learning_value.value].append(interaction)
        
        value_analysis = {}
        for value, value_interactions in value_groups.items():
            # ä»»å‹™é¡å‹åˆ†å¸ƒ
            task_dist = Counter(i.task_type.value for i in value_interactions)
            
            # è¤‡é›œåº¦åˆ†å¸ƒ
            complexity_dist = Counter(i.complexity_level.value for i in value_interactions)
            
            # æˆåŠŸç‡
            success_count = sum(1 for i in value_interactions 
                              if i.result_status in [ResultStatus.SUCCESS_PERFECT, 
                                                   ResultStatus.SUCCESS_PARTIAL,
                                                   ResultStatus.SUCCESS_ACCEPTABLE])
            success_rate = success_count / len(value_interactions)
            
            value_analysis[value] = {
                "count": len(value_interactions),
                "percentage": len(value_interactions) / len(interactions) * 100,
                "task_distribution": dict(task_dist),
                "complexity_distribution": dict(complexity_dist),
                "success_rate": success_rate
            }
        
        return value_analysis
    
    def _analyze_tool_usage(self, interactions: List[CLIInteractionData]) -> Dict[str, Any]:
        """åˆ†æå·¥å…·ä½¿ç”¨æ¨¡å¼"""
        
        # å·¥å…·ä½¿ç”¨çµ±è¨ˆ
        tool_usage = defaultdict(int)
        tool_success = defaultdict(int)
        tool_accuracy = defaultdict(list)
        tool_execution_time = defaultdict(list)
        
        for interaction in interactions:
            is_success = interaction.result_status in [
                ResultStatus.SUCCESS_PERFECT, 
                ResultStatus.SUCCESS_PARTIAL,
                ResultStatus.SUCCESS_ACCEPTABLE
            ]
            
            for tool in interaction.tools_used:
                tool_usage[tool] += 1
                if is_success:
                    tool_success[tool] += 1
                
                if interaction.accuracy_score is not None:
                    tool_accuracy[tool].append(interaction.accuracy_score)
                
                if interaction.execution_time > 0:
                    tool_execution_time[tool].append(interaction.execution_time)
        
        # è¨ˆç®—å·¥å…·çµ±è¨ˆ
        tool_stats = {}
        for tool in tool_usage:
            success_rate = tool_success[tool] / tool_usage[tool] if tool_usage[tool] > 0 else 0
            avg_accuracy = np.mean(tool_accuracy[tool]) if tool_accuracy[tool] else None
            avg_execution_time = np.mean(tool_execution_time[tool]) if tool_execution_time[tool] else None
            
            tool_stats[tool] = {
                "usage_count": tool_usage[tool],
                "success_rate": success_rate,
                "average_accuracy": avg_accuracy,
                "average_execution_time": avg_execution_time
            }
        
        # æ’åºå·¥å…·
        most_used = sorted(tool_stats.items(), key=lambda x: x[1]["usage_count"], reverse=True)[:10]
        most_successful = sorted(
            [(k, v) for k, v in tool_stats.items() if v["usage_count"] >= 2],
            key=lambda x: x[1]["success_rate"], 
            reverse=True
        )[:10]
        
        return {
            "tool_statistics": tool_stats,
            "most_used_tools": dict(most_used),
            "most_successful_tools": dict(most_successful),
            "total_unique_tools": len(tool_stats)
        }
    
    def _analyze_temporal_patterns(self, interactions: List[CLIInteractionData]) -> Dict[str, Any]:
        """åˆ†ææ™‚é–“æ¨¡å¼"""
        
        # æŒ‰å°æ™‚åˆ†çµ„
        hourly_usage = defaultdict(int)
        daily_usage = defaultdict(int)
        
        for interaction in interactions:
            hour = interaction.timestamp.hour
            date = interaction.timestamp.date()
            
            hourly_usage[hour] += 1
            daily_usage[str(date)] += 1
        
        # è¨ˆç®—è¶¨å‹¢
        sorted_daily = sorted(daily_usage.items())
        if len(sorted_daily) > 1:
            daily_trend = "increasing" if sorted_daily[-1][1] > sorted_daily[0][1] else "decreasing"
        else:
            daily_trend = "stable"
        
        return {
            "hourly_distribution": dict(hourly_usage),
            "daily_usage": dict(daily_usage),
            "usage_trend": daily_trend,
            "peak_hour": max(hourly_usage, key=hourly_usage.get) if hourly_usage else None
        }
    
    def _calculate_quality_metrics(self, interactions: List[CLIInteractionData]) -> Dict[str, Any]:
        """è¨ˆç®—æ•¸æ“šè³ªé‡æŒ‡æ¨™"""
        
        total = len(interactions)
        
        # å®Œæ•´æ€§æª¢æŸ¥
        complete_accuracy = sum(1 for i in interactions if i.accuracy_score is not None)
        complete_satisfaction = sum(1 for i in interactions if i.user_satisfaction is not None)
        complete_tools = sum(1 for i in interactions if i.tools_used)
        complete_execution_time = sum(1 for i in interactions if i.execution_time > 0)
        
        # ä¸€è‡´æ€§æª¢æŸ¥
        consistent_results = sum(1 for i in interactions 
                               if (i.result_status in [ResultStatus.SUCCESS_PERFECT, 
                                                     ResultStatus.SUCCESS_PARTIAL,
                                                     ResultStatus.SUCCESS_ACCEPTABLE] 
                                   and (i.accuracy_score is None or i.accuracy_score > 0)))
        
        return {
            "completeness": {
                "accuracy_score": complete_accuracy / total,
                "user_satisfaction": complete_satisfaction / total,
                "tools_used": complete_tools / total,
                "execution_time": complete_execution_time / total
            },
            "consistency": {
                "result_accuracy_alignment": consistent_results / total
            },
            "overall_quality_score": (
                (complete_accuracy + complete_satisfaction + complete_tools + complete_execution_time) 
                / (4 * total)
            )
        }
    
    def _assess_training_readiness(self, interactions: List[CLIInteractionData]) -> Dict[str, Any]:
        """è©•ä¼°è¨“ç·´æº–å‚™åº¦"""
        
        # é«˜åƒ¹å€¼æ•¸æ“šçµ±è¨ˆ
        high_value_count = sum(1 for i in interactions if i.learning_value == LearningValue.HIGH)
        medium_value_count = sum(1 for i in interactions if i.learning_value == LearningValue.MEDIUM)
        
        # ä»»å‹™é¡å‹å¹³è¡¡æ€§
        task_counts = Counter(i.task_type.value for i in interactions)
        task_balance_score = 1.0 - (np.std(list(task_counts.values())) / np.mean(list(task_counts.values())))
        
        # è¤‡é›œåº¦å¹³è¡¡æ€§
        complexity_counts = Counter(i.complexity_level.value for i in interactions)
        complexity_balance_score = 1.0 - (np.std(list(complexity_counts.values())) / np.mean(list(complexity_counts.values())))
        
        # æˆåŠŸæ¡ˆä¾‹æ¯”ä¾‹
        success_count = sum(1 for i in interactions 
                          if i.result_status in [ResultStatus.SUCCESS_PERFECT, 
                                               ResultStatus.SUCCESS_PARTIAL,
                                               ResultStatus.SUCCESS_ACCEPTABLE])
        success_ratio = success_count / len(interactions)
        
        # è¨“ç·´æº–å‚™åº¦è©•åˆ†
        readiness_score = (
            min(high_value_count / 50, 1.0) * 0.3 +  # é«˜åƒ¹å€¼æ•¸æ“šå……è¶³æ€§
            min(medium_value_count / 100, 1.0) * 0.2 +  # ä¸­åƒ¹å€¼æ•¸æ“šå……è¶³æ€§
            task_balance_score * 0.2 +  # ä»»å‹™é¡å‹å¹³è¡¡æ€§
            complexity_balance_score * 0.1 +  # è¤‡é›œåº¦å¹³è¡¡æ€§
            min(success_ratio / 0.7, 1.0) * 0.2  # æˆåŠŸæ¡ˆä¾‹æ¯”ä¾‹
        )
        
        return {
            "high_value_samples": high_value_count,
            "medium_value_samples": medium_value_count,
            "task_balance_score": task_balance_score,
            "complexity_balance_score": complexity_balance_score,
            "success_ratio": success_ratio,
            "overall_readiness_score": readiness_score,
            "readiness_level": self._get_readiness_level(readiness_score),
            "recommendations": self._get_training_recommendations(readiness_score, interactions)
        }
    
    def _get_readiness_level(self, score: float) -> str:
        """ç²å–æº–å‚™åº¦ç­‰ç´š"""
        if score >= 0.8:
            return "excellent"
        elif score >= 0.6:
            return "good"
        elif score >= 0.4:
            return "fair"
        else:
            return "poor"
    
    def _get_training_recommendations(self, score: float, interactions: List[CLIInteractionData]) -> List[str]:
        """ç²å–è¨“ç·´å»ºè­°"""
        recommendations = []
        
        if score < 0.8:
            # æª¢æŸ¥å…·é«”å•é¡Œ
            high_value_count = sum(1 for i in interactions if i.learning_value == LearningValue.HIGH)
            if high_value_count < 50:
                recommendations.append(f"éœ€è¦æ›´å¤šé«˜åƒ¹å€¼æ•¸æ“šæ¨£æœ¬ï¼ˆç•¶å‰ï¼š{high_value_count}ï¼Œå»ºè­°ï¼š50+ï¼‰")
            
            task_counts = Counter(i.task_type.value for i in interactions)
            min_task_count = min(task_counts.values())
            if min_task_count < 10:
                recommendations.append(f"æŸäº›ä»»å‹™é¡å‹æ•¸æ“šä¸è¶³ï¼ˆæœ€å°‘ï¼š{min_task_count}ï¼Œå»ºè­°ï¼š10+ï¼‰")
            
            success_count = sum(1 for i in interactions 
                              if i.result_status in [ResultStatus.SUCCESS_PERFECT, 
                                                   ResultStatus.SUCCESS_PARTIAL,
                                                   ResultStatus.SUCCESS_ACCEPTABLE])
            success_ratio = success_count / len(interactions)
            if success_ratio < 0.7:
                recommendations.append(f"æˆåŠŸæ¡ˆä¾‹æ¯”ä¾‹åä½ï¼ˆç•¶å‰ï¼š{success_ratio:.2f}ï¼Œå»ºè­°ï¼š0.7+ï¼‰")
        
        if not recommendations:
            recommendations.append("æ•¸æ“šè³ªé‡è‰¯å¥½ï¼Œå¯ä»¥é–‹å§‹è¨“ç·´")
        
        return recommendations

class CLITrainingDataBuilder:
    """CLIè¨“ç·´æ•¸æ“šæ§‹å»ºå™¨"""
    
    def __init__(self, storage_dir: str = "/home/ubuntu/Powerauto.ai/cli_training_data"):
        self.storage = CLIDataStorage(storage_dir)
        self.storage_dir = Path(storage_dir)
        self.training_sets_dir = self.storage_dir / "training_sets"
        
    def build_gaia_optimization_dataset(self) -> Dict[str, Any]:
        """æ§‹å»ºGAIAå„ªåŒ–è¨“ç·´é›†"""
        
        # æŸ¥è©¢GAIAç›¸é—œçš„é«˜åƒ¹å€¼äº¤äº’
        gaia_interactions = self.storage.query_interactions(
            task_type=TaskType.GAIA_TESTING
        )
        
        if not gaia_interactions:
            return {"error": "æ²’æœ‰æ‰¾åˆ°GAIAæ¸¬è©¦æ•¸æ“š"}
        
        # éæ¿¾é«˜åƒ¹å€¼æ•¸æ“š
        high_value_interactions = [
            i for i in gaia_interactions 
            if i.learning_value in [LearningValue.HIGH, LearningValue.MEDIUM]
            and i.result_status in [ResultStatus.SUCCESS_PERFECT, ResultStatus.SUCCESS_PARTIAL]
        ]
        
        # æ§‹å»ºè¨“ç·´æ¨£æœ¬
        training_samples = []
        for interaction in high_value_interactions:
            sample = {
                "input": {
                    "task_type": interaction.task_type.value,
                    "complexity": interaction.complexity_level.value,
                    "command": interaction.command,
                    "arguments": interaction.arguments,
                    "context": interaction.context
                },
                "optimal_strategy": {
                    "tools": interaction.tools_used,
                    "mcp_adapters": interaction.mcp_adapters_involved,
                    "execution_sequence": self._extract_execution_sequence(interaction)
                },
                "expected_outcome": {
                    "accuracy": interaction.accuracy_score,
                    "execution_time": interaction.execution_time,
                    "success_probability": self._calculate_success_probability(interaction)
                },
                "metadata": {
                    "interaction_id": interaction.interaction_id,
                    "timestamp": interaction.timestamp.isoformat(),
                    "learning_value": interaction.learning_value.value
                }
            }
            training_samples.append(sample)
        
        dataset = {
            "dataset_name": "gaia_optimization",
            "version": "1.0",
            "created_at": datetime.now().isoformat(),
            "samples": training_samples,
            "metadata": {
                "total_samples": len(training_samples),
                "source_interactions": len(gaia_interactions),
                "quality_score": self._calculate_dataset_quality(training_samples)
            }
        }
        
        # ä¿å­˜æ•¸æ“šé›†
        output_path = self.training_sets_dir / "gaia_optimization_dataset.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, indent=2, ensure_ascii=False)
        
        logger.info(f"GAIAå„ªåŒ–æ•¸æ“šé›†å·²ä¿å­˜åˆ°: {output_path}")
        
        return dataset
    
    def build_tool_selection_dataset(self) -> Dict[str, Any]:
        """æ§‹å»ºå·¥å…·é¸æ“‡è¨“ç·´é›†"""
        
        # ç²å–æ‰€æœ‰æˆåŠŸçš„äº¤äº’
        successful_interactions = self.storage.query_interactions(
            result_status=[ResultStatus.SUCCESS_PERFECT, ResultStatus.SUCCESS_PARTIAL]
        )
        
        if not successful_interactions:
            return {"error": "æ²’æœ‰æ‰¾åˆ°æˆåŠŸçš„äº¤äº’æ•¸æ“š"}
        
        # åˆ†æå·¥å…·é¸æ“‡æ¨¡å¼
        tool_patterns = self._analyze_tool_selection_patterns(successful_interactions)
        
        # æ§‹å»ºå·¥å…·é¸æ“‡æ¨£æœ¬
        selection_samples = []
        for pattern in tool_patterns:
            sample = {
                "task_description": {
                    "task_type": pattern["task_type"],
                    "complexity": pattern["complexity"],
                    "context_features": pattern["context_features"]
                },
                "available_tools": pattern["available_tools"],
                "optimal_selection": {
                    "selected_tools": pattern["best_tools"],
                    "selection_rationale": pattern["rationale"],
                    "expected_performance": pattern["performance"]
                },
                "alternatives": pattern["alternatives"]
            }
            selection_samples.append(sample)
        
        dataset = {
            "dataset_name": "tool_selection",
            "version": "1.0",
            "created_at": datetime.now().isoformat(),
            "samples": selection_samples,
            "metadata": {
                "total_samples": len(selection_samples),
                "pattern_count": len(tool_patterns)
            }
        }
        
        # ä¿å­˜æ•¸æ“šé›†
        output_path = self.training_sets_dir / "tool_selection_dataset.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, indent=2, ensure_ascii=False)
        
        logger.info(f"å·¥å…·é¸æ“‡æ•¸æ“šé›†å·²ä¿å­˜åˆ°: {output_path}")
        
        return dataset
    
    def build_error_prevention_dataset(self) -> Dict[str, Any]:
        """æ§‹å»ºéŒ¯èª¤é é˜²è¨“ç·´é›†"""
        
        # ç²å–å¤±æ•—çš„äº¤äº’
        failed_interactions = self.storage.query_interactions(
            result_status=[ResultStatus.FAILURE_USER, ResultStatus.FAILURE_SYSTEM, 
                          ResultStatus.FAILURE_CONFIG, ResultStatus.FAILURE_RESOURCE]
        )
        
        if not failed_interactions:
            return {"error": "æ²’æœ‰æ‰¾åˆ°å¤±æ•—çš„äº¤äº’æ•¸æ“š"}
        
        # æ§‹å»ºéŒ¯èª¤é é˜²æ¨£æœ¬
        error_samples = []
        for interaction in failed_interactions:
            sample = {
                "input_context": {
                    "command": interaction.command,
                    "arguments": interaction.arguments,
                    "context": interaction.context,
                    "task_type": interaction.task_type.value,
                    "complexity": interaction.complexity_level.value
                },
                "error_info": {
                    "error_type": interaction.result_status.value,
                    "error_details": interaction.error_info,
                    "tools_attempted": interaction.tools_used
                },
                "prevention_strategy": {
                    "validation_checks": self._suggest_validation_checks(interaction),
                    "alternative_approaches": self._suggest_alternatives(interaction),
                    "risk_factors": self._identify_risk_factors(interaction)
                }
            }
            error_samples.append(sample)
        
        dataset = {
            "dataset_name": "error_prevention",
            "version": "1.0",
            "created_at": datetime.now().isoformat(),
            "samples": error_samples,
            "metadata": {
                "total_samples": len(error_samples),
                "error_types": list(set(i.result_status.value for i in failed_interactions))
            }
        }
        
        # ä¿å­˜æ•¸æ“šé›†
        output_path = self.training_sets_dir / "error_prevention_dataset.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, indent=2, ensure_ascii=False)
        
        logger.info(f"éŒ¯èª¤é é˜²æ•¸æ“šé›†å·²ä¿å­˜åˆ°: {output_path}")
        
        return dataset
    
    def _extract_execution_sequence(self, interaction: CLIInteractionData) -> List[str]:
        """æå–åŸ·è¡Œåºåˆ—"""
        # é€™è£¡å¯ä»¥æ ¹æ“šå¯¦éš›æƒ…æ³æå–å·¥å…·çš„åŸ·è¡Œé †åº
        return interaction.tools_used
    
    def _calculate_success_probability(self, interaction: CLIInteractionData) -> float:
        """è¨ˆç®—æˆåŠŸæ¦‚ç‡"""
        # åŸºæ–¼æ­·å²æ•¸æ“šè¨ˆç®—é¡ä¼¼ä»»å‹™çš„æˆåŠŸæ¦‚ç‡
        base_prob = 0.7  # åŸºç¤æ¦‚ç‡
        
        if interaction.accuracy_score:
            base_prob = interaction.accuracy_score
        
        # æ ¹æ“šè¤‡é›œåº¦èª¿æ•´
        complexity_adjustment = {
            ComplexityLevel.SIMPLE: 0.1,
            ComplexityLevel.MODERATE: 0.0,
            ComplexityLevel.COMPLEX: -0.1,
            ComplexityLevel.EXPERT: -0.2
        }
        
        adjusted_prob = base_prob + complexity_adjustment.get(interaction.complexity_level, 0)
        return max(0.0, min(1.0, adjusted_prob))
    
    def _calculate_dataset_quality(self, samples: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—æ•¸æ“šé›†è³ªé‡åˆ†æ•¸"""
        if not samples:
            return 0.0
        
        # åŸºæ–¼æ¨£æœ¬æ•¸é‡ã€å¤šæ¨£æ€§ç­‰å› ç´ è¨ˆç®—è³ªé‡åˆ†æ•¸
        sample_count_score = min(len(samples) / 100, 1.0)  # 100å€‹æ¨£æœ¬ç‚ºæ»¿åˆ†
        
        # å¯ä»¥æ·»åŠ æ›´å¤šè³ªé‡æŒ‡æ¨™
        return sample_count_score
    
    def _analyze_tool_selection_patterns(self, interactions: List[CLIInteractionData]) -> List[Dict[str, Any]]:
        """åˆ†æå·¥å…·é¸æ“‡æ¨¡å¼"""
        
        patterns = []
        
        # æŒ‰ä»»å‹™é¡å‹å’Œè¤‡é›œåº¦åˆ†çµ„
        groups = defaultdict(list)
        for interaction in interactions:
            key = (interaction.task_type.value, interaction.complexity_level.value)
            groups[key].append(interaction)
        
        for (task_type, complexity), group_interactions in groups.items():
            if len(group_interactions) < 2:  # è‡³å°‘éœ€è¦2å€‹æ¨£æœ¬
                continue
            
            # åˆ†ææœ€å¸¸ç”¨çš„å·¥å…·çµ„åˆ
            tool_combinations = Counter(tuple(sorted(i.tools_used)) for i in group_interactions)
            most_common_tools = tool_combinations.most_common(1)[0][0] if tool_combinations else []
            
            # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
            avg_accuracy = np.mean([i.accuracy_score for i in group_interactions 
                                  if i.accuracy_score is not None])
            avg_execution_time = np.mean([i.execution_time for i in group_interactions 
                                        if i.execution_time > 0])
            
            pattern = {
                "task_type": task_type,
                "complexity": complexity,
                "context_features": self._extract_context_features(group_interactions),
                "available_tools": list(set(tool for i in group_interactions for tool in i.tools_used)),
                "best_tools": list(most_common_tools),
                "rationale": f"æœ€å¸¸ç”¨çµ„åˆï¼Œå¹³å‡æº–ç¢ºç‡: {avg_accuracy:.2f}",
                "performance": {
                    "accuracy": avg_accuracy,
                    "execution_time": avg_execution_time
                },
                "alternatives": []  # å¯ä»¥æ·»åŠ å…¶ä»–å·¥å…·çµ„åˆ
            }
            patterns.append(pattern)
        
        return patterns
    
    def _extract_context_features(self, interactions: List[CLIInteractionData]) -> Dict[str, Any]:
        """æå–ä¸Šä¸‹æ–‡ç‰¹å¾µ"""
        
        # åˆ†æå¸¸è¦‹çš„ä¸Šä¸‹æ–‡ç‰¹å¾µ
        all_contexts = [i.context for i in interactions if i.context]
        
        if not all_contexts:
            return {}
        
        # æå–å¸¸è¦‹çš„éµå€¼å°
        common_features = {}
        for context in all_contexts:
            for key, value in context.items():
                if key not in common_features:
                    common_features[key] = []
                common_features[key].append(value)
        
        # è¨ˆç®—æœ€å¸¸è¦‹çš„å€¼
        feature_summary = {}
        for key, values in common_features.items():
            if len(set(values)) == 1:
                feature_summary[key] = values[0]
            else:
                feature_summary[key] = Counter(values).most_common(1)[0][0]
        
        return feature_summary
    
    def _suggest_validation_checks(self, interaction: CLIInteractionData) -> List[str]:
        """å»ºè­°é©—è­‰æª¢æŸ¥"""
        checks = []
        
        if interaction.result_status == ResultStatus.FAILURE_USER:
            checks.append("è¼¸å…¥åƒæ•¸é©—è­‰")
            checks.append("å‘½ä»¤æ ¼å¼æª¢æŸ¥")
        elif interaction.result_status == ResultStatus.FAILURE_SYSTEM:
            checks.append("ç³»çµ±è³‡æºæª¢æŸ¥")
            checks.append("ä¾è³´é …é©—è­‰")
        elif interaction.result_status == ResultStatus.FAILURE_CONFIG:
            checks.append("é…ç½®æ–‡ä»¶é©—è­‰")
            checks.append("ç’°å¢ƒè®Šé‡æª¢æŸ¥")
        
        return checks
    
    def _suggest_alternatives(self, interaction: CLIInteractionData) -> List[str]:
        """å»ºè­°æ›¿ä»£æ–¹æ¡ˆ"""
        alternatives = []
        
        # åŸºæ–¼å¤±æ•—çš„å·¥å…·å»ºè­°æ›¿ä»£å·¥å…·
        for tool in interaction.tools_used:
            if "claude" in tool.lower():
                alternatives.append("å˜—è©¦ä½¿ç”¨gemini_mcp")
            elif "gemini" in tool.lower():
                alternatives.append("å˜—è©¦ä½¿ç”¨claude_mcp")
        
        return alternatives
    
    def _identify_risk_factors(self, interaction: CLIInteractionData) -> List[str]:
        """è­˜åˆ¥é¢¨éšªå› ç´ """
        risks = []
        
        if interaction.complexity_level == ComplexityLevel.EXPERT:
            risks.append("é«˜è¤‡é›œåº¦ä»»å‹™")
        
        if interaction.execution_time > 60:
            risks.append("é•·æ™‚é–“åŸ·è¡Œ")
        
        if len(interaction.tools_used) > 5:
            risks.append("ä½¿ç”¨éå¤šå·¥å…·")
        
        return risks

def main():
    """ä¸»å‡½æ•¸ - æ¼”ç¤ºæ•¸æ“šåˆ†æå’Œè™•ç†åŠŸèƒ½"""
    
    print("ğŸ” CLIæ•¸æ“šè™•ç†å’Œåˆ†æå·¥å…·")
    print("=" * 40)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = CLIDataAnalyzer()
    
    # ç”Ÿæˆç¶œåˆå ±å‘Š
    print("ğŸ“Š ç”Ÿæˆç¶œåˆåˆ†æå ±å‘Š...")
    report = analyzer.generate_comprehensive_report()
    
    if "error" in report:
        print(f"âŒ {report['error']}")
        return
    
    # é¡¯ç¤ºæ¦‚è¦½
    overview = report["overview"]
    print(f"\nğŸ“ˆ æ•¸æ“šæ¦‚è¦½:")
    print(f"   ç¸½äº¤äº’æ•¸: {overview['total_interactions']}")
    print(f"   æ™‚é–“ç¯„åœ: {overview['time_range']['duration_days']}å¤©")
    print(f"   ä»»å‹™é¡å‹: {list(overview['task_type_distribution'].keys())}")
    
    # é¡¯ç¤ºæ€§èƒ½åˆ†æ
    performance = report["performance_analysis"]
    if performance["accuracy_statistics"]:
        acc_stats = performance["accuracy_statistics"]
        print(f"\nğŸ¯ æº–ç¢ºç‡çµ±è¨ˆ:")
        print(f"   å¹³å‡æº–ç¢ºç‡: {acc_stats['mean']:.3f}")
        print(f"   æº–ç¢ºç‡ç¯„åœ: {acc_stats['min']:.3f} - {acc_stats['max']:.3f}")
    
    # é¡¯ç¤ºè¨“ç·´æº–å‚™åº¦
    readiness = report["training_readiness"]
    print(f"\nğŸš€ è¨“ç·´æº–å‚™åº¦:")
    print(f"   æº–å‚™åº¦ç­‰ç´š: {readiness['readiness_level']}")
    print(f"   æº–å‚™åº¦åˆ†æ•¸: {readiness['overall_readiness_score']:.3f}")
    print(f"   é«˜åƒ¹å€¼æ¨£æœ¬: {readiness['high_value_samples']}")
    
    # ä¿å­˜å ±å‘Š
    report_path = "/home/ubuntu/Powerauto.ai/cli_training_data/metadata/analysis_report.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ’¾ åˆ†æå ±å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    # æ§‹å»ºè¨“ç·´æ•¸æ“šé›†
    print("\nğŸ—ï¸ æ§‹å»ºè¨“ç·´æ•¸æ“šé›†...")
    builder = CLITrainingDataBuilder()
    
    # æ§‹å»ºGAIAå„ªåŒ–æ•¸æ“šé›†
    gaia_dataset = builder.build_gaia_optimization_dataset()
    if "error" not in gaia_dataset:
        print(f"   âœ… GAIAå„ªåŒ–æ•¸æ“šé›†: {gaia_dataset['metadata']['total_samples']}å€‹æ¨£æœ¬")
    
    # æ§‹å»ºå·¥å…·é¸æ“‡æ•¸æ“šé›†
    tool_dataset = builder.build_tool_selection_dataset()
    if "error" not in tool_dataset:
        print(f"   âœ… å·¥å…·é¸æ“‡æ•¸æ“šé›†: {tool_dataset['metadata']['total_samples']}å€‹æ¨£æœ¬")
    
    # æ§‹å»ºéŒ¯èª¤é é˜²æ•¸æ“šé›†
    error_dataset = builder.build_error_prevention_dataset()
    if "error" not in error_dataset:
        print(f"   âœ… éŒ¯èª¤é é˜²æ•¸æ“šé›†: {error_dataset['metadata']['total_samples']}å€‹æ¨£æœ¬")
    
    print("\nğŸ‰ æ•¸æ“šè™•ç†å’Œåˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()

