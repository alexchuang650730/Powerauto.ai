#!/usr/bin/env python3
"""
KiloRAGå®Œæ•´é›†æˆç³»çµ±
å°‡æ‰€æœ‰é …ç›®æ–‡ä»¶å’Œäº¤äº’æ•¸æ“šæ’å…¥RAGæª¢ç´¢ç³»çµ±
"""

import os
import sys
import json
import asyncio
import logging
import hashlib
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path
import sqlite3

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append('/home/ubuntu/Powerauto.ai')

# å°å…¥ç¾æœ‰çµ„ä»¶
try:
    from memory_system.rag_integration.rag_integration import RAGIntegration
except ImportError:
    class RAGIntegration:
        def __init__(self):
            self.logger = logging.getLogger("rag_integration")
        
        def add_memory(self, content, metadata):
            return {"status": "mock", "memory_id": "test_id"}

try:
    from mcptool.adapters.simple_kilocode_adapter import SimpleKiloCodeAdapter
except ImportError:
    class SimpleKiloCodeAdapter:
        def process_request(self, request_type, data):
            return {"status": "mock", "result": "KiloCodeè™•ç†çµæœ"}

logger = logging.getLogger(__name__)

class KiloRAGIntegrationSystem:
    """KiloRAGå®Œæ•´é›†æˆç³»çµ±"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ–KiloRAGé›†æˆç³»çµ±"""
        self.config = config or {}
        self.base_path = self.config.get('base_path', '/home/ubuntu/Powerauto.ai')
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.rag_integration = RAGIntegration()
        self.kilocode_adapter = SimpleKiloCodeAdapter()
        
        # æ•¸æ“šåº«é€£æ¥
        self.db_path = os.path.join(self.base_path, 'data', 'kilorag.db')
        self._init_database()
        
        # ç´¢å¼•ç‹€æ…‹
        self.indexed_files = set()
        self.indexed_interactions = set()
        
        # æ”¯æŒçš„æ–‡ä»¶é¡å‹
        self.supported_extensions = {
            '.py', '.js', '.ts', '.java', '.cpp', '.c', '.h',
            '.md', '.txt', '.json', '.yaml', '.yml', '.xml',
            '.html', '.css', '.sql', '.sh', '.bat'
        }
        
        logger.info("KiloRAGé›†æˆç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•¸æ“šåº«"""
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # æ–‡ä»¶ç´¢å¼•è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS file_index (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_path TEXT UNIQUE NOT NULL,
                    file_hash TEXT NOT NULL,
                    content_preview TEXT,
                    file_type TEXT,
                    size INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    rag_memory_id TEXT,
                    indexed BOOLEAN DEFAULT FALSE
                )
            ''')
            
            # äº¤äº’æ•¸æ“šè¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS interaction_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    interaction_id TEXT UNIQUE NOT NULL,
                    user_input TEXT,
                    ai_response TEXT,
                    context_data TEXT,
                    quality_score REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    rag_memory_id TEXT,
                    indexed BOOLEAN DEFAULT FALSE
                )
            ''')
            
            # RAGæª¢ç´¢è¨˜éŒ„è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS rag_queries (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    query_text TEXT NOT NULL,
                    results_count INTEGER,
                    query_time REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            logger.info("KiloRAGæ•¸æ“šåº«åˆå§‹åŒ–å®Œæˆ")
    
    async def scan_and_index_all_files(self) -> Dict[str, Any]:
        """æƒæä¸¦ç´¢å¼•æ‰€æœ‰é …ç›®æ–‡ä»¶"""
        logger.info("é–‹å§‹æƒæå’Œç´¢å¼•æ‰€æœ‰é …ç›®æ–‡ä»¶...")
        
        results = {
            "total_files": 0,
            "indexed_files": 0,
            "skipped_files": 0,
            "failed_files": 0,
            "file_types": {},
            "errors": []
        }
        
        try:
            # éæ­·é …ç›®ç›®éŒ„
            for root, dirs, files in os.walk(self.base_path):
                # è·³éç‰¹å®šç›®éŒ„
                dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'node_modules']]
                
                for file in files:
                    file_path = os.path.join(root, file)
                    relative_path = os.path.relpath(file_path, self.base_path)
                    
                    results["total_files"] += 1
                    
                    # æª¢æŸ¥æ–‡ä»¶é¡å‹
                    file_ext = Path(file).suffix.lower()
                    if file_ext not in self.supported_extensions:
                        results["skipped_files"] += 1
                        continue
                    
                    # çµ±è¨ˆæ–‡ä»¶é¡å‹
                    results["file_types"][file_ext] = results["file_types"].get(file_ext, 0) + 1
                    
                    # ç´¢å¼•æ–‡ä»¶
                    try:
                        index_result = await self._index_single_file(file_path, relative_path)
                        if index_result["status"] == "success":
                            results["indexed_files"] += 1
                        else:
                            results["failed_files"] += 1
                            results["errors"].append(f"{relative_path}: {index_result.get('error', 'Unknown error')}")
                    except Exception as e:
                        results["failed_files"] += 1
                        results["errors"].append(f"{relative_path}: {str(e)}")
                        logger.error(f"ç´¢å¼•æ–‡ä»¶å¤±æ•— {relative_path}: {e}")
            
            logger.info(f"æ–‡ä»¶ç´¢å¼•å®Œæˆ: {results['indexed_files']}/{results['total_files']} æˆåŠŸ")
            return results
            
        except Exception as e:
            logger.error(f"æƒææ–‡ä»¶å¤±æ•—: {e}")
            results["errors"].append(f"æƒæå¤±æ•—: {str(e)}")
            return results
    
    async def _index_single_file(self, file_path: str, relative_path: str) -> Dict[str, Any]:
        """ç´¢å¼•å–®å€‹æ–‡ä»¶"""
        try:
            # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç´¢å¼•
            file_hash = self._calculate_file_hash(file_path)
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT file_hash, rag_memory_id FROM file_index WHERE file_path = ?",
                    (relative_path,)
                )
                existing = cursor.fetchone()
                
                if existing and existing[0] == file_hash:
                    return {"status": "skipped", "reason": "already_indexed"}
            
            # è®€å–æ–‡ä»¶å…§å®¹
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            except UnicodeDecodeError:
                # å˜—è©¦å…¶ä»–ç·¨ç¢¼
                with open(file_path, 'r', encoding='latin-1') as f:
                    content = f.read()
            
            # æº–å‚™RAGè¨˜æ†¶æ•¸æ“š
            file_info = os.stat(file_path)
            metadata = {
                "file_path": relative_path,
                "file_type": Path(file_path).suffix.lower(),
                "file_size": file_info.st_size,
                "created_at": datetime.fromtimestamp(file_info.st_ctime).isoformat(),
                "modified_at": datetime.fromtimestamp(file_info.st_mtime).isoformat(),
                "content_type": "project_file"
            }
            
            # å‰µå»ºå…§å®¹é è¦½
            content_preview = content[:500] + "..." if len(content) > 500 else content
            
            # æ·»åŠ åˆ°RAGç³»çµ±
            rag_result = self.rag_integration.add_memory(
                content=content,
                metadata=metadata
            )
            
            if rag_result.get("status") == "success":
                rag_memory_id = rag_result.get("memory_id")
                
                # ä¿å­˜åˆ°æ•¸æ“šåº«
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute('''
                        INSERT OR REPLACE INTO file_index 
                        (file_path, file_hash, content_preview, file_type, size, rag_memory_id, indexed, updated_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                    ''', (
                        relative_path, file_hash, content_preview,
                        metadata["file_type"], metadata["file_size"],
                        rag_memory_id, True
                    ))
                    conn.commit()
                
                return {"status": "success", "rag_memory_id": rag_memory_id}
            else:
                return {"status": "error", "error": "RAGç´¢å¼•å¤±æ•—"}
                
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """è¨ˆç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
        except Exception:
            # å¦‚æœç„¡æ³•è®€å–æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶è·¯å¾‘å’Œä¿®æ”¹æ™‚é–“
            stat = os.stat(file_path)
            hash_md5.update(f"{file_path}_{stat.st_mtime}".encode())
        
        return hash_md5.hexdigest()
    
    async def index_interaction_data(self, interaction_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç´¢å¼•äº¤äº’æ•¸æ“š"""
        try:
            interaction_id = interaction_data.get("interaction_id") or self._generate_interaction_id(interaction_data)
            
            # æª¢æŸ¥æ˜¯å¦å·²ç´¢å¼•
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT rag_memory_id FROM interaction_data WHERE interaction_id = ?",
                    (interaction_id,)
                )
                existing = cursor.fetchone()
                
                if existing:
                    return {"status": "skipped", "reason": "already_indexed"}
            
            # æº–å‚™RAGè¨˜æ†¶æ•¸æ“š
            user_input = interaction_data.get("user_input", "")
            ai_response = interaction_data.get("ai_response", "")
            context_data = interaction_data.get("context", {})
            
            # çµ„åˆå…§å®¹ç”¨æ–¼RAGæª¢ç´¢
            combined_content = f"""
ç”¨æˆ¶è¼¸å…¥: {user_input}
AIå›æ‡‰: {ai_response}
ä¸Šä¸‹æ–‡: {json.dumps(context_data, ensure_ascii=False, indent=2)}
            """.strip()
            
            metadata = {
                "interaction_id": interaction_id,
                "user_input": user_input,
                "ai_response": ai_response,
                "quality_score": interaction_data.get("quality_score", 0.5),
                "timestamp": interaction_data.get("timestamp", datetime.now().isoformat()),
                "content_type": "interaction_data"
            }
            
            # æ·»åŠ åˆ°RAGç³»çµ±
            rag_result = self.rag_integration.add_memory(
                content=combined_content,
                metadata=metadata
            )
            
            if rag_result.get("status") == "success":
                rag_memory_id = rag_result.get("memory_id")
                
                # ä¿å­˜åˆ°æ•¸æ“šåº«
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute('''
                        INSERT INTO interaction_data 
                        (interaction_id, user_input, ai_response, context_data, quality_score, rag_memory_id, indexed)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        interaction_id, user_input, ai_response,
                        json.dumps(context_data), metadata["quality_score"],
                        rag_memory_id, True
                    ))
                    conn.commit()
                
                return {"status": "success", "rag_memory_id": rag_memory_id}
            else:
                return {"status": "error", "error": "RAGç´¢å¼•å¤±æ•—"}
                
        except Exception as e:
            logger.error(f"ç´¢å¼•äº¤äº’æ•¸æ“šå¤±æ•—: {e}")
            return {"status": "error", "error": str(e)}
    
    def _generate_interaction_id(self, interaction_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆäº¤äº’ID"""
        content = f"{interaction_data.get('user_input', '')}{interaction_data.get('ai_response', '')}"
        return hashlib.md5(content.encode()).hexdigest()
    
    async def search_knowledge(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """æœç´¢çŸ¥è­˜åº«"""
        try:
            start_time = datetime.now()
            
            # ä½¿ç”¨RAGç³»çµ±æœç´¢
            search_results = self.rag_integration.search_memories(
                query=query,
                limit=limit
            )
            
            end_time = datetime.now()
            query_time = (end_time - start_time).total_seconds()
            
            # è¨˜éŒ„æŸ¥è©¢
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO rag_queries (query_text, results_count, query_time)
                    VALUES (?, ?, ?)
                ''', (query, len(search_results.get("results", [])), query_time))
                conn.commit()
            
            return {
                "status": "success",
                "query": query,
                "results": search_results.get("results", []),
                "total_results": len(search_results.get("results", [])),
                "query_time": query_time
            }
            
        except Exception as e:
            logger.error(f"æœç´¢çŸ¥è­˜åº«å¤±æ•—: {e}")
            return {
                "status": "error",
                "error": str(e),
                "results": []
            }
    
    async def get_system_status(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±ç‹€æ…‹"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # çµ±è¨ˆæ–‡ä»¶ç´¢å¼•
                cursor.execute("SELECT COUNT(*) FROM file_index WHERE indexed = TRUE")
                indexed_files_count = cursor.fetchone()[0]
                
                cursor.execute("SELECT COUNT(*) FROM file_index")
                total_files_count = cursor.fetchone()[0]
                
                # çµ±è¨ˆäº¤äº’æ•¸æ“š
                cursor.execute("SELECT COUNT(*) FROM interaction_data WHERE indexed = TRUE")
                indexed_interactions_count = cursor.fetchone()[0]
                
                # çµ±è¨ˆæŸ¥è©¢
                cursor.execute("SELECT COUNT(*) FROM rag_queries")
                total_queries_count = cursor.fetchone()[0]
                
                # æ–‡ä»¶é¡å‹çµ±è¨ˆ
                cursor.execute("SELECT file_type, COUNT(*) FROM file_index GROUP BY file_type")
                file_types = dict(cursor.fetchall())
                
                return {
                    "status": "active",
                    "indexed_files": indexed_files_count,
                    "total_files": total_files_count,
                    "indexed_interactions": indexed_interactions_count,
                    "total_queries": total_queries_count,
                    "file_types": file_types,
                    "database_path": self.db_path,
                    "last_updated": datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"ç²å–ç³»çµ±ç‹€æ…‹å¤±æ•—: {e}")
            return {
                "status": "error",
                "error": str(e)
            }


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ KiloRAGå®Œæ•´é›†æˆç³»çµ±æ¸¬è©¦\\n")
    
    # åˆå§‹åŒ–ç³»çµ±
    kilorag = KiloRAGIntegrationSystem()
    
    # ç²å–åˆå§‹ç‹€æ…‹
    print("ğŸ“Š ç³»çµ±åˆå§‹ç‹€æ…‹:")
    status = await kilorag.get_system_status()
    print(f"  å·²ç´¢å¼•æ–‡ä»¶: {status.get('indexed_files', 0)}")
    print(f"  å·²ç´¢å¼•äº¤äº’: {status.get('indexed_interactions', 0)}")
    print(f"  ç¸½æŸ¥è©¢æ¬¡æ•¸: {status.get('total_queries', 0)}")
    print()
    
    # æƒæå’Œç´¢å¼•æ–‡ä»¶
    print("ğŸ“ é–‹å§‹æƒæå’Œç´¢å¼•é …ç›®æ–‡ä»¶...")
    index_results = await kilorag.scan_and_index_all_files()
    
    print(f"  ç¸½æ–‡ä»¶æ•¸: {index_results['total_files']}")
    print(f"  æˆåŠŸç´¢å¼•: {index_results['indexed_files']}")
    print(f"  è·³éæ–‡ä»¶: {index_results['skipped_files']}")
    print(f"  å¤±æ•—æ–‡ä»¶: {index_results['failed_files']}")
    
    if index_results['file_types']:
        print("  æ–‡ä»¶é¡å‹åˆ†ä½ˆ:")
        for ext, count in index_results['file_types'].items():
            print(f"    {ext}: {count}")
    
    if index_results['errors']:
        print("  éŒ¯èª¤ä¿¡æ¯:")
        for error in index_results['errors'][:5]:  # åªé¡¯ç¤ºå‰5å€‹éŒ¯èª¤
            print(f"    {error}")
    print()
    
    # æ¸¬è©¦äº¤äº’æ•¸æ“šç´¢å¼•
    print("ğŸ’¬ æ¸¬è©¦äº¤äº’æ•¸æ“šç´¢å¼•...")
    test_interaction = {
        "user_input": "å¦‚ä½•ä½¿ç”¨PowerAutomationé€²è¡ŒGAIAæ¸¬è©¦ï¼Ÿ",
        "ai_response": "PowerAutomationæä¾›äº†å®Œæ•´çš„GAIAæ¸¬è©¦æ¡†æ¶ï¼ŒåŒ…æ‹¬MCPé©é…å™¨ã€ç•°æ­¥RLè¨“ç·´ç­‰åŠŸèƒ½ã€‚",
        "context": {"test_type": "gaia", "level": 1},
        "quality_score": 0.9
    }
    
    interaction_result = await kilorag.index_interaction_data(test_interaction)
    print(f"  äº¤äº’æ•¸æ“šç´¢å¼•: {interaction_result['status']}")
    print()
    
    # æ¸¬è©¦çŸ¥è­˜æœç´¢
    print("ğŸ” æ¸¬è©¦çŸ¥è­˜æœç´¢...")
    search_queries = [
        "GAIAæ¸¬è©¦",
        "MCPé©é…å™¨",
        "ç•°æ­¥RLè¨“ç·´",
        "è¨˜æ†¶ç³»çµ±"
    ]
    
    for query in search_queries:
        search_result = await kilorag.search_knowledge(query, limit=3)
        print(f"  æŸ¥è©¢ '{query}': {search_result['total_results']} çµæœ ({search_result.get('query_time', 0):.3f}s)")
    print()
    
    # ç²å–æœ€çµ‚ç‹€æ…‹
    print("ğŸ“Š ç³»çµ±æœ€çµ‚ç‹€æ…‹:")
    final_status = await kilorag.get_system_status()
    print(f"  å·²ç´¢å¼•æ–‡ä»¶: {final_status.get('indexed_files', 0)}")
    print(f"  å·²ç´¢å¼•äº¤äº’: {final_status.get('indexed_interactions', 0)}")
    print(f"  ç¸½æŸ¥è©¢æ¬¡æ•¸: {final_status.get('total_queries', 0)}")
    
    print("\\nâœ… KiloRAGé›†æˆæ¸¬è©¦å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())

