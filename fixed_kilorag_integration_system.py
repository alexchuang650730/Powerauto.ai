#!/usr/bin/env python3
"""
ä¿®å¾©ç‰ˆKiloRAGé›†æˆç³»çµ±
è§£æ±ºRAGé›†æˆå•é¡Œï¼Œå¯¦ç¾å®Œæ•´çš„æ–‡ä»¶å’Œäº¤äº’æ•¸æ“šç´¢å¼•
"""

import os
import sys
import json
import asyncio
import logging
import hashlib
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path
import sqlite3

logger = logging.getLogger(__name__)

class SimpleRAGSystem:
    """ç°¡åŒ–çš„RAGç³»çµ±å¯¦ç¾"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç°¡åŒ–RAGç³»çµ±"""
        self.memories = []
        self.memory_index = {}
        self.next_id = 1
        
    def add_memory(self, content: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """æ·»åŠ è¨˜æ†¶åˆ°RAGç³»çµ±"""
        try:
            memory_id = f"mem_{self.next_id:06d}"
            self.next_id += 1
            
            memory = {
                "id": memory_id,
                "content": content,
                "metadata": metadata,
                "created_at": datetime.now().isoformat()
            }
            
            self.memories.append(memory)
            self.memory_index[memory_id] = len(self.memories) - 1
            
            return {
                "status": "success",
                "memory_id": memory_id
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def search_memories(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """æœç´¢è¨˜æ†¶"""
        try:
            query_lower = query.lower()
            results = []
            
            for memory in self.memories:
                content_lower = memory["content"].lower()
                
                # ç°¡å–®çš„é—œéµè©åŒ¹é…
                if query_lower in content_lower:
                    score = content_lower.count(query_lower) / len(content_lower.split())
                    results.append({
                        "memory_id": memory["id"],
                        "content": memory["content"][:200] + "..." if len(memory["content"]) > 200 else memory["content"],
                        "metadata": memory["metadata"],
                        "score": score
                    })
            
            # æŒ‰åˆ†æ•¸æ’åº
            results.sort(key=lambda x: x["score"], reverse=True)
            
            return {
                "status": "success",
                "results": results[:limit]
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "results": []
            }

class FixedKiloRAGIntegrationSystem:
    """ä¿®å¾©ç‰ˆKiloRAGå®Œæ•´é›†æˆç³»çµ±"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ–KiloRAGé›†æˆç³»çµ±"""
        self.config = config or {}
        self.base_path = self.config.get('base_path', '/home/ubuntu/Powerauto.ai')
        
        # ä½¿ç”¨ç°¡åŒ–RAGç³»çµ±
        self.rag_system = SimpleRAGSystem()
        
        # æ•¸æ“šåº«é€£æ¥
        self.db_path = os.path.join(self.base_path, 'data', 'kilorag.db')
        self._init_database()
        
        # æ”¯æŒçš„æ–‡ä»¶é¡å‹
        self.supported_extensions = {
            '.py', '.js', '.ts', '.java', '.cpp', '.c', '.h',
            '.md', '.txt', '.json', '.yaml', '.yml', '.xml',
            '.html', '.css', '.sql', '.sh', '.bat'
        }
        
        logger.info("ä¿®å¾©ç‰ˆKiloRAGé›†æˆç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•¸æ“šåº«"""
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # æ–‡ä»¶ç´¢å¼•è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS file_index (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_path TEXT UNIQUE NOT NULL,
                    file_hash TEXT NOT NULL,
                    content_preview TEXT,
                    file_type TEXT,
                    size INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    rag_memory_id TEXT,
                    indexed BOOLEAN DEFAULT FALSE
                )
            ''')
            
            # äº¤äº’æ•¸æ“šè¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS interaction_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    interaction_id TEXT UNIQUE NOT NULL,
                    user_input TEXT,
                    ai_response TEXT,
                    context_data TEXT,
                    quality_score REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    rag_memory_id TEXT,
                    indexed BOOLEAN DEFAULT FALSE
                )
            ''')
            
            # RAGæª¢ç´¢è¨˜éŒ„è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS rag_queries (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    query_text TEXT NOT NULL,
                    results_count INTEGER,
                    query_time REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
    
    async def scan_and_index_all_files(self) -> Dict[str, Any]:
        """æƒæä¸¦ç´¢å¼•æ‰€æœ‰é …ç›®æ–‡ä»¶"""
        logger.info("é–‹å§‹æƒæå’Œç´¢å¼•æ‰€æœ‰é …ç›®æ–‡ä»¶...")
        
        results = {
            "total_files": 0,
            "indexed_files": 0,
            "skipped_files": 0,
            "failed_files": 0,
            "file_types": {},
            "errors": []
        }
        
        try:
            # éæ­·é …ç›®ç›®éŒ„
            for root, dirs, files in os.walk(self.base_path):
                # è·³éç‰¹å®šç›®éŒ„
                dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'node_modules', 'data']]
                
                for file in files:
                    file_path = os.path.join(root, file)
                    relative_path = os.path.relpath(file_path, self.base_path)
                    
                    results["total_files"] += 1
                    
                    # æª¢æŸ¥æ–‡ä»¶é¡å‹
                    file_ext = Path(file).suffix.lower()
                    if file_ext not in self.supported_extensions:
                        results["skipped_files"] += 1
                        continue
                    
                    # çµ±è¨ˆæ–‡ä»¶é¡å‹
                    results["file_types"][file_ext] = results["file_types"].get(file_ext, 0) + 1
                    
                    # ç´¢å¼•æ–‡ä»¶
                    try:
                        index_result = await self._index_single_file(file_path, relative_path)
                        if index_result["status"] == "success":
                            results["indexed_files"] += 1
                        elif index_result["status"] == "skipped":
                            results["skipped_files"] += 1
                        else:
                            results["failed_files"] += 1
                            if len(results["errors"]) < 10:  # é™åˆ¶éŒ¯èª¤æ•¸é‡
                                results["errors"].append(f"{relative_path}: {index_result.get('error', 'Unknown error')}")
                    except Exception as e:
                        results["failed_files"] += 1
                        if len(results["errors"]) < 10:
                            results["errors"].append(f"{relative_path}: {str(e)}")
            
            logger.info(f"æ–‡ä»¶ç´¢å¼•å®Œæˆ: {results['indexed_files']}/{results['total_files']} æˆåŠŸ")
            return results
            
        except Exception as e:
            logger.error(f"æƒææ–‡ä»¶å¤±æ•—: {e}")
            results["errors"].append(f"æƒæå¤±æ•—: {str(e)}")
            return results
    
    async def _index_single_file(self, file_path: str, relative_path: str) -> Dict[str, Any]:
        """ç´¢å¼•å–®å€‹æ–‡ä»¶"""
        try:
            # æª¢æŸ¥æ–‡ä»¶å¤§å°ï¼ˆè·³ééå¤§çš„æ–‡ä»¶ï¼‰
            file_size = os.path.getsize(file_path)
            if file_size > 1024 * 1024:  # 1MBé™åˆ¶
                return {"status": "skipped", "reason": "file_too_large"}
            
            # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç´¢å¼•
            file_hash = self._calculate_file_hash(file_path)
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT file_hash, rag_memory_id FROM file_index WHERE file_path = ?",
                    (relative_path,)
                )
                existing = cursor.fetchone()
                
                if existing and existing[0] == file_hash:
                    return {"status": "skipped", "reason": "already_indexed"}
            
            # è®€å–æ–‡ä»¶å…§å®¹
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            except UnicodeDecodeError:
                try:
                    with open(file_path, 'r', encoding='latin-1') as f:
                        content = f.read()
                except Exception:
                    return {"status": "error", "error": "encoding_error"}
            
            # é™åˆ¶å…§å®¹é•·åº¦
            if len(content) > 10000:
                content = content[:10000] + "\\n... (å…§å®¹å·²æˆªæ–·)"
            
            # æº–å‚™RAGè¨˜æ†¶æ•¸æ“š
            file_info = os.stat(file_path)
            metadata = {
                "file_path": relative_path,
                "file_type": Path(file_path).suffix.lower(),
                "file_size": file_info.st_size,
                "created_at": datetime.fromtimestamp(file_info.st_ctime).isoformat(),
                "modified_at": datetime.fromtimestamp(file_info.st_mtime).isoformat(),
                "content_type": "project_file"
            }
            
            # å‰µå»ºå…§å®¹é è¦½
            content_preview = content[:500] + "..." if len(content) > 500 else content
            
            # æ·»åŠ åˆ°RAGç³»çµ±
            rag_result = self.rag_system.add_memory(
                content=content,
                metadata=metadata
            )
            
            if rag_result.get("status") == "success":
                rag_memory_id = rag_result.get("memory_id")
                
                # ä¿å­˜åˆ°æ•¸æ“šåº«
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute('''
                        INSERT OR REPLACE INTO file_index 
                        (file_path, file_hash, content_preview, file_type, size, rag_memory_id, indexed, updated_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                    ''', (
                        relative_path, file_hash, content_preview,
                        metadata["file_type"], metadata["file_size"],
                        rag_memory_id, True
                    ))
                    conn.commit()
                
                return {"status": "success", "rag_memory_id": rag_memory_id}
            else:
                return {"status": "error", "error": "RAGç´¢å¼•å¤±æ•—"}
                
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """è¨ˆç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
        except Exception:
            # å¦‚æœç„¡æ³•è®€å–æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶è·¯å¾‘å’Œä¿®æ”¹æ™‚é–“
            stat = os.stat(file_path)
            hash_md5.update(f"{file_path}_{stat.st_mtime}".encode())
        
        return hash_md5.hexdigest()
    
    async def index_interaction_data(self, interaction_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç´¢å¼•äº¤äº’æ•¸æ“š"""
        try:
            interaction_id = interaction_data.get("interaction_id") or self._generate_interaction_id(interaction_data)
            
            # æª¢æŸ¥æ˜¯å¦å·²ç´¢å¼•
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT rag_memory_id FROM interaction_data WHERE interaction_id = ?",
                    (interaction_id,)
                )
                existing = cursor.fetchone()
                
                if existing:
                    return {"status": "skipped", "reason": "already_indexed"}
            
            # æº–å‚™RAGè¨˜æ†¶æ•¸æ“š
            user_input = interaction_data.get("user_input", "")
            ai_response = interaction_data.get("ai_response", "")
            context_data = interaction_data.get("context", {})
            
            # çµ„åˆå…§å®¹ç”¨æ–¼RAGæª¢ç´¢
            combined_content = f"""
ç”¨æˆ¶è¼¸å…¥: {user_input}
AIå›æ‡‰: {ai_response}
ä¸Šä¸‹æ–‡: {json.dumps(context_data, ensure_ascii=False, indent=2)}
            """.strip()
            
            metadata = {
                "interaction_id": interaction_id,
                "user_input": user_input,
                "ai_response": ai_response,
                "quality_score": interaction_data.get("quality_score", 0.5),
                "timestamp": interaction_data.get("timestamp", datetime.now().isoformat()),
                "content_type": "interaction_data"
            }
            
            # æ·»åŠ åˆ°RAGç³»çµ±
            rag_result = self.rag_system.add_memory(
                content=combined_content,
                metadata=metadata
            )
            
            if rag_result.get("status") == "success":
                rag_memory_id = rag_result.get("memory_id")
                
                # ä¿å­˜åˆ°æ•¸æ“šåº«
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute('''
                        INSERT INTO interaction_data 
                        (interaction_id, user_input, ai_response, context_data, quality_score, rag_memory_id, indexed)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        interaction_id, user_input, ai_response,
                        json.dumps(context_data), metadata["quality_score"],
                        rag_memory_id, True
                    ))
                    conn.commit()
                
                return {"status": "success", "rag_memory_id": rag_memory_id}
            else:
                return {"status": "error", "error": "RAGç´¢å¼•å¤±æ•—"}
                
        except Exception as e:
            logger.error(f"ç´¢å¼•äº¤äº’æ•¸æ“šå¤±æ•—: {e}")
            return {"status": "error", "error": str(e)}
    
    def _generate_interaction_id(self, interaction_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆäº¤äº’ID"""
        content = f"{interaction_data.get('user_input', '')}{interaction_data.get('ai_response', '')}"
        return hashlib.md5(content.encode()).hexdigest()
    
    async def search_knowledge(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """æœç´¢çŸ¥è­˜åº«"""
        try:
            start_time = datetime.now()
            
            # ä½¿ç”¨RAGç³»çµ±æœç´¢
            search_results = self.rag_system.search_memories(
                query=query,
                limit=limit
            )
            
            end_time = datetime.now()
            query_time = (end_time - start_time).total_seconds()
            
            results = search_results.get("results", [])
            
            # è¨˜éŒ„æŸ¥è©¢
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO rag_queries (query_text, results_count, query_time)
                    VALUES (?, ?, ?)
                ''', (query, len(results), query_time))
                conn.commit()
            
            return {
                "status": "success",
                "query": query,
                "results": results,
                "total_results": len(results),
                "query_time": query_time
            }
            
        except Exception as e:
            logger.error(f"æœç´¢çŸ¥è­˜åº«å¤±æ•—: {e}")
            return {
                "status": "error",
                "error": str(e),
                "results": [],
                "total_results": 0,
                "query_time": 0
            }
    
    async def get_system_status(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±ç‹€æ…‹"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # çµ±è¨ˆæ–‡ä»¶ç´¢å¼•
                cursor.execute("SELECT COUNT(*) FROM file_index WHERE indexed = TRUE")
                indexed_files_count = cursor.fetchone()[0]
                
                cursor.execute("SELECT COUNT(*) FROM file_index")
                total_files_count = cursor.fetchone()[0]
                
                # çµ±è¨ˆäº¤äº’æ•¸æ“š
                cursor.execute("SELECT COUNT(*) FROM interaction_data WHERE indexed = TRUE")
                indexed_interactions_count = cursor.fetchone()[0]
                
                # çµ±è¨ˆæŸ¥è©¢
                cursor.execute("SELECT COUNT(*) FROM rag_queries")
                total_queries_count = cursor.fetchone()[0]
                
                # æ–‡ä»¶é¡å‹çµ±è¨ˆ
                cursor.execute("SELECT file_type, COUNT(*) FROM file_index GROUP BY file_type")
                file_types = dict(cursor.fetchall())
                
                return {
                    "status": "active",
                    "indexed_files": indexed_files_count,
                    "total_files": total_files_count,
                    "indexed_interactions": indexed_interactions_count,
                    "total_queries": total_queries_count,
                    "file_types": file_types,
                    "database_path": self.db_path,
                    "rag_memories_count": len(self.rag_system.memories),
                    "last_updated": datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"ç²å–ç³»çµ±ç‹€æ…‹å¤±æ•—: {e}")
            return {
                "status": "error",
                "error": str(e)
            }


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ ä¿®å¾©ç‰ˆKiloRAGå®Œæ•´é›†æˆç³»çµ±æ¸¬è©¦\\n")
    
    # åˆå§‹åŒ–ç³»çµ±
    kilorag = FixedKiloRAGIntegrationSystem()
    
    # ç²å–åˆå§‹ç‹€æ…‹
    print("ğŸ“Š ç³»çµ±åˆå§‹ç‹€æ…‹:")
    status = await kilorag.get_system_status()
    print(f"  å·²ç´¢å¼•æ–‡ä»¶: {status.get('indexed_files', 0)}")
    print(f"  å·²ç´¢å¼•äº¤äº’: {status.get('indexed_interactions', 0)}")
    print(f"  ç¸½æŸ¥è©¢æ¬¡æ•¸: {status.get('total_queries', 0)}")
    print(f"  RAGè¨˜æ†¶æ•¸é‡: {status.get('rag_memories_count', 0)}")
    print()
    
    # æƒæå’Œç´¢å¼•æ–‡ä»¶ï¼ˆé™åˆ¶æ•¸é‡ä»¥é¿å…éé•·ï¼‰
    print("ğŸ“ é–‹å§‹æƒæå’Œç´¢å¼•é …ç›®æ–‡ä»¶...")
    index_results = await kilorag.scan_and_index_all_files()
    
    print(f"  ç¸½æ–‡ä»¶æ•¸: {index_results['total_files']}")
    print(f"  æˆåŠŸç´¢å¼•: {index_results['indexed_files']}")
    print(f"  è·³éæ–‡ä»¶: {index_results['skipped_files']}")
    print(f"  å¤±æ•—æ–‡ä»¶: {index_results['failed_files']}")
    
    if index_results['file_types']:
        print("  æ–‡ä»¶é¡å‹åˆ†ä½ˆ:")
        for ext, count in sorted(index_results['file_types'].items()):
            print(f"    {ext}: {count}")
    
    if index_results['errors']:
        print("  éƒ¨åˆ†éŒ¯èª¤ä¿¡æ¯:")
        for error in index_results['errors'][:3]:  # åªé¡¯ç¤ºå‰3å€‹éŒ¯èª¤
            print(f"    {error}")
    print()
    
    # æ¸¬è©¦äº¤äº’æ•¸æ“šç´¢å¼•
    print("ğŸ’¬ æ¸¬è©¦äº¤äº’æ•¸æ“šç´¢å¼•...")
    test_interactions = [
        {
            "user_input": "å¦‚ä½•ä½¿ç”¨PowerAutomationé€²è¡ŒGAIAæ¸¬è©¦ï¼Ÿ",
            "ai_response": "PowerAutomationæä¾›äº†å®Œæ•´çš„GAIAæ¸¬è©¦æ¡†æ¶ï¼ŒåŒ…æ‹¬MCPé©é…å™¨ã€ç•°æ­¥RLè¨“ç·´ç­‰åŠŸèƒ½ã€‚",
            "context": {"test_type": "gaia", "level": 1},
            "quality_score": 0.9
        },
        {
            "user_input": "RL_SRTå¦‚ä½•èˆ‡è¨˜æ†¶ç³»çµ±é›†æˆï¼Ÿ",
            "ai_response": "RL_SRTé€šéCloudEdgeDataMCPå¾çµ±ä¸€è¨˜æ†¶æ¨¡å¡Šç²å–äº¤äº’æ•¸æ“šé€²è¡Œè¨“ç·´ã€‚",
            "context": {"topic": "rl_memory_integration"},
            "quality_score": 0.8
        }
    ]
    
    for i, interaction in enumerate(test_interactions):
        result = await kilorag.index_interaction_data(interaction)
        print(f"  äº¤äº’æ•¸æ“š {i+1} ç´¢å¼•: {result['status']}")
    print()
    
    # æ¸¬è©¦çŸ¥è­˜æœç´¢
    print("ğŸ” æ¸¬è©¦çŸ¥è­˜æœç´¢...")
    search_queries = [
        "GAIAæ¸¬è©¦",
        "MCPé©é…å™¨", 
        "ç•°æ­¥RLè¨“ç·´",
        "è¨˜æ†¶ç³»çµ±",
        "PowerAutomation"
    ]
    
    for query in search_queries:
        search_result = await kilorag.search_knowledge(query, limit=3)
        print(f"  æŸ¥è©¢ '{query}': {search_result['total_results']} çµæœ ({search_result.get('query_time', 0):.3f}s)")
        
        # é¡¯ç¤ºç¬¬ä¸€å€‹çµæœçš„é è¦½
        if search_result['results']:
            first_result = search_result['results'][0]
            preview = first_result['content'][:100] + "..." if len(first_result['content']) > 100 else first_result['content']
            print(f"    é è¦½: {preview}")
    print()
    
    # ç²å–æœ€çµ‚ç‹€æ…‹
    print("ğŸ“Š ç³»çµ±æœ€çµ‚ç‹€æ…‹:")
    final_status = await kilorag.get_system_status()
    print(f"  å·²ç´¢å¼•æ–‡ä»¶: {final_status.get('indexed_files', 0)}")
    print(f"  å·²ç´¢å¼•äº¤äº’: {final_status.get('indexed_interactions', 0)}")
    print(f"  ç¸½æŸ¥è©¢æ¬¡æ•¸: {final_status.get('total_queries', 0)}")
    print(f"  RAGè¨˜æ†¶æ•¸é‡: {final_status.get('rag_memories_count', 0)}")
    
    print("\\nâœ… KiloRAGé›†æˆæ¸¬è©¦å®Œæˆï¼")
    print(f"ğŸ“„ æ•¸æ“šåº«ä½ç½®: {final_status.get('database_path')}")

if __name__ == "__main__":
    asyncio.run(main())

