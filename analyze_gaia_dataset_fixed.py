#!/usr/bin/env python3
"""
çµ±è¨ˆGAIAæ•¸æ“šé›†çš„å®Œæ•´åˆ†å¸ƒ - ä¿®æ­£ç‰ˆ
"""

from datasets import load_dataset
import json

def analyze_gaia_dataset():
    print("ğŸ” æ­£åœ¨åˆ†æGAIAæ•¸æ“šé›†çš„å®Œæ•´åˆ†å¸ƒ...")
    
    try:
        # åŠ è¼‰å®Œæ•´æ•¸æ“šé›†
        dataset = load_dataset('gaia-benchmark/GAIA', '2023_all', trust_remote_code=True)
        
        # æª¢æŸ¥å¯ç”¨çš„åˆ†å‰²
        available_splits = list(dataset.keys())
        print(f"ğŸ“Š å¯ç”¨çš„æ•¸æ“šåˆ†å‰²: {available_splits}")
        
        total_questions = 0
        for split in available_splits:
            split_size = len(dataset[split])
            total_questions += split_size
            print(f"   - {split}: {split_size} å€‹å•é¡Œ")
        
        print(f"   - ç¸½è¨ˆ: {total_questions} å€‹å•é¡Œ")
        
        # çµ±è¨ˆå„å€‹åˆ†å‰²çš„Levelåˆ†å¸ƒ
        total_stats = {}
        
        for split in available_splits:
            print(f"\nğŸ“ˆ {split.upper()}é›†Levelåˆ†å¸ƒ:")
            level_count = {}
            
            for item in dataset[split]:
                level = item.get('Level', 'Unknown')
                level_key = f'Level {level}'
                level_count[level_key] = level_count.get(level_key, 0) + 1
                total_stats[level_key] = total_stats.get(level_key, 0) + 1
            
            for level in sorted(level_count.keys()):
                count = level_count[level]
                percentage = (count / len(dataset[split])) * 100
                print(f"   - {level}: {count} å€‹å•é¡Œ ({percentage:.1f}%)")
        
        print(f"\nğŸ¯ ç¸½é«”Levelåˆ†å¸ƒ:")
        for level in sorted(total_stats.keys()):
            count = total_stats[level]
            percentage = (count / total_questions) * 100
            print(f"   - {level}: {count} å€‹å•é¡Œ ({percentage:.1f}%)")
        
        # æª¢æŸ¥æ–‡ä»¶é™„ä»¶æƒ…æ³
        print(f"\nğŸ“ æ–‡ä»¶é™„ä»¶çµ±è¨ˆ:")
        for split in available_splits:
            with_files = 0
            without_files = 0
            
            for item in dataset[split]:
                if item.get('file_name'):
                    with_files += 1
                else:
                    without_files += 1
            
            total = len(dataset[split])
            print(f"   - {split.upper()}é›†: {with_files} å€‹æœ‰é™„ä»¶ ({(with_files/total)*100:.1f}%), {without_files} å€‹ç„¡é™„ä»¶ ({(without_files/total)*100:.1f}%)")
        
        # æª¢æŸ¥å•é¡Œé•·åº¦åˆ†å¸ƒ
        print(f"\nğŸ“ å•é¡Œé•·åº¦çµ±è¨ˆ:")
        for split in available_splits:
            lengths = [len(item['Question']) for item in dataset[split]]
            avg_length = sum(lengths) / len(lengths)
            min_length = min(lengths)
            max_length = max(lengths)
            print(f"   - {split.upper()}é›†: å¹³å‡ {avg_length:.0f} å­—ç¬¦, æœ€çŸ­ {min_length}, æœ€é•· {max_length}")
        
        # æª¢æŸ¥ç¬¬ä¸€å€‹å•é¡Œçš„çµæ§‹
        print(f"\nğŸ” æ•¸æ“šçµæ§‹ç¤ºä¾‹ (ç¬¬ä¸€å€‹å•é¡Œ):")
        first_item = dataset[available_splits[0]][0]
        print(f"   - å­—æ®µ: {list(first_item.keys())}")
        print(f"   - å•é¡ŒID: {first_item.get('task_id', 'N/A')}")
        print(f"   - Level: {first_item.get('Level', 'N/A')}")
        print(f"   - å•é¡Œé•·åº¦: {len(first_item.get('Question', ''))} å­—ç¬¦")
        print(f"   - æœ‰é™„ä»¶: {'æ˜¯' if first_item.get('file_name') else 'å¦'}")
        
        # ä¿å­˜è©³ç´°çµ±è¨ˆ
        detailed_stats = {
            'dataset_overview': {
                'available_splits': available_splits,
                'total_size': total_questions
            },
            'split_sizes': {},
            'level_distribution': {},
            'file_attachment_stats': {},
            'question_length_stats': {}
        }
        
        # å„åˆ†å‰²å¤§å°
        for split in available_splits:
            detailed_stats['split_sizes'][split] = len(dataset[split])
        
        # è©³ç´°Levelåˆ†å¸ƒ
        for split in available_splits:
            level_count = {}
            for item in dataset[split]:
                level = item.get('Level', 'Unknown')
                level_key = f'Level {level}'
                level_count[level_key] = level_count.get(level_key, 0) + 1
            detailed_stats['level_distribution'][split] = level_count
        
        # æ–‡ä»¶é™„ä»¶çµ±è¨ˆ
        for split in available_splits:
            with_files = sum(1 for item in dataset[split] if item.get('file_name'))
            without_files = len(dataset[split]) - with_files
            detailed_stats['file_attachment_stats'][split] = {
                'with_files': with_files,
                'without_files': without_files,
                'total': len(dataset[split])
            }
        
        # å•é¡Œé•·åº¦çµ±è¨ˆ
        for split in available_splits:
            lengths = [len(item['Question']) for item in dataset[split]]
            detailed_stats['question_length_stats'][split] = {
                'average': sum(lengths) / len(lengths),
                'min': min(lengths),
                'max': max(lengths)
            }
        
        # ä¿å­˜çµ±è¨ˆçµæœ
        with open('gaia_dataset_statistics.json', 'w', encoding='utf-8') as f:
            json.dump(detailed_stats, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è©³ç´°çµ±è¨ˆå·²ä¿å­˜åˆ°: gaia_dataset_statistics.json")
        
        return detailed_stats
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    stats = analyze_gaia_dataset()

