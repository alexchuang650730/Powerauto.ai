#!/usr/bin/env python3
"""
PowerAutomation v0.53 çµ±ä¸€æ¶æ§‹æ•´åˆæ¸¬è©¦

æ¸¬è©¦çµ±ä¸€æ¶æ§‹çš„æ•´åˆæ•ˆæœï¼ŒåŒ…æ‹¬ï¼š
1. åŠŸèƒ½å®Œæ•´æ€§æ¸¬è©¦
2. æ€§èƒ½å°æ¯”æ¸¬è©¦
3. æ•¸æ“šä¸€è‡´æ€§æ¸¬è©¦
4. çµ„ä»¶å”èª¿æ¸¬è©¦
"""

import os
import json
import time
import asyncio
import threading
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
import unittest
from unittest.mock import Mock, patch

# å°å…¥çµ±ä¸€æ¶æ§‹
from unified_architecture import (
    UnifiedArchitectureCoordinator,
    InteractionSource,
    DeliverableType,
    DataLayer,
    initialize_unified_architecture
)

class UnifiedArchitectureIntegrationTest:
    """çµ±ä¸€æ¶æ§‹æ•´åˆæ¸¬è©¦"""
    
    def __init__(self):
        self.test_results = {
            'functionality_tests': {},
            'performance_tests': {},
            'integration_tests': {},
            'data_consistency_tests': {},
            'overall_score': 0.0
        }
        self.logger = logging.getLogger(__name__)
        
        # æ¸¬è©¦é…ç½®
        self.test_config = {
            'base_dir': '/home/ubuntu/Powerauto.ai/test_unified',
            'test_data_count': 10,
            'performance_iterations': 5,
            'timeout_seconds': 30
        }
        
        # åˆå§‹åŒ–æ¸¬è©¦ç’°å¢ƒ
        self.setup_test_environment()
    
    def setup_test_environment(self):
        """è¨­ç½®æ¸¬è©¦ç’°å¢ƒ"""
        test_dir = Path(self.test_config['base_dir'])
        test_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¸…ç†ä¹‹å‰çš„æ¸¬è©¦æ•¸æ“š
        for subdir in ['unified_data', 'test_results']:
            if (test_dir / subdir).exists():
                import shutil
                shutil.rmtree(test_dir / subdir)
        
        self.logger.info(f"âœ… æ¸¬è©¦ç’°å¢ƒå·²è¨­ç½®: {test_dir}")
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """é‹è¡Œæ‰€æœ‰æ¸¬è©¦"""
        print("ğŸš€ é–‹å§‹çµ±ä¸€æ¶æ§‹æ•´åˆæ¸¬è©¦")
        
        # 1. åŠŸèƒ½å®Œæ•´æ€§æ¸¬è©¦
        print("\nğŸ“‹ 1. åŠŸèƒ½å®Œæ•´æ€§æ¸¬è©¦")
        functionality_results = await self.test_functionality()
        self.test_results['functionality_tests'] = functionality_results
        
        # 2. æ€§èƒ½å°æ¯”æ¸¬è©¦
        print("\nâš¡ 2. æ€§èƒ½å°æ¯”æ¸¬è©¦")
        performance_results = await self.test_performance()
        self.test_results['performance_tests'] = performance_results
        
        # 3. æ•´åˆæ•ˆæœæ¸¬è©¦
        print("\nğŸ”„ 3. æ•´åˆæ•ˆæœæ¸¬è©¦")
        integration_results = await self.test_integration()
        self.test_results['integration_tests'] = integration_results
        
        # 4. æ•¸æ“šä¸€è‡´æ€§æ¸¬è©¦
        print("\nğŸ“Š 4. æ•¸æ“šä¸€è‡´æ€§æ¸¬è©¦")
        consistency_results = await self.test_data_consistency()
        self.test_results['data_consistency_tests'] = consistency_results
        
        # 5. è¨ˆç®—ç¸½é«”è©•åˆ†
        overall_score = self.calculate_overall_score()
        self.test_results['overall_score'] = overall_score
        
        # 6. ç”Ÿæˆæ¸¬è©¦å ±å‘Š
        await self.generate_test_report()
        
        print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼Œç¸½é«”è©•åˆ†: {overall_score:.2f}/100")
        return self.test_results
    
    async def test_functionality(self) -> Dict[str, Any]:
        """åŠŸèƒ½å®Œæ•´æ€§æ¸¬è©¦"""
        results = {
            'data_collection': {'passed': False, 'score': 0, 'details': ''},
            'data_storage': {'passed': False, 'score': 0, 'details': ''},
            'learning_generation': {'passed': False, 'score': 0, 'details': ''},
            'template_generation': {'passed': False, 'score': 0, 'details': ''},
            'component_coordination': {'passed': False, 'score': 0, 'details': ''}
        }
        
        try:
            # åˆå§‹åŒ–å”èª¿å™¨
            config = {
                'base_dir': self.test_config['base_dir'],
                'collector': {'batch_size': 5, 'log_level': 'INFO'},
                'data_manager': {'local_storage': True}
            }
            coordinator = UnifiedArchitectureCoordinator(config)
            await coordinator.start_processing()
            
            # æ¸¬è©¦æ•¸æ“šæ”¶é›†
            print("  ğŸ“¥ æ¸¬è©¦æ•¸æ“šæ”¶é›†åŠŸèƒ½...")
            test_interaction = {
                'user_request': {'text': 'æ¸¬è©¦åŠŸèƒ½', 'type': 'test'},
                'agent_response': {'text': 'æ¸¬è©¦éŸ¿æ‡‰'},
                'deliverables': [{
                    'name': 'test.py',
                    'content': 'def test(): return "hello"',
                    'metadata': {'test': True}
                }],
                'context': {'test_mode': True}
            }
            
            result = await coordinator.process_interaction(
                InteractionSource.SYSTEM_INTERNAL, test_interaction
            )
            
            if result['success']:
                results['data_collection']['passed'] = True
                results['data_collection']['score'] = 20
                results['data_collection']['details'] = 'æ•¸æ“šæ”¶é›†åŠŸèƒ½æ­£å¸¸'
                print("    âœ… æ•¸æ“šæ”¶é›†æ¸¬è©¦é€šé")
            else:
                results['data_collection']['details'] = f"æ•¸æ“šæ”¶é›†å¤±æ•—: {result.get('error', 'Unknown')}"
                print("    âŒ æ•¸æ“šæ”¶é›†æ¸¬è©¦å¤±æ•—")
            
            # æ¸¬è©¦æ•¸æ“šå­˜å„²
            print("  ğŸ’¾ æ¸¬è©¦æ•¸æ“šå­˜å„²åŠŸèƒ½...")
            if result['success']:
                storage_path = result['storage_path']
                if os.path.exists(storage_path):
                    results['data_storage']['passed'] = True
                    results['data_storage']['score'] = 20
                    results['data_storage']['details'] = 'æ•¸æ“šå­˜å„²åŠŸèƒ½æ­£å¸¸'
                    print("    âœ… æ•¸æ“šå­˜å„²æ¸¬è©¦é€šé")
                else:
                    results['data_storage']['details'] = 'å­˜å„²æ–‡ä»¶ä¸å­˜åœ¨'
                    print("    âŒ æ•¸æ“šå­˜å„²æ¸¬è©¦å¤±æ•—")
            
            # æ¸¬è©¦å­¸ç¿’ç¶“é©—ç”Ÿæˆ
            print("  ğŸ§  æ¸¬è©¦å­¸ç¿’ç¶“é©—ç”Ÿæˆ...")
            if result['success'] and result['learning_experiences_generated'] > 0:
                results['learning_generation']['passed'] = True
                results['learning_generation']['score'] = 20
                results['learning_generation']['details'] = 'å­¸ç¿’ç¶“é©—ç”Ÿæˆæ­£å¸¸'
                print("    âœ… å­¸ç¿’ç¶“é©—ç”Ÿæˆæ¸¬è©¦é€šé")
            else:
                results['learning_generation']['details'] = 'æœªç”Ÿæˆå­¸ç¿’ç¶“é©—'
                print("    âŒ å­¸ç¿’ç¶“é©—ç”Ÿæˆæ¸¬è©¦å¤±æ•—")
            
            # æ¸¬è©¦æ¨¡æ¿ç”Ÿæˆï¼ˆéœ€è¦é«˜è³ªé‡äº¤ä»˜ä»¶ï¼‰
            print("  ğŸ”§ æ¸¬è©¦æ¨¡æ¿ç”ŸæˆåŠŸèƒ½...")
            high_quality_interaction = {
                'user_request': {'text': 'å‰µå»ºé«˜è³ªé‡ä»£ç¢¼', 'type': 'code_generation'},
                'agent_response': {'text': 'å‰µå»ºé«˜è³ªé‡Pythoné¡'},
                'deliverables': [{
                    'name': 'high_quality.py',
                    'content': '''
class HighQualityClass:
    """é«˜è³ªé‡çš„Pythoné¡ç¤ºä¾‹"""
    
    def __init__(self, name: str):
        self.name = name
    
    def process(self, data: dict) -> dict:
        """è™•ç†æ•¸æ“šçš„æ–¹æ³•"""
        try:
            result = {"processed": True, "data": data}
            return result
        except Exception as e:
            return {"error": str(e)}
    
    def __str__(self) -> str:
        return f"HighQualityClass({self.name})"
                    ''',
                    'metadata': {'lines': 20, 'functions': 3, 'classes': 1}
                }],
                'context': {'quality_focus': True}
            }
            
            template_result = await coordinator.process_interaction(
                InteractionSource.SYSTEM_INTERNAL, high_quality_interaction
            )
            
            if template_result['success'] and template_result['templates_generated'] > 0:
                results['template_generation']['passed'] = True
                results['template_generation']['score'] = 20
                results['template_generation']['details'] = 'æ¨¡æ¿ç”ŸæˆåŠŸèƒ½æ­£å¸¸'
                print("    âœ… æ¨¡æ¿ç”Ÿæˆæ¸¬è©¦é€šé")
            else:
                results['template_generation']['details'] = 'æœªç”Ÿæˆæ¨¡æ¿'
                print("    âŒ æ¨¡æ¿ç”Ÿæˆæ¸¬è©¦å¤±æ•—")
            
            # æ¸¬è©¦çµ„ä»¶å”èª¿
            print("  ğŸ”„ æ¸¬è©¦çµ„ä»¶å”èª¿åŠŸèƒ½...")
            status = await coordinator.get_system_status()
            if (status['coordinator_status']['initialized'] and 
                status['coordinator_status']['processed_interactions'] > 0):
                results['component_coordination']['passed'] = True
                results['component_coordination']['score'] = 20
                results['component_coordination']['details'] = 'çµ„ä»¶å”èª¿åŠŸèƒ½æ­£å¸¸'
                print("    âœ… çµ„ä»¶å”èª¿æ¸¬è©¦é€šé")
            else:
                results['component_coordination']['details'] = 'çµ„ä»¶å”èª¿ç•°å¸¸'
                print("    âŒ çµ„ä»¶å”èª¿æ¸¬è©¦å¤±æ•—")
            
            await coordinator.stop_processing()
            
        except Exception as e:
            error_msg = f"åŠŸèƒ½æ¸¬è©¦ç•°å¸¸: {e}"
            print(f"    âŒ {error_msg}")
            for test_name in results:
                if not results[test_name]['passed']:
                    results[test_name]['details'] = error_msg
        
        return results
    
    async def test_performance(self) -> Dict[str, Any]:
        """æ€§èƒ½å°æ¯”æ¸¬è©¦"""
        results = {
            'processing_speed': {'score': 0, 'details': ''},
            'memory_usage': {'score': 0, 'details': ''},
            'concurrent_processing': {'score': 0, 'details': ''},
            'scalability': {'score': 0, 'details': ''}
        }
        
        try:
            # åˆå§‹åŒ–å”èª¿å™¨
            config = {'base_dir': self.test_config['base_dir']}
            coordinator = UnifiedArchitectureCoordinator(config)
            await coordinator.start_processing()
            
            # æ¸¬è©¦è™•ç†é€Ÿåº¦
            print("  âš¡ æ¸¬è©¦è™•ç†é€Ÿåº¦...")
            start_time = time.time()
            
            for i in range(self.test_config['performance_iterations']):
                test_interaction = {
                    'user_request': {'text': f'æ€§èƒ½æ¸¬è©¦ {i}', 'type': 'performance_test'},
                    'agent_response': {'text': f'éŸ¿æ‡‰ {i}'},
                    'deliverables': [{
                        'name': f'perf_test_{i}.py',
                        'content': f'# Performance test {i}\nprint("test {i}")',
                        'metadata': {'iteration': i}
                    }]
                }
                
                result = await coordinator.process_interaction(
                    InteractionSource.SYSTEM_INTERNAL, test_interaction
                )
                
                if not result['success']:
                    print(f"    âš ï¸ ç¬¬{i+1}æ¬¡è™•ç†å¤±æ•—")
            
            end_time = time.time()
            total_time = end_time - start_time
            avg_time = total_time / self.test_config['performance_iterations']
            
            # è©•åˆ†ï¼šå¹³å‡è™•ç†æ™‚é–“ < 1ç§’å¾—æ»¿åˆ†
            if avg_time < 1.0:
                speed_score = 25
            elif avg_time < 2.0:
                speed_score = 20
            elif avg_time < 5.0:
                speed_score = 15
            else:
                speed_score = 10
            
            results['processing_speed']['score'] = speed_score
            results['processing_speed']['details'] = f'å¹³å‡è™•ç†æ™‚é–“: {avg_time:.3f}ç§’'
            print(f"    âœ… å¹³å‡è™•ç†æ™‚é–“: {avg_time:.3f}ç§’ (è©•åˆ†: {speed_score}/25)")
            
            # æ¸¬è©¦å…§å­˜ä½¿ç”¨ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            print("  ğŸ’¾ æ¸¬è©¦å…§å­˜ä½¿ç”¨...")
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            
            # è©•åˆ†ï¼šå…§å­˜ä½¿ç”¨ < 100MBå¾—æ»¿åˆ†
            if memory_mb < 100:
                memory_score = 25
            elif memory_mb < 200:
                memory_score = 20
            elif memory_mb < 500:
                memory_score = 15
            else:
                memory_score = 10
            
            results['memory_usage']['score'] = memory_score
            results['memory_usage']['details'] = f'å…§å­˜ä½¿ç”¨: {memory_mb:.1f}MB'
            print(f"    âœ… å…§å­˜ä½¿ç”¨: {memory_mb:.1f}MB (è©•åˆ†: {memory_score}/25)")
            
            # æ¸¬è©¦ä¸¦ç™¼è™•ç†ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            print("  ğŸ”„ æ¸¬è©¦ä¸¦ç™¼è™•ç†...")
            concurrent_start = time.time()
            
            # å‰µå»ºä¸¦ç™¼ä»»å‹™
            tasks = []
            for i in range(3):  # 3å€‹ä¸¦ç™¼ä»»å‹™
                task_interaction = {
                    'user_request': {'text': f'ä¸¦ç™¼æ¸¬è©¦ {i}', 'type': 'concurrent_test'},
                    'agent_response': {'text': f'ä¸¦ç™¼éŸ¿æ‡‰ {i}'},
                    'deliverables': [{
                        'name': f'concurrent_{i}.py',
                        'content': f'# Concurrent test {i}',
                        'metadata': {'concurrent': True}
                    }]
                }
                
                task = coordinator.process_interaction(
                    InteractionSource.SYSTEM_INTERNAL, task_interaction
                )
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
            concurrent_results = await asyncio.gather(*tasks, return_exceptions=True)
            concurrent_end = time.time()
            concurrent_time = concurrent_end - concurrent_start
            
            successful_tasks = sum(1 for r in concurrent_results 
                                 if isinstance(r, dict) and r.get('success', False))
            
            # è©•åˆ†ï¼šæˆåŠŸç‡å’Œæ™‚é–“
            if successful_tasks == 3 and concurrent_time < 3.0:
                concurrent_score = 25
            elif successful_tasks >= 2 and concurrent_time < 5.0:
                concurrent_score = 20
            elif successful_tasks >= 1:
                concurrent_score = 15
            else:
                concurrent_score = 10
            
            results['concurrent_processing']['score'] = concurrent_score
            results['concurrent_processing']['details'] = f'ä¸¦ç™¼æˆåŠŸç‡: {successful_tasks}/3, æ™‚é–“: {concurrent_time:.3f}ç§’'
            print(f"    âœ… ä¸¦ç™¼è™•ç†: {successful_tasks}/3æˆåŠŸ, {concurrent_time:.3f}ç§’ (è©•åˆ†: {concurrent_score}/25)")
            
            # æ¸¬è©¦å¯æ“´å±•æ€§ï¼ˆæ•¸æ“šé‡æ¸¬è©¦ï¼‰
            print("  ğŸ“ˆ æ¸¬è©¦å¯æ“´å±•æ€§...")
            scalability_start = time.time()
            
            # è™•ç†å¤§é‡å°ä»»å‹™
            for i in range(20):
                simple_interaction = {
                    'user_request': {'text': f'ç°¡å–®ä»»å‹™ {i}'},
                    'agent_response': {'text': f'ç°¡å–®éŸ¿æ‡‰ {i}'},
                    'context': {'batch_test': True}
                }
                
                await coordinator.process_interaction(
                    InteractionSource.SYSTEM_INTERNAL, simple_interaction
                )
            
            scalability_end = time.time()
            scalability_time = scalability_end - scalability_start
            
            # è©•åˆ†ï¼šè™•ç†20å€‹ä»»å‹™çš„æ™‚é–“
            if scalability_time < 10.0:
                scalability_score = 25
            elif scalability_time < 20.0:
                scalability_score = 20
            elif scalability_time < 30.0:
                scalability_score = 15
            else:
                scalability_score = 10
            
            results['scalability']['score'] = scalability_score
            results['scalability']['details'] = f'è™•ç†20å€‹ä»»å‹™æ™‚é–“: {scalability_time:.3f}ç§’'
            print(f"    âœ… å¯æ“´å±•æ€§: 20å€‹ä»»å‹™ {scalability_time:.3f}ç§’ (è©•åˆ†: {scalability_score}/25)")
            
            await coordinator.stop_processing()
            
        except Exception as e:
            error_msg = f"æ€§èƒ½æ¸¬è©¦ç•°å¸¸: {e}"
            print(f"    âŒ {error_msg}")
            for test_name in results:
                results[test_name]['details'] = error_msg
        
        return results
    
    async def test_integration(self) -> Dict[str, Any]:
        """æ•´åˆæ•ˆæœæ¸¬è©¦"""
        results = {
            'component_communication': {'score': 0, 'details': ''},
            'data_flow_integrity': {'score': 0, 'details': ''},
            'error_handling': {'score': 0, 'details': ''},
            'configuration_management': {'score': 0, 'details': ''}
        }
        
        try:
            # æ¸¬è©¦çµ„ä»¶é–“é€šä¿¡
            print("  ğŸ“¡ æ¸¬è©¦çµ„ä»¶é–“é€šä¿¡...")
            coordinator = UnifiedArchitectureCoordinator({'base_dir': self.test_config['base_dir']})
            await coordinator.start_processing()
            
            # æ¸¬è©¦æ”¶é›†å™¨å’Œæ•¸æ“šç®¡ç†å™¨çš„é€šä¿¡
            collector_stats = await coordinator.interaction_collector.get_statistics()
            data_manager_config = coordinator.data_manager.storage_config
            
            if collector_stats and data_manager_config:
                results['component_communication']['score'] = 25
                results['component_communication']['details'] = 'çµ„ä»¶é–“é€šä¿¡æ­£å¸¸'
                print("    âœ… çµ„ä»¶é–“é€šä¿¡æ¸¬è©¦é€šé")
            else:
                results['component_communication']['details'] = 'çµ„ä»¶é–“é€šä¿¡ç•°å¸¸'
                print("    âŒ çµ„ä»¶é–“é€šä¿¡æ¸¬è©¦å¤±æ•—")
            
            # æ¸¬è©¦æ•¸æ“šæµå®Œæ•´æ€§
            print("  ğŸ”„ æ¸¬è©¦æ•¸æ“šæµå®Œæ•´æ€§...")
            test_interaction = {
                'user_request': {'text': 'æ•¸æ“šæµæ¸¬è©¦', 'type': 'data_flow_test'},
                'agent_response': {'text': 'æ•¸æ“šæµéŸ¿æ‡‰'},
                'deliverables': [{
                    'name': 'data_flow_test.py',
                    'content': 'def data_flow_test(): pass',
                    'metadata': {'test_type': 'data_flow'}
                }]
            }
            
            result = await coordinator.process_interaction(
                InteractionSource.SYSTEM_INTERNAL, test_interaction
            )
            
            if result['success']:
                # æª¢æŸ¥æ•¸æ“šæ˜¯å¦æ­£ç¢ºå­˜å„²
                log_id = result['interaction_log_id']
                retrieved_data = await coordinator.data_manager.retrieve_data(
                    log_id, DataLayer.INTERACTION
                )
                
                if retrieved_data and retrieved_data['log_id'] == log_id:
                    results['data_flow_integrity']['score'] = 25
                    results['data_flow_integrity']['details'] = 'æ•¸æ“šæµå®Œæ•´æ€§æ­£å¸¸'
                    print("    âœ… æ•¸æ“šæµå®Œæ•´æ€§æ¸¬è©¦é€šé")
                else:
                    results['data_flow_integrity']['details'] = 'æ•¸æ“šæª¢ç´¢å¤±æ•—'
                    print("    âŒ æ•¸æ“šæµå®Œæ•´æ€§æ¸¬è©¦å¤±æ•—")
            else:
                results['data_flow_integrity']['details'] = 'æ•¸æ“šè™•ç†å¤±æ•—'
                print("    âŒ æ•¸æ“šæµå®Œæ•´æ€§æ¸¬è©¦å¤±æ•—")
            
            # æ¸¬è©¦éŒ¯èª¤è™•ç†
            print("  âš ï¸ æ¸¬è©¦éŒ¯èª¤è™•ç†...")
            try:
                # æ•…æ„è§¸ç™¼éŒ¯èª¤
                invalid_interaction = {
                    'user_request': None,  # ç„¡æ•ˆæ•¸æ“š
                    'agent_response': {'text': 'éŒ¯èª¤æ¸¬è©¦'},
                    'deliverables': 'invalid_format'  # éŒ¯èª¤æ ¼å¼
                }
                
                error_result = await coordinator.process_interaction(
                    InteractionSource.SYSTEM_INTERNAL, invalid_interaction
                )
                
                # æª¢æŸ¥æ˜¯å¦æ­£ç¢ºè™•ç†éŒ¯èª¤
                if not error_result['success'] and 'error' in error_result:
                    results['error_handling']['score'] = 25
                    results['error_handling']['details'] = 'éŒ¯èª¤è™•ç†æ©Ÿåˆ¶æ­£å¸¸'
                    print("    âœ… éŒ¯èª¤è™•ç†æ¸¬è©¦é€šé")
                else:
                    results['error_handling']['details'] = 'éŒ¯èª¤è™•ç†æ©Ÿåˆ¶ç•°å¸¸'
                    print("    âŒ éŒ¯èª¤è™•ç†æ¸¬è©¦å¤±æ•—")
                    
            except Exception as e:
                # å¦‚æœæ‹‹å‡ºç•°å¸¸ï¼Œæª¢æŸ¥æ˜¯å¦æ˜¯é æœŸçš„
                results['error_handling']['score'] = 20  # éƒ¨åˆ†åˆ†æ•¸
                results['error_handling']['details'] = f'éŒ¯èª¤è™•ç†: {str(e)[:100]}'
                print("    âš ï¸ éŒ¯èª¤è™•ç†æ¸¬è©¦éƒ¨åˆ†é€šé")
            
            # æ¸¬è©¦é…ç½®ç®¡ç†
            print("  âš™ï¸ æ¸¬è©¦é…ç½®ç®¡ç†...")
            system_status = await coordinator.get_system_status()
            
            if (system_status and 
                'coordinator_status' in system_status and
                'collector_statistics' in system_status and
                'data_manager_config' in system_status):
                results['configuration_management']['score'] = 25
                results['configuration_management']['details'] = 'é…ç½®ç®¡ç†æ­£å¸¸'
                print("    âœ… é…ç½®ç®¡ç†æ¸¬è©¦é€šé")
            else:
                results['configuration_management']['details'] = 'é…ç½®ç®¡ç†ç•°å¸¸'
                print("    âŒ é…ç½®ç®¡ç†æ¸¬è©¦å¤±æ•—")
            
            await coordinator.stop_processing()
            
        except Exception as e:
            error_msg = f"æ•´åˆæ¸¬è©¦ç•°å¸¸: {e}"
            print(f"    âŒ {error_msg}")
            for test_name in results:
                if results[test_name]['score'] == 0:
                    results[test_name]['details'] = error_msg
        
        return results
    
    async def test_data_consistency(self) -> Dict[str, Any]:
        """æ•¸æ“šä¸€è‡´æ€§æ¸¬è©¦"""
        results = {
            'data_format_consistency': {'score': 0, 'details': ''},
            'storage_integrity': {'score': 0, 'details': ''},
            'retrieval_accuracy': {'score': 0, 'details': ''},
            'cross_component_consistency': {'score': 0, 'details': ''}
        }
        
        try:
            coordinator = UnifiedArchitectureCoordinator({'base_dir': self.test_config['base_dir']})
            await coordinator.start_processing()
            
            # æ¸¬è©¦æ•¸æ“šæ ¼å¼ä¸€è‡´æ€§
            print("  ğŸ“‹ æ¸¬è©¦æ•¸æ“šæ ¼å¼ä¸€è‡´æ€§...")
            test_interactions = []
            
            for i in range(3):
                interaction = {
                    'user_request': {'text': f'ä¸€è‡´æ€§æ¸¬è©¦ {i}', 'type': 'consistency_test'},
                    'agent_response': {'text': f'ä¸€è‡´æ€§éŸ¿æ‡‰ {i}'},
                    'deliverables': [{
                        'name': f'consistency_{i}.py',
                        'content': f'# Consistency test {i}',
                        'metadata': {'test_id': i}
                    }]
                }
                
                result = await coordinator.process_interaction(
                    InteractionSource.SYSTEM_INTERNAL, interaction
                )
                
                if result['success']:
                    test_interactions.append(result['interaction_log_id'])
            
            if len(test_interactions) == 3:
                results['data_format_consistency']['score'] = 25
                results['data_format_consistency']['details'] = 'æ•¸æ“šæ ¼å¼ä¸€è‡´æ€§æ­£å¸¸'
                print("    âœ… æ•¸æ“šæ ¼å¼ä¸€è‡´æ€§æ¸¬è©¦é€šé")
            else:
                results['data_format_consistency']['details'] = f'åªæœ‰{len(test_interactions)}/3å€‹äº¤äº’æˆåŠŸ'
                print("    âŒ æ•¸æ“šæ ¼å¼ä¸€è‡´æ€§æ¸¬è©¦å¤±æ•—")
            
            # æ¸¬è©¦å­˜å„²å®Œæ•´æ€§
            print("  ğŸ’¾ æ¸¬è©¦å­˜å„²å®Œæ•´æ€§...")
            stored_count = 0
            for log_id in test_interactions:
                retrieved_data = await coordinator.data_manager.retrieve_data(
                    log_id, DataLayer.INTERACTION
                )
                if retrieved_data:
                    stored_count += 1
            
            if stored_count == len(test_interactions):
                results['storage_integrity']['score'] = 25
                results['storage_integrity']['details'] = 'å­˜å„²å®Œæ•´æ€§æ­£å¸¸'
                print("    âœ… å­˜å„²å®Œæ•´æ€§æ¸¬è©¦é€šé")
            else:
                results['storage_integrity']['details'] = f'åªæœ‰{stored_count}/{len(test_interactions)}å€‹æ•¸æ“šå¯æª¢ç´¢'
                print("    âŒ å­˜å„²å®Œæ•´æ€§æ¸¬è©¦å¤±æ•—")
            
            # æ¸¬è©¦æª¢ç´¢æº–ç¢ºæ€§
            print("  ğŸ” æ¸¬è©¦æª¢ç´¢æº–ç¢ºæ€§...")
            if test_interactions:
                query_result = await coordinator.data_manager.query_data(
                    {'interaction_source': 'system_internal'}, DataLayer.INTERACTION
                )
                
                if len(query_result) >= len(test_interactions):
                    results['retrieval_accuracy']['score'] = 25
                    results['retrieval_accuracy']['details'] = 'æª¢ç´¢æº–ç¢ºæ€§æ­£å¸¸'
                    print("    âœ… æª¢ç´¢æº–ç¢ºæ€§æ¸¬è©¦é€šé")
                else:
                    results['retrieval_accuracy']['details'] = f'æŸ¥è©¢çµæœä¸å®Œæ•´: {len(query_result)}/{len(test_interactions)}'
                    print("    âŒ æª¢ç´¢æº–ç¢ºæ€§æ¸¬è©¦å¤±æ•—")
            
            # æ¸¬è©¦è·¨çµ„ä»¶ä¸€è‡´æ€§
            print("  ğŸ”„ æ¸¬è©¦è·¨çµ„ä»¶ä¸€è‡´æ€§...")
            collector_stats = await coordinator.interaction_collector.get_statistics()
            system_status = await coordinator.get_system_status()
            
            collector_count = collector_stats.get('total_collected', 0)
            coordinator_count = system_status['coordinator_status'].get('processed_interactions', 0)
            
            if collector_count > 0 and coordinator_count > 0:
                results['cross_component_consistency']['score'] = 25
                results['cross_component_consistency']['details'] = 'è·¨çµ„ä»¶ä¸€è‡´æ€§æ­£å¸¸'
                print("    âœ… è·¨çµ„ä»¶ä¸€è‡´æ€§æ¸¬è©¦é€šé")
            else:
                results['cross_component_consistency']['details'] = f'çµ±è¨ˆä¸ä¸€è‡´: æ”¶é›†å™¨{collector_count}, å”èª¿å™¨{coordinator_count}'
                print("    âŒ è·¨çµ„ä»¶ä¸€è‡´æ€§æ¸¬è©¦å¤±æ•—")
            
            await coordinator.stop_processing()
            
        except Exception as e:
            error_msg = f"æ•¸æ“šä¸€è‡´æ€§æ¸¬è©¦ç•°å¸¸: {e}"
            print(f"    âŒ {error_msg}")
            for test_name in results:
                if results[test_name]['score'] == 0:
                    results[test_name]['details'] = error_msg
        
        return results
    
    def calculate_overall_score(self) -> float:
        """è¨ˆç®—ç¸½é«”è©•åˆ†"""
        total_score = 0
        max_score = 0
        
        # åŠŸèƒ½æ¸¬è©¦ (40%)
        functionality_scores = [test['score'] for test in self.test_results['functionality_tests'].values()]
        functionality_total = sum(functionality_scores)
        functionality_max = len(functionality_scores) * 20
        functionality_weight = 0.4
        
        # æ€§èƒ½æ¸¬è©¦ (25%)
        performance_scores = [test['score'] for test in self.test_results['performance_tests'].values()]
        performance_total = sum(performance_scores)
        performance_max = len(performance_scores) * 25
        performance_weight = 0.25
        
        # æ•´åˆæ¸¬è©¦ (20%)
        integration_scores = [test['score'] for test in self.test_results['integration_tests'].values()]
        integration_total = sum(integration_scores)
        integration_max = len(integration_scores) * 25
        integration_weight = 0.20
        
        # æ•¸æ“šä¸€è‡´æ€§æ¸¬è©¦ (15%)
        consistency_scores = [test['score'] for test in self.test_results['data_consistency_tests'].values()]
        consistency_total = sum(consistency_scores)
        consistency_max = len(consistency_scores) * 25
        consistency_weight = 0.15
        
        # è¨ˆç®—åŠ æ¬Šç¸½åˆ†
        if functionality_max > 0:
            total_score += (functionality_total / functionality_max) * 100 * functionality_weight
        if performance_max > 0:
            total_score += (performance_total / performance_max) * 100 * performance_weight
        if integration_max > 0:
            total_score += (integration_total / integration_max) * 100 * integration_weight
        if consistency_max > 0:
            total_score += (consistency_total / consistency_max) * 100 * consistency_weight
        
        return round(total_score, 2)
    
    async def generate_test_report(self):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        report = {
            'test_summary': {
                'timestamp': datetime.now().isoformat(),
                'overall_score': self.test_results['overall_score'],
                'test_environment': self.test_config,
                'status': 'PASSED' if self.test_results['overall_score'] >= 70 else 'FAILED'
            },
            'detailed_results': self.test_results,
            'recommendations': self.generate_recommendations()
        }
        
        # ä¿å­˜å ±å‘Š
        report_path = Path(self.test_config['base_dir']) / 'unified_architecture_test_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ æ¸¬è©¦å ±å‘Šå·²ç”Ÿæˆ: {report_path}")
        return report
    
    def generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼æ¸¬è©¦çµæœç”Ÿæˆå»ºè­°
        if self.test_results['overall_score'] < 70:
            recommendations.append("æ•´é«”è©•åˆ†åä½ï¼Œéœ€è¦å…¨é¢å„ªåŒ–ç³»çµ±")
        
        # åŠŸèƒ½æ¸¬è©¦å»ºè­°
        functionality_tests = self.test_results['functionality_tests']
        for test_name, test_result in functionality_tests.items():
            if not test_result['passed']:
                recommendations.append(f"éœ€è¦ä¿®å¾©{test_name}åŠŸèƒ½: {test_result['details']}")
        
        # æ€§èƒ½æ¸¬è©¦å»ºè­°
        performance_tests = self.test_results['performance_tests']
        for test_name, test_result in performance_tests.items():
            if test_result['score'] < 20:
                recommendations.append(f"éœ€è¦å„ªåŒ–{test_name}æ€§èƒ½: {test_result['details']}")
        
        if not recommendations:
            recommendations.append("ç³»çµ±é‹è¡Œè‰¯å¥½ï¼Œå»ºè­°ç¹¼çºŒç›£æ§å’Œç¶­è­·")
        
        return recommendations

async def main():
    """ä¸»æ¸¬è©¦ç¨‹åº"""
    print("ğŸ§ª PowerAutomation v0.53 çµ±ä¸€æ¶æ§‹æ•´åˆæ¸¬è©¦")
    print("=" * 60)
    
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(level=logging.WARNING)  # æ¸›å°‘æ—¥èªŒè¼¸å‡º
    
    # å‰µå»ºæ¸¬è©¦å¯¦ä¾‹
    test_suite = UnifiedArchitectureIntegrationTest()
    
    # é‹è¡Œæ‰€æœ‰æ¸¬è©¦
    results = await test_suite.run_all_tests()
    
    # è¼¸å‡ºç¸½çµ
    print("\n" + "=" * 60)
    print("ğŸ“Š æ¸¬è©¦ç¸½çµ")
    print("=" * 60)
    
    overall_score = results['overall_score']
    status = "âœ… é€šé" if overall_score >= 70 else "âŒ å¤±æ•—"
    
    print(f"ç¸½é«”è©•åˆ†: {overall_score:.2f}/100 {status}")
    
    # åˆ†é¡è©•åˆ†
    categories = [
        ('åŠŸèƒ½å®Œæ•´æ€§', results['functionality_tests']),
        ('æ€§èƒ½è¡¨ç¾', results['performance_tests']),
        ('æ•´åˆæ•ˆæœ', results['integration_tests']),
        ('æ•¸æ“šä¸€è‡´æ€§', results['data_consistency_tests'])
    ]
    
    for category_name, category_results in categories:
        scores = [test['score'] for test in category_results.values()]
        if scores:
            avg_score = sum(scores) / len(scores)
            max_possible = 25 if category_name != 'åŠŸèƒ½å®Œæ•´æ€§' else 20
            percentage = (avg_score / max_possible) * 100
            print(f"{category_name}: {percentage:.1f}% ({avg_score:.1f}/{max_possible})")
    
    print("\nğŸ¯ çµ±ä¸€æ¶æ§‹æ•´åˆæ¸¬è©¦å®Œæˆï¼")
    
    return results

if __name__ == "__main__":
    asyncio.run(main())

